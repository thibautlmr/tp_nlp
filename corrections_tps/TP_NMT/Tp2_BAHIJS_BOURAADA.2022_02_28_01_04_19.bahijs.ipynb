{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUC2WREPgd8v"
      },
      "source": [
        "**Préambule**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQYTJhLg7X9p",
        "outputId": "2594e047-adab-4ed9-c8ab-8af1975078bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEiFt9v47wCe",
        "outputId": "5e48f5ef-2492-4a78-8d6b-012e6455d400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 51 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 71 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 92 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 102 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 112 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 122 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 133 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 143 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 153 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 163 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 174 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 184 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 194 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 204 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 215 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 216 kB 20.7 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (3.13)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.4)\n",
            "Collecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.30.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.8.0)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.21.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.43.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.2.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py) (2.0.1)\n",
            "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 pyonmttok-1.30.1 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install OpenNMT-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRCwO4Sn4v4N"
      },
      "source": [
        "**Q1:** \n",
        "  Les fichiers **IWSLT10_BTEC.train.en.txt** et **IWSLT10_BTEC.train.fr.txt** contiennent des données d'apprentissage. Le fichier **IWSLT10_BTEC.train.en.txt** contient des passage en anglais et le fichier **IWSLT10_BTEC.train.fr.txt** contient leur traduction en français en utilisant les commandes linux, ces fichiers servent de base pour les modèles et sont considérés comme la traduction \"correcte\" des fichiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tEZvQ848ZJl"
      },
      "source": [
        "**Q2:** Dans les fichiers tok.txt, les apostrophes sont représentées par \"\\&apos;\", son code HTML. Cela est efféctué dans le but que les caractères spéciaux soient reconnus en tant que tel par notre programme. \n",
        "\n",
        "<span style=\"color:red\">La difference importante est representé par les espaces entroduit entre chanque mot et la ponctuation. Le but de cette stratégie basique de tokenization est de réduire la taille du vocabulaire. Si on ne sépare pas les mots et la ponctuation on sera obligé à avoir plus de tokens dans le vocabularie. E.g. \"Salut\", \"Salut!\", \"Salut,\", \"Salut?\", etc. </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtTXwm3S76SO",
        "outputId": "b29ca015-f9ad-43f0-996e-8bff08d6ec95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/BTEC-en-fr\n",
        "cd ./dev\n",
        "awk -F '\\' '{print $NF}' IWSLT10.devset1_CSTAR03.fr.txt > IWSLT10.devset1_CSTAR03.fr.clean.txt\n",
        "awk -F '\\' '{print $NF}' IWSLT10.devset1_CSTAR03.en.txt > IWSLT10.devset1_CSTAR03.en.clean.txt\n",
        "\n",
        "cd ../test\n",
        "awk -F '\\' '{print $NF}' IWSLT09_BTEC.testset.fr.txt > IWSLT09_BTEC.testset.fr.clean.txt\n",
        "awk -F '\\' '{print $NF}' IWSLT09_BTEC.testset.en.txt > IWSLT09_BTEC.testset.en.clean.txt\n",
        "\n",
        "cd ../train\n",
        "awk -F '\\' '{print $NF}' IWSLT10_BTEC.train.fr.txt > IWSLT10_BTEC.train.fr.clean.txt\n",
        "awk -F '\\' '{print $NF}' IWSLT10_BTEC.train.en.txt > IWSLT10_BTEC.train.en.clean.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcIbV_5qATuh",
        "outputId": "0b13edb5-0e56-4a56-e755-3166b31e87a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "perl tokenizer.perl -l fr -lc < ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.clean.txt > ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt\n",
        "perl tokenizer.perl -l en -lc < ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.clean.txt > ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt\n",
        "\n",
        "perl tokenizer.perl -l fr -lc < ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.clean.txt > ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt\n",
        "perl tokenizer.perl -l en -lc < ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.clean.txt > ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt\n",
        "\n",
        "perl tokenizer.perl -l fr -lc < ./BTEC-en-fr/train/IWSLT10_BTEC.train.fr.clean.txt > ./BTEC-en-fr/train/IWSLT10_BTEC.train.fr.tok.txt\n",
        "perl tokenizer.perl -l en -lc < ./BTEC-en-fr/train/IWSLT10_BTEC.train.en.clean.txt > ./BTEC-en-fr/train/IWSLT10_BTEC.train.en.tok.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCJj2hGw_8FX"
      },
      "source": [
        "**Q3:** Étant donné un ensemble de données, il faut utiliser cette ressource pour l'apprentissge, le réglage des paramètres et le test de la précision du modèle. Nous ne pouvons pas effectuer cela avec les mêmes données. Nous choisissons ainsi un corpus séparé en trois parties (train, dev et test)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz65fWoVA3NQ"
      },
      "source": [
        "Les fichiers train ont pour but d'entraîner le classificateur. Les fichiers dev, utilisés pendant l'évaluation du classificateur avec différentes configurations ou variations dans la représentation des caractéristiques. <span style=\"color:red\">Je ne comprends pas ce que vous voulez dire. En tout cas le dev et utilisé pour évaluer le modèle pendent l'entrainement est décidé s'il faut l'arreter, ou encore pour établir la meilleure configuration de hyperparamétres. </span>  Les fichiers test contiennent les données sur lequel on vérifie finalement la précision du classificateur et on obtient des résultats non biaisés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh1nUfYeFRmH"
      },
      "source": [
        "**Q4:** Le but du vocabulaire est de permettre à notre programme de reconnaitre les mots qui font référence à la même chose (exemple: reservations, reservation). <span style=\"color:red\"> C'est à dire? Le vocabulaire permet d'associer à chaque mot un idéntifiant unique et un vecteur qui le represente et dont les valuers seronts appris par le réseau à travers l'entrainement. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRDr3i4HmdA"
      },
      "source": [
        "On utilise un vocabulaire issu de la phase d'apprentissage pour des raisons d'optimisation. Le programme n'a que des mots qu'il va utiliser. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCRQVpsOJG5J",
        "outputId": "0bcae4b6-83c0-41ec-d9ff-a11d1dfe8757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 18:20:16,242 INFO] Counter vocab from -1 samples.\n",
            "[2022-02-27 18:20:16,242 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2022-02-27 18:20:16,258 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 18:20:16,662 INFO] Counters src:9978\n",
            "[2022-02-27 18:20:16,663 INFO] Counters tgt:8194\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_build_vocab -config config-base.yaml -n_sample -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EScBdwCRKl0o"
      },
      "source": [
        "**Q5:** Counters src:9978 fait référence au nombre de mots dans le vocabulaire source.\n",
        "\n",
        "Counters tgt:8194 fait référence au nombre de mots dans le vocabulaire du ficher de sortie (ou target).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExQYLT1lJeoG",
        "outputId": "c7df792b-7f99-47f0-a045-d9f9cb197d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 18:20:36,761 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-27 18:20:36,931 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 18:20:36,931 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-27 18:20:36,932 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-27 18:20:36,933 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-27 18:20:36,933 INFO] Loading vocab from text file...\n",
            "[2022-02-27 18:20:36,933 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-27 18:20:36,959 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-27 18:20:36,964 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-27 18:20:36,990 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-27 18:20:36,994 INFO] Building fields with vocab in counters...\n",
            "[2022-02-27 18:20:37,004 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-27 18:20:37,019 INFO]  * src vocab size: 9980.\n",
            "[2022-02-27 18:20:37,021 INFO]  * src vocab size = 9980\n",
            "[2022-02-27 18:20:37,021 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-27 18:20:37,024 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 18:20:37,310 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-27 18:20:37,311 INFO] encoder: 5635120\n",
            "[2022-02-27 18:20:37,311 INFO] decoder: 7244222\n",
            "[2022-02-27 18:20:37,312 INFO] * number of parameters: 12879342\n",
            "[2022-02-27 18:20:37,314 INFO] Starting training on CPU, could be very slow\n",
            "[2022-02-27 18:20:37,314 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-27 18:20:37,314 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 18:20:37,315 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "[2022-02-27 18:20:42,592 INFO] Step 20/ 2000; acc:   9.21; ppl: 1954.02; xent: 7.58; lr: 1.00000; 1119/1164 tok/s;      5 sec\n",
            "[2022-02-27 18:20:48,332 INFO] Step 40/ 2000; acc:  14.89; ppl: 483.91; xent: 6.18; lr: 1.00000; 1199/1261 tok/s;     11 sec\n",
            "[2022-02-27 18:20:54,854 INFO] Step 60/ 2000; acc:  15.66; ppl: 315.28; xent: 5.75; lr: 1.00000; 1052/1106 tok/s;     18 sec\n",
            "[2022-02-27 18:20:59,833 INFO] Step 80/ 2000; acc:  20.28; ppl: 204.88; xent: 5.32; lr: 1.00000; 1119/1182 tok/s;     23 sec\n",
            "[2022-02-27 18:21:05,806 INFO] Step 100/ 2000; acc:  20.77; ppl: 205.98; xent: 5.33; lr: 1.00000; 1241/1250 tok/s;     28 sec\n",
            "[2022-02-27 18:21:11,768 INFO] Step 120/ 2000; acc:  21.77; ppl: 164.80; xent: 5.10; lr: 1.00000; 1180/1228 tok/s;     34 sec\n",
            "[2022-02-27 18:21:16,523 INFO] Step 140/ 2000; acc:  26.95; ppl: 109.73; xent: 4.70; lr: 1.00000; 1113/1178 tok/s;     39 sec\n",
            "[2022-02-27 18:21:22,529 INFO] Step 160/ 2000; acc:  22.88; ppl: 147.85; xent: 5.00; lr: 1.00000; 1224/1258 tok/s;     45 sec\n",
            "[2022-02-27 18:21:27,846 INFO] Step 180/ 2000; acc:  28.07; ppl: 97.11; xent: 4.58; lr: 1.00000; 1194/1251 tok/s;     51 sec\n",
            "[2022-02-27 18:21:33,398 INFO] Step 200/ 2000; acc:  27.60; ppl: 108.31; xent: 4.68; lr: 1.00000; 1122/1187 tok/s;     56 sec\n",
            "[2022-02-27 18:21:38,840 INFO] Step 220/ 2000; acc:  31.26; ppl: 87.89; xent: 4.48; lr: 1.00000; 1216/1221 tok/s;     62 sec\n",
            "[2022-02-27 18:21:43,816 INFO] Step 240/ 2000; acc:  33.25; ppl: 74.07; xent: 4.30; lr: 1.00000; 1166/1256 tok/s;     67 sec\n",
            "[2022-02-27 18:21:49,813 INFO] Step 260/ 2000; acc:  32.14; ppl: 79.45; xent: 4.38; lr: 1.00000; 1083/1176 tok/s;     72 sec\n",
            "[2022-02-27 18:21:55,151 INFO] Step 280/ 2000; acc:  34.37; ppl: 71.55; xent: 4.27; lr: 1.00000; 1173/1198 tok/s;     78 sec\n",
            "[2022-02-27 18:22:00,222 INFO] Step 300/ 2000; acc:  35.18; ppl: 62.25; xent: 4.13; lr: 1.00000; 1219/1310 tok/s;     83 sec\n",
            "[2022-02-27 18:22:05,922 INFO] Step 320/ 2000; acc:  33.50; ppl: 71.08; xent: 4.26; lr: 1.00000; 1131/1218 tok/s;     89 sec\n",
            "[2022-02-27 18:22:10,938 INFO] Step 340/ 2000; acc:  36.09; ppl: 59.44; xent: 4.09; lr: 1.00000; 1200/1221 tok/s;     94 sec\n",
            "[2022-02-27 18:22:16,585 INFO] Step 360/ 2000; acc:  34.88; ppl: 61.99; xent: 4.13; lr: 1.00000; 1236/1255 tok/s;     99 sec\n",
            "[2022-02-27 18:22:22,467 INFO] Step 380/ 2000; acc:  33.63; ppl: 66.26; xent: 4.19; lr: 1.00000; 1195/1187 tok/s;    105 sec\n",
            "[2022-02-27 18:22:27,154 INFO] Step 400/ 2000; acc:  37.92; ppl: 49.51; xent: 3.90; lr: 1.00000; 1192/1221 tok/s;    110 sec\n",
            "[2022-02-27 18:22:33,044 INFO] Step 420/ 2000; acc:  34.40; ppl: 57.80; xent: 4.06; lr: 1.00000; 1265/1287 tok/s;    116 sec\n",
            "[2022-02-27 18:22:38,933 INFO] Step 440/ 2000; acc:  35.16; ppl: 61.25; xent: 4.11; lr: 1.00000; 1205/1220 tok/s;    122 sec\n",
            "[2022-02-27 18:22:43,577 INFO] Step 460/ 2000; acc:  40.79; ppl: 40.84; xent: 3.71; lr: 1.00000; 1136/1174 tok/s;    126 sec\n",
            "[2022-02-27 18:22:49,572 INFO] Step 480/ 2000; acc:  34.50; ppl: 60.41; xent: 4.10; lr: 1.00000; 1223/1253 tok/s;    132 sec\n",
            "[2022-02-27 18:22:54,844 INFO] Step 500/ 2000; acc:  38.16; ppl: 44.63; xent: 3.80; lr: 1.00000; 1191/1291 tok/s;    138 sec\n",
            "[2022-02-27 18:23:00,674 INFO] Step 520/ 2000; acc:  38.51; ppl: 48.52; xent: 3.88; lr: 1.00000; 1062/1109 tok/s;    143 sec\n",
            "[2022-02-27 18:23:06,208 INFO] Step 540/ 2000; acc:  38.63; ppl: 48.12; xent: 3.87; lr: 1.00000; 1219/1241 tok/s;    149 sec\n",
            "[2022-02-27 18:23:11,008 INFO] Step 560/ 2000; acc:  43.17; ppl: 34.99; xent: 3.56; lr: 1.00000; 1212/1303 tok/s;    154 sec\n",
            "[2022-02-27 18:23:15,699 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-27 18:23:17,039 INFO] Step 580/ 2000; acc:  38.00; ppl: 47.70; xent: 3.87; lr: 1.00000; 1094/1185 tok/s;    160 sec\n",
            "[2022-02-27 18:23:22,230 INFO] Step 600/ 2000; acc:  40.38; ppl: 42.02; xent: 3.74; lr: 1.00000; 1203/1216 tok/s;    165 sec\n",
            "[2022-02-27 18:23:27,261 INFO] Step 620/ 2000; acc:  41.02; ppl: 38.49; xent: 3.65; lr: 1.00000; 1232/1310 tok/s;    170 sec\n",
            "[2022-02-27 18:23:28,682 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-27 18:23:30,197 INFO] Validation perplexity: 20.9908\n",
            "[2022-02-27 18:23:30,197 INFO] Validation accuracy: 48.7714\n",
            "[2022-02-27 18:23:30,254 INFO] Saving checkpoint ./models/base/model_step_625.pt\n",
            "[2022-02-27 18:23:35,921 INFO] Step 640/ 2000; acc:  38.81; ppl: 44.53; xent: 3.80; lr: 1.00000; 737/797 tok/s;    179 sec\n",
            "[2022-02-27 18:23:41,232 INFO] Step 660/ 2000; acc:  43.07; ppl: 34.11; xent: 3.53; lr: 1.00000; 1111/1138 tok/s;    184 sec\n",
            "[2022-02-27 18:23:46,872 INFO] Step 680/ 2000; acc:  38.35; ppl: 42.49; xent: 3.75; lr: 1.00000; 1209/1274 tok/s;    190 sec\n",
            "[2022-02-27 18:23:53,163 INFO] Step 700/ 2000; acc:  40.10; ppl: 38.63; xent: 3.65; lr: 1.00000; 1082/1133 tok/s;    196 sec\n",
            "[2022-02-27 18:23:57,909 INFO] Step 720/ 2000; acc:  44.23; ppl: 32.82; xent: 3.49; lr: 1.00000; 1181/1229 tok/s;    201 sec\n",
            "[2022-02-27 18:24:03,798 INFO] Step 740/ 2000; acc:  39.47; ppl: 39.22; xent: 3.67; lr: 1.00000; 1277/1292 tok/s;    206 sec\n",
            "[2022-02-27 18:24:09,699 INFO] Step 760/ 2000; acc:  39.69; ppl: 39.30; xent: 3.67; lr: 1.00000; 1212/1261 tok/s;    212 sec\n",
            "[2022-02-27 18:24:14,424 INFO] Step 780/ 2000; acc:  48.07; ppl: 25.47; xent: 3.24; lr: 1.00000; 1110/1180 tok/s;    217 sec\n",
            "[2022-02-27 18:24:20,225 INFO] Step 800/ 2000; acc:  40.71; ppl: 36.90; xent: 3.61; lr: 1.00000; 1238/1280 tok/s;    223 sec\n",
            "[2022-02-27 18:24:25,440 INFO] Step 820/ 2000; acc:  44.41; ppl: 29.56; xent: 3.39; lr: 1.00000; 1187/1279 tok/s;    228 sec\n",
            "[2022-02-27 18:24:30,925 INFO] Step 840/ 2000; acc:  43.21; ppl: 35.04; xent: 3.56; lr: 1.00000; 1126/1173 tok/s;    234 sec\n",
            "[2022-02-27 18:24:36,261 INFO] Step 860/ 2000; acc:  44.25; ppl: 29.53; xent: 3.39; lr: 1.00000; 1250/1256 tok/s;    239 sec\n",
            "[2022-02-27 18:24:41,102 INFO] Step 880/ 2000; acc:  45.51; ppl: 26.10; xent: 3.26; lr: 1.00000; 1205/1317 tok/s;    244 sec\n",
            "[2022-02-27 18:24:47,039 INFO] Step 900/ 2000; acc:  43.40; ppl: 30.80; xent: 3.43; lr: 1.00000; 1110/1189 tok/s;    250 sec\n",
            "[2022-02-27 18:24:52,096 INFO] Step 920/ 2000; acc:  44.40; ppl: 30.14; xent: 3.41; lr: 1.00000; 1258/1267 tok/s;    255 sec\n",
            "[2022-02-27 18:24:57,231 INFO] Step 940/ 2000; acc:  44.48; ppl: 27.23; xent: 3.30; lr: 1.00000; 1211/1330 tok/s;    260 sec\n",
            "[2022-02-27 18:25:02,895 INFO] Step 960/ 2000; acc:  43.43; ppl: 32.01; xent: 3.47; lr: 1.00000; 1166/1204 tok/s;    266 sec\n",
            "[2022-02-27 18:25:07,825 INFO] Step 980/ 2000; acc:  45.36; ppl: 26.18; xent: 3.26; lr: 1.00000; 1209/1236 tok/s;    271 sec\n",
            "[2022-02-27 18:25:13,486 INFO] Step 1000/ 2000; acc:  44.96; ppl: 26.31; xent: 3.27; lr: 1.00000; 1216/1251 tok/s;    276 sec\n",
            "[2022-02-27 18:25:19,320 INFO] Step 1020/ 2000; acc:  43.27; ppl: 30.13; xent: 3.41; lr: 1.00000; 1178/1165 tok/s;    282 sec\n",
            "[2022-02-27 18:25:23,942 INFO] Step 1040/ 2000; acc:  47.75; ppl: 24.14; xent: 3.18; lr: 1.00000; 1221/1252 tok/s;    287 sec\n",
            "[2022-02-27 18:25:29,860 INFO] Step 1060/ 2000; acc:  42.38; ppl: 29.42; xent: 3.38; lr: 1.00000; 1278/1299 tok/s;    293 sec\n",
            "[2022-02-27 18:25:35,876 INFO] Step 1080/ 2000; acc:  42.71; ppl: 32.74; xent: 3.49; lr: 1.00000; 1203/1226 tok/s;    299 sec\n",
            "[2022-02-27 18:25:40,382 INFO] Step 1100/ 2000; acc:  49.32; ppl: 21.51; xent: 3.07; lr: 1.00000; 1173/1206 tok/s;    303 sec\n",
            "[2022-02-27 18:25:46,192 INFO] Step 1120/ 2000; acc:  43.09; ppl: 29.91; xent: 3.40; lr: 1.00000; 1256/1272 tok/s;    309 sec\n",
            "[2022-02-27 18:25:51,524 INFO] Step 1140/ 2000; acc:  45.61; ppl: 23.99; xent: 3.18; lr: 1.00000; 1171/1291 tok/s;    314 sec\n",
            "[2022-02-27 18:25:56,935 INFO] Step 1160/ 2000; acc:  44.88; ppl: 25.59; xent: 3.24; lr: 1.00000; 1122/1225 tok/s;    320 sec\n",
            "[2022-02-27 18:26:02,484 INFO] Step 1180/ 2000; acc:  45.47; ppl: 25.49; xent: 3.24; lr: 1.00000; 1203/1233 tok/s;    325 sec\n",
            "[2022-02-27 18:26:07,297 INFO] Step 1200/ 2000; acc:  50.28; ppl: 18.92; xent: 2.94; lr: 1.00000; 1200/1264 tok/s;    330 sec\n",
            "[2022-02-27 18:26:11,989 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-27 18:26:13,250 INFO] Step 1220/ 2000; acc:  44.28; ppl: 28.85; xent: 3.36; lr: 1.00000; 1106/1176 tok/s;    336 sec\n",
            "[2022-02-27 18:26:18,773 INFO] Step 1240/ 2000; acc:  45.65; ppl: 24.66; xent: 3.21; lr: 1.00000; 1135/1169 tok/s;    341 sec\n",
            "[2022-02-27 18:26:23,986 INFO] Validation perplexity: 13.4459\n",
            "[2022-02-27 18:26:23,987 INFO] Validation accuracy: 55.0765\n",
            "[2022-02-27 18:26:24,046 INFO] Saving checkpoint ./models/base/model_step_1250.pt\n",
            "[2022-02-27 18:26:27,600 INFO] Step 1260/ 2000; acc:  46.79; ppl: 22.72; xent: 3.12; lr: 1.00000; 704/772 tok/s;    350 sec\n",
            "[2022-02-27 18:26:33,525 INFO] Step 1280/ 2000; acc:  46.58; ppl: 25.07; xent: 3.22; lr: 1.00000; 1086/1121 tok/s;    356 sec\n",
            "[2022-02-27 18:26:38,674 INFO] Step 1300/ 2000; acc:  49.42; ppl: 20.71; xent: 3.03; lr: 1.00000; 1158/1212 tok/s;    361 sec\n",
            "[2022-02-27 18:26:44,262 INFO] Step 1320/ 2000; acc:  46.58; ppl: 22.40; xent: 3.11; lr: 1.00000; 1240/1261 tok/s;    367 sec\n",
            "[2022-02-27 18:26:50,523 INFO] Step 1340/ 2000; acc:  45.70; ppl: 24.47; xent: 3.20; lr: 1.00000; 1099/1137 tok/s;    373 sec\n",
            "[2022-02-27 18:26:55,286 INFO] Step 1360/ 2000; acc:  50.23; ppl: 20.16; xent: 3.00; lr: 1.00000; 1169/1251 tok/s;    378 sec\n",
            "[2022-02-27 18:27:01,226 INFO] Step 1380/ 2000; acc:  45.72; ppl: 24.74; xent: 3.21; lr: 1.00000; 1256/1276 tok/s;    384 sec\n",
            "[2022-02-27 18:27:07,168 INFO] Step 1400/ 2000; acc:  45.89; ppl: 24.67; xent: 3.21; lr: 1.00000; 1207/1239 tok/s;    390 sec\n",
            "[2022-02-27 18:27:11,928 INFO] Step 1420/ 2000; acc:  52.20; ppl: 16.98; xent: 2.83; lr: 1.00000; 1080/1179 tok/s;    395 sec\n",
            "[2022-02-27 18:27:17,571 INFO] Step 1440/ 2000; acc:  47.67; ppl: 20.81; xent: 3.04; lr: 1.00000; 1251/1283 tok/s;    400 sec\n",
            "[2022-02-27 18:27:22,675 INFO] Step 1460/ 2000; acc:  49.79; ppl: 19.47; xent: 2.97; lr: 1.00000; 1197/1277 tok/s;    405 sec\n",
            "[2022-02-27 18:27:28,189 INFO] Step 1480/ 2000; acc:  48.50; ppl: 21.32; xent: 3.06; lr: 1.00000; 1104/1188 tok/s;    411 sec\n",
            "[2022-02-27 18:27:33,477 INFO] Step 1500/ 2000; acc:  48.72; ppl: 19.69; xent: 2.98; lr: 1.00000; 1273/1288 tok/s;    416 sec\n",
            "[2022-02-27 18:27:38,413 INFO] Step 1520/ 2000; acc:  50.87; ppl: 17.40; xent: 2.86; lr: 1.00000; 1184/1328 tok/s;    421 sec\n",
            "[2022-02-27 18:27:44,369 INFO] Step 1540/ 2000; acc:  47.85; ppl: 22.19; xent: 3.10; lr: 1.00000; 1121/1146 tok/s;    427 sec\n",
            "[2022-02-27 18:27:49,637 INFO] Step 1560/ 2000; acc:  49.60; ppl: 19.49; xent: 2.97; lr: 1.00000; 1214/1268 tok/s;    432 sec\n",
            "[2022-02-27 18:27:54,797 INFO] Step 1580/ 2000; acc:  49.56; ppl: 17.53; xent: 2.86; lr: 1.00000; 1221/1276 tok/s;    437 sec\n",
            "[2022-02-27 18:28:00,400 INFO] Step 1600/ 2000; acc:  47.98; ppl: 21.37; xent: 3.06; lr: 1.00000; 1196/1218 tok/s;    443 sec\n",
            "[2022-02-27 18:28:05,367 INFO] Step 1620/ 2000; acc:  50.38; ppl: 17.88; xent: 2.88; lr: 1.00000; 1201/1232 tok/s;    448 sec\n",
            "[2022-02-27 18:28:11,092 INFO] Step 1640/ 2000; acc:  48.08; ppl: 19.17; xent: 2.95; lr: 1.00000; 1212/1246 tok/s;    454 sec\n",
            "[2022-02-27 18:28:16,862 INFO] Step 1660/ 2000; acc:  48.13; ppl: 21.25; xent: 3.06; lr: 1.00000; 1206/1193 tok/s;    460 sec\n",
            "[2022-02-27 18:28:21,695 INFO] Step 1680/ 2000; acc:  52.00; ppl: 16.78; xent: 2.82; lr: 1.00000; 1154/1184 tok/s;    464 sec\n",
            "[2022-02-27 18:28:27,641 INFO] Step 1700/ 2000; acc:  47.89; ppl: 19.75; xent: 2.98; lr: 1.00000; 1255/1272 tok/s;    470 sec\n",
            "[2022-02-27 18:28:33,572 INFO] Step 1720/ 2000; acc:  46.79; ppl: 22.60; xent: 3.12; lr: 1.00000; 1194/1222 tok/s;    476 sec\n",
            "[2022-02-27 18:28:38,216 INFO] Step 1740/ 2000; acc:  54.16; ppl: 13.60; xent: 2.61; lr: 1.00000; 1128/1190 tok/s;    481 sec\n",
            "[2022-02-27 18:28:44,172 INFO] Step 1760/ 2000; acc:  47.07; ppl: 20.54; xent: 3.02; lr: 1.00000; 1224/1255 tok/s;    487 sec\n",
            "[2022-02-27 18:28:49,401 INFO] Step 1780/ 2000; acc:  51.19; ppl: 16.04; xent: 2.78; lr: 1.00000; 1189/1306 tok/s;    492 sec\n",
            "[2022-02-27 18:28:54,779 INFO] Step 1800/ 2000; acc:  48.63; ppl: 19.71; xent: 2.98; lr: 1.00000; 1133/1230 tok/s;    497 sec\n",
            "[2022-02-27 18:29:00,281 INFO] Step 1820/ 2000; acc:  50.55; ppl: 16.57; xent: 2.81; lr: 1.00000; 1207/1253 tok/s;    503 sec\n",
            "[2022-02-27 18:29:04,931 INFO] Step 1840/ 2000; acc:  54.43; ppl: 13.29; xent: 2.59; lr: 1.00000; 1243/1294 tok/s;    508 sec\n",
            "[2022-02-27 18:29:09,580 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-27 18:29:10,767 INFO] Step 1860/ 2000; acc:  48.22; ppl: 19.96; xent: 2.99; lr: 1.00000; 1121/1182 tok/s;    513 sec\n",
            "[2022-02-27 18:29:15,858 INFO] Validation perplexity: 10.0033\n",
            "[2022-02-27 18:29:15,858 INFO] Validation accuracy: 59.0172\n",
            "[2022-02-27 18:29:15,917 INFO] Saving checkpoint ./models/base/model_step_1875.pt\n",
            "[2022-02-27 18:29:18,766 INFO] Step 1880/ 2000; acc:  49.71; ppl: 17.40; xent: 2.86; lr: 1.00000; 789/807 tok/s;    521 sec\n",
            "[2022-02-27 18:29:24,416 INFO] Step 1900/ 2000; acc:  51.37; ppl: 15.94; xent: 2.77; lr: 1.00000; 1105/1211 tok/s;    527 sec\n",
            "[2022-02-27 18:29:30,337 INFO] Step 1920/ 2000; acc:  49.36; ppl: 18.50; xent: 2.92; lr: 1.00000; 1104/1152 tok/s;    533 sec\n",
            "[2022-02-27 18:29:35,475 INFO] Step 1940/ 2000; acc:  51.32; ppl: 15.78; xent: 2.76; lr: 1.00000; 1165/1243 tok/s;    538 sec\n",
            "[2022-02-27 18:29:41,268 INFO] Step 1960/ 2000; acc:  50.02; ppl: 17.56; xent: 2.87; lr: 1.00000; 1207/1211 tok/s;    544 sec\n",
            "[2022-02-27 18:29:47,508 INFO] Step 1980/ 2000; acc:  50.11; ppl: 17.35; xent: 2.85; lr: 1.00000; 1108/1141 tok/s;    550 sec\n",
            "[2022-02-27 18:29:52,250 INFO] Step 2000/ 2000; acc:  54.71; ppl: 13.27; xent: 2.59; lr: 1.00000; 1183/1246 tok/s;    555 sec\n",
            "[2022-02-27 18:29:52,311 INFO] Saving checkpoint ./models/base/model_step_2000.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-base.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5_j99AUMgZu"
      },
      "source": [
        "**Q6:** La valeur de acc fait référence au niveau de précision du modèle. Plus cette valeur est élevée, plus c'est un bon indicateur de notre modèle. La valeur de ppl est la pérplexité. Plus elle est faible, plus c'est bon résultat pour notre modèle.  <span style=\"color:red\"> Définition mathématique de acc et ppl? Intuition dérrière la définition de perplexity? </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75HgT_W9R8qJ"
      },
      "source": [
        "Q7:\n",
        ">valid_steps : effectue une validation tous les X pas.\n",
        "\n",
        ">train_steps : le nombre d'étapes d'apprentissage. <span style=\"color:red\">C'est quoi un pas d'apprentissage? (vous n'avez que traduit le nom de la variable sans expliquer sa signification). train_step = nombre total d'étapes d'optimisation des poids du modèle, où chaque étape est constituée d'un passage en avant d'un lot (batch) de données d'apprentissage dans le modèle (forward pass) et d'un passage en arrière des gradients générés par la fonction de perte (backward pass), suivi par une mise à jours des poids du modèle. </span>\n",
        "\n",
        ">enc_layers: le nombre de couches de l'encodeur.  \n",
        "\n",
        ">dec_layers: le nombre de couches du décodeur.  \n",
        "\n",
        "> enc_rnn_size:  la taille des états cachéq de l'encodeur RNN.\n",
        "\n",
        "> dec_rnn_size: la taille des états cachéq du décodeur RNN. \n",
        "\n",
        "> Batch_size: le nombre d'échantillons qui seront propagés dans le réseau. <span style=\"color:red\"> ...dans la même étape d'entrainement/évalutaion (c'est un précisation importante) </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ker3aNelPCAL"
      },
      "source": [
        "**Q8:** Le score BLEU est compris entre 0 et 1, ici il est affiché en pourcentage. Plus la valeur est élevée, plus les textes sont similaires. <span style=\"color:red\"> Similaires à quoi? Est-ce qu'un score BLEU plus élévé veut toujours dire que la trad est méilleure ? -> non</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_lWovUhde9"
      },
      "source": [
        "**Q9:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKplJIwDbMFC",
        "outputId": "58f05e8e-7e2e-45ed-d00f-47df00367b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:53:59,741 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:54:03,637 INFO] PRED AVG SCORE: -1.2845, PRED PPL: 3.6129\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:54:05,844 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:54:11,357 INFO] PRED AVG SCORE: -1.1271, PRED PPL: 3.0868\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:54:13,573 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:54:18,134 INFO] PRED AVG SCORE: -0.9826, PRED PPL: 2.6715\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:54:20,405 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:54:24,813 INFO] PRED AVG SCORE: -0.9560, PRED PPL: 2.6012\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/models/base\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output pred2000.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv0pwEuSdLpP",
        "outputId": "0ca3ebed-75f6-48df-b1fd-f06cd9e0f16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 9.54, 47.0/17.7/9.3/3.2 (BP=0.762, ratio=0.786, hyp_len=2810, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 12.31, 41.0/16.4/9.2/3.7 (BP=1.000, ratio=1.113, hyp_len=3978, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.17, 52.8/24.7/15.1/7.4 (BP=0.931, ratio=0.933, hyp_len=3334, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.36, 54.9/26.4/16.3/8.0 (BP=0.881, ratio=0.888, hyp_len=3173, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy6OMyfrVWVf"
      },
      "source": [
        "**Q9:** Le modèle pred2000 à le meilleur score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnmrH6Qihutp"
      },
      "source": [
        "**Q10:** On obtient un meilleur score BLEU en augmentant le nombre de train_steps puisque cela permet plus de précision. <span style=\"color:red\"> (oui mais, pourquoi?)</span>  On augmente le train_steps jusqu'à ce que le score BLEU converge. L'option -early_stopping  **X** permet d'arreter l'apprentissage quand le modèle ne montre pas d'amélioration **X** fois de suite. Il est donc possible de donner un très grand nombre de train_steps et de donner un condition avec early_stopping pour obtenir le nobre optimal de train_steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTf5HRTGil0F"
      },
      "source": [
        "Ici, on va essayer avec 6000 train-steps. J'ai essayé de faire la traduction et l'obtention du score bleu dans le même bloc mais je n'ai pas pris en compte les models qui sont produits tous les 625 steps et donc il y a une erreur, mais l'entrainnement est fini, dans le bloc suivant je fais la traduction et je calcule le score bleu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ul0m4D35QfjB",
        "outputId": "d380a897-c6b5-4901-f0f0-e1c571d427b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 22:50:02,104 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-27 22:50:02,106 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 22:50:02,106 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-27 22:50:02,107 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-27 22:50:02,108 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-27 22:50:02,108 INFO] Loading vocab from text file...\n",
            "[2022-02-27 22:50:02,108 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-27 22:50:02,135 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-27 22:50:02,141 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-27 22:50:02,161 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-27 22:50:02,166 INFO] Building fields with vocab in counters...\n",
            "[2022-02-27 22:50:02,176 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-27 22:50:02,190 INFO]  * src vocab size: 9980.\n",
            "[2022-02-27 22:50:02,191 INFO]  * src vocab size = 9980\n",
            "[2022-02-27 22:50:02,191 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-27 22:50:02,195 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:50:02,399 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-27 22:50:02,400 INFO] encoder: 5635120\n",
            "[2022-02-27 22:50:02,400 INFO] decoder: 7244222\n",
            "[2022-02-27 22:50:02,401 INFO] * number of parameters: 12879342\n",
            "[2022-02-27 22:50:02,402 INFO] Starting training on CPU, could be very slow\n",
            "[2022-02-27 22:50:02,403 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-27 22:50:02,403 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 22:50:02,405 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "[2022-02-27 22:50:09,785 INFO] Step 20/ 6000; acc:   8.32; ppl: 2517.63; xent: 7.83; lr: 1.00000; 1043/1089 tok/s;      7 sec\n",
            "[2022-02-27 22:50:14,935 INFO] Step 40/ 6000; acc:  17.41; ppl: 354.97; xent: 5.87; lr: 1.00000; 1114/1207 tok/s;     13 sec\n",
            "[2022-02-27 22:50:19,851 INFO] Step 60/ 6000; acc:  20.29; ppl: 250.60; xent: 5.52; lr: 1.00000; 1162/1180 tok/s;     17 sec\n",
            "[2022-02-27 22:50:26,316 INFO] Step 80/ 6000; acc:  16.24; ppl: 300.25; xent: 5.70; lr: 1.00000; 1161/1197 tok/s;     24 sec\n",
            "[2022-02-27 22:50:31,889 INFO] Step 100/ 6000; acc:  20.81; ppl: 175.18; xent: 5.17; lr: 1.00000; 1194/1229 tok/s;     29 sec\n",
            "[2022-02-27 22:50:36,818 INFO] Step 120/ 6000; acc:  26.86; ppl: 123.11; xent: 4.81; lr: 1.00000; 1147/1215 tok/s;     34 sec\n",
            "[2022-02-27 22:50:42,908 INFO] Step 140/ 6000; acc:  24.22; ppl: 167.81; xent: 5.12; lr: 1.00000; 1227/1219 tok/s;     41 sec\n",
            "[2022-02-27 22:50:48,674 INFO] Step 160/ 6000; acc:  26.09; ppl: 125.50; xent: 4.83; lr: 1.00000; 1211/1248 tok/s;     46 sec\n",
            "[2022-02-27 22:50:53,494 INFO] Step 180/ 6000; acc:  32.89; ppl: 80.01; xent: 4.38; lr: 1.00000; 1137/1248 tok/s;     51 sec\n",
            "[2022-02-27 22:50:59,232 INFO] Step 200/ 6000; acc:  29.32; ppl: 107.02; xent: 4.67; lr: 1.00000; 1079/1149 tok/s;     57 sec\n",
            "[2022-02-27 22:51:05,065 INFO] Step 220/ 6000; acc:  29.26; ppl: 92.41; xent: 4.53; lr: 1.00000; 1196/1233 tok/s;     63 sec\n",
            "[2022-02-27 22:51:10,427 INFO] Step 240/ 6000; acc:  35.57; ppl: 66.07; xent: 4.19; lr: 1.00000; 1093/1191 tok/s;     68 sec\n",
            "[2022-02-27 22:51:15,619 INFO] Step 260/ 6000; acc:  34.61; ppl: 65.80; xent: 4.19; lr: 1.00000; 1105/1176 tok/s;     73 sec\n",
            "[2022-02-27 22:51:22,253 INFO] Step 280/ 6000; acc:  29.18; ppl: 99.80; xent: 4.60; lr: 1.00000; 1109/1159 tok/s;     80 sec\n",
            "[2022-02-27 22:51:27,725 INFO] Step 300/ 6000; acc:  37.29; ppl: 58.12; xent: 4.06; lr: 1.00000; 1112/1194 tok/s;     85 sec\n",
            "[2022-02-27 22:51:32,700 INFO] Step 320/ 6000; acc:  38.16; ppl: 55.01; xent: 4.01; lr: 1.00000; 1129/1171 tok/s;     90 sec\n",
            "[2022-02-27 22:51:39,306 INFO] Step 340/ 6000; acc:  30.48; ppl: 91.77; xent: 4.52; lr: 1.00000; 1194/1159 tok/s;     97 sec\n",
            "[2022-02-27 22:51:44,351 INFO] Step 360/ 6000; acc:  39.10; ppl: 45.52; xent: 3.82; lr: 1.00000; 1167/1202 tok/s;    102 sec\n",
            "[2022-02-27 22:51:49,391 INFO] Step 380/ 6000; acc:  39.19; ppl: 46.98; xent: 3.85; lr: 1.00000; 1141/1212 tok/s;    107 sec\n",
            "[2022-02-27 22:51:55,756 INFO] Step 400/ 6000; acc:  31.79; ppl: 81.63; xent: 4.40; lr: 1.00000; 1189/1189 tok/s;    113 sec\n",
            "[2022-02-27 22:52:01,135 INFO] Step 420/ 6000; acc:  36.64; ppl: 53.83; xent: 3.99; lr: 1.00000; 1251/1228 tok/s;    119 sec\n",
            "[2022-02-27 22:52:06,161 INFO] Step 440/ 6000; acc:  40.67; ppl: 41.57; xent: 3.73; lr: 1.00000; 1112/1210 tok/s;    124 sec\n",
            "[2022-02-27 22:52:12,871 INFO] Step 460/ 6000; acc:  33.72; ppl: 65.27; xent: 4.18; lr: 1.00000; 1103/1117 tok/s;    130 sec\n",
            "[2022-02-27 22:52:19,038 INFO] Step 480/ 6000; acc:  36.56; ppl: 49.95; xent: 3.91; lr: 1.00000; 1125/1162 tok/s;    137 sec\n",
            "[2022-02-27 22:52:24,056 INFO] Step 500/ 6000; acc:  41.79; ppl: 37.37; xent: 3.62; lr: 1.00000; 1085/1178 tok/s;    142 sec\n",
            "[2022-02-27 22:52:30,022 INFO] Step 520/ 6000; acc:  38.72; ppl: 54.75; xent: 4.00; lr: 1.00000; 1046/1094 tok/s;    148 sec\n",
            "[2022-02-27 22:52:35,988 INFO] Step 540/ 6000; acc:  37.22; ppl: 50.28; xent: 3.92; lr: 1.00000; 1193/1232 tok/s;    154 sec\n",
            "[2022-02-27 22:52:41,333 INFO] Step 560/ 6000; acc:  42.55; ppl: 34.98; xent: 3.55; lr: 1.00000; 1102/1196 tok/s;    159 sec\n",
            "[2022-02-27 22:52:45,471 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-27 22:52:46,592 INFO] Step 580/ 6000; acc:  41.77; ppl: 39.68; xent: 3.68; lr: 1.00000; 1105/1153 tok/s;    164 sec\n",
            "[2022-02-27 22:52:53,398 INFO] Step 600/ 6000; acc:  35.15; ppl: 56.38; xent: 4.03; lr: 1.00000; 1070/1133 tok/s;    171 sec\n",
            "[2022-02-27 22:52:58,812 INFO] Step 620/ 6000; acc:  42.72; ppl: 35.20; xent: 3.56; lr: 1.00000; 1129/1185 tok/s;    176 sec\n",
            "[2022-02-27 22:52:59,997 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-27 22:53:01,560 INFO] Validation perplexity: 22.0608\n",
            "[2022-02-27 22:53:01,560 INFO] Validation accuracy: 49.2814\n",
            "[2022-02-27 22:53:01,561 INFO] Model is improving ppl: inf --> 22.0608.\n",
            "[2022-02-27 22:53:01,561 INFO] Model is improving acc: -inf --> 49.2814.\n",
            "[2022-02-27 22:53:01,618 INFO] Saving checkpoint ./models/6000step/model_step_625.pt\n",
            "[2022-02-27 22:53:05,847 INFO] Step 640/ 6000; acc:  42.88; ppl: 35.17; xent: 3.56; lr: 1.00000; 790/836 tok/s;    183 sec\n",
            "[2022-02-27 22:53:13,518 INFO] Step 660/ 6000; acc:  36.45; ppl: 50.93; xent: 3.93; lr: 1.00000; 994/1012 tok/s;    191 sec\n",
            "[2022-02-27 22:53:18,784 INFO] Step 680/ 6000; acc:  43.77; ppl: 30.19; xent: 3.41; lr: 1.00000; 1087/1181 tok/s;    196 sec\n",
            "[2022-02-27 22:53:23,826 INFO] Step 700/ 6000; acc:  43.28; ppl: 32.04; xent: 3.47; lr: 1.00000; 1130/1158 tok/s;    201 sec\n",
            "[2022-02-27 22:53:30,721 INFO] Step 720/ 6000; acc:  37.17; ppl: 47.93; xent: 3.87; lr: 1.00000; 1101/1135 tok/s;    208 sec\n",
            "[2022-02-27 22:53:36,491 INFO] Step 740/ 6000; acc:  41.88; ppl: 36.10; xent: 3.59; lr: 1.00000; 1170/1192 tok/s;    214 sec\n",
            "[2022-02-27 22:53:41,663 INFO] Step 760/ 6000; acc:  45.92; ppl: 27.85; xent: 3.33; lr: 1.00000; 1099/1164 tok/s;    219 sec\n",
            "[2022-02-27 22:53:48,123 INFO] Step 780/ 6000; acc:  38.72; ppl: 44.77; xent: 3.80; lr: 1.00000; 1140/1150 tok/s;    226 sec\n",
            "[2022-02-27 22:53:54,042 INFO] Step 800/ 6000; acc:  42.45; ppl: 33.55; xent: 3.51; lr: 1.00000; 1154/1215 tok/s;    232 sec\n",
            "[2022-02-27 22:53:59,082 INFO] Step 820/ 6000; acc:  46.51; ppl: 25.92; xent: 3.25; lr: 1.00000; 1062/1201 tok/s;    237 sec\n",
            "[2022-02-27 22:54:05,276 INFO] Step 840/ 6000; acc:  44.56; ppl: 31.04; xent: 3.44; lr: 1.00000; 996/1042 tok/s;    243 sec\n",
            "[2022-02-27 22:54:11,280 INFO] Step 860/ 6000; acc:  42.21; ppl: 33.58; xent: 3.51; lr: 1.00000; 1168/1212 tok/s;    249 sec\n",
            "[2022-02-27 22:54:16,748 INFO] Step 880/ 6000; acc:  46.81; ppl: 25.57; xent: 3.24; lr: 1.00000; 1086/1153 tok/s;    254 sec\n",
            "[2022-02-27 22:54:22,089 INFO] Step 900/ 6000; acc:  46.18; ppl: 26.30; xent: 3.27; lr: 1.00000; 1088/1133 tok/s;    260 sec\n",
            "[2022-02-27 22:54:28,910 INFO] Step 920/ 6000; acc:  40.38; ppl: 38.60; xent: 3.65; lr: 1.00000; 1097/1147 tok/s;    267 sec\n",
            "[2022-02-27 22:54:34,486 INFO] Step 940/ 6000; acc:  44.85; ppl: 27.17; xent: 3.30; lr: 1.00000; 1103/1192 tok/s;    272 sec\n",
            "[2022-02-27 22:54:39,451 INFO] Step 960/ 6000; acc:  48.15; ppl: 24.92; xent: 3.22; lr: 1.00000; 1147/1161 tok/s;    277 sec\n",
            "[2022-02-27 22:54:46,443 INFO] Step 980/ 6000; acc:  40.18; ppl: 36.72; xent: 3.60; lr: 1.00000; 1103/1070 tok/s;    284 sec\n",
            "[2022-02-27 22:54:51,798 INFO] Step 1000/ 6000; acc:  48.13; ppl: 22.10; xent: 3.10; lr: 1.00000; 1084/1138 tok/s;    289 sec\n",
            "[2022-02-27 22:54:56,785 INFO] Step 1020/ 6000; acc:  47.60; ppl: 22.94; xent: 3.13; lr: 1.00000; 1145/1210 tok/s;    294 sec\n",
            "[2022-02-27 22:55:03,719 INFO] Step 1040/ 6000; acc:  40.01; ppl: 39.89; xent: 3.69; lr: 1.00000; 1113/1111 tok/s;    301 sec\n",
            "[2022-02-27 22:55:09,400 INFO] Step 1060/ 6000; acc:  44.80; ppl: 26.50; xent: 3.28; lr: 1.00000; 1197/1205 tok/s;    307 sec\n",
            "[2022-02-27 22:55:14,533 INFO] Step 1080/ 6000; acc:  48.82; ppl: 21.50; xent: 3.07; lr: 1.00000; 1105/1165 tok/s;    312 sec\n",
            "[2022-02-27 22:55:21,022 INFO] Step 1100/ 6000; acc:  41.06; ppl: 33.57; xent: 3.51; lr: 1.00000; 1135/1138 tok/s;    319 sec\n",
            "[2022-02-27 22:55:27,183 INFO] Step 1120/ 6000; acc:  44.54; ppl: 26.82; xent: 3.29; lr: 1.00000; 1120/1178 tok/s;    325 sec\n",
            "[2022-02-27 22:55:31,996 INFO] Step 1140/ 6000; acc:  48.19; ppl: 21.46; xent: 3.07; lr: 1.00000; 1126/1259 tok/s;    330 sec\n",
            "[2022-02-27 22:55:37,908 INFO] Step 1160/ 6000; acc:  45.24; ppl: 28.79; xent: 3.36; lr: 1.00000; 1059/1076 tok/s;    336 sec\n",
            "[2022-02-27 22:55:43,943 INFO] Step 1180/ 6000; acc:  43.94; ppl: 27.73; xent: 3.32; lr: 1.00000; 1168/1201 tok/s;    342 sec\n",
            "[2022-02-27 22:55:49,081 INFO] Step 1200/ 6000; acc:  49.25; ppl: 19.82; xent: 2.99; lr: 1.00000; 1133/1241 tok/s;    347 sec\n",
            "[2022-02-27 22:55:53,281 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-27 22:55:54,402 INFO] Step 1220/ 6000; acc:  48.89; ppl: 20.57; xent: 3.02; lr: 1.00000; 1080/1138 tok/s;    352 sec\n",
            "[2022-02-27 22:56:01,216 INFO] Step 1240/ 6000; acc:  42.37; ppl: 32.21; xent: 3.47; lr: 1.00000; 1065/1130 tok/s;    359 sec\n",
            "[2022-02-27 22:56:05,585 INFO] Validation perplexity: 13.0911\n",
            "[2022-02-27 22:56:05,585 INFO] Validation accuracy: 55.3083\n",
            "[2022-02-27 22:56:05,585 INFO] Model is improving ppl: 22.0608 --> 13.0911.\n",
            "[2022-02-27 22:56:05,585 INFO] Model is improving acc: 49.2814 --> 55.3083.\n",
            "[2022-02-27 22:56:05,642 INFO] Saving checkpoint ./models/6000step/model_step_1250.pt\n",
            "[2022-02-27 22:56:08,599 INFO] Step 1260/ 6000; acc:  48.98; ppl: 21.59; xent: 3.07; lr: 1.00000; 836/881 tok/s;    366 sec\n",
            "[2022-02-27 22:56:13,895 INFO] Step 1280/ 6000; acc:  47.88; ppl: 21.52; xent: 3.07; lr: 1.00000; 1062/1115 tok/s;    371 sec\n",
            "[2022-02-27 22:56:21,386 INFO] Step 1300/ 6000; acc:  42.22; ppl: 29.80; xent: 3.39; lr: 1.00000; 1033/1027 tok/s;    379 sec\n",
            "[2022-02-27 22:56:26,774 INFO] Step 1320/ 6000; acc:  49.89; ppl: 18.48; xent: 2.92; lr: 1.00000; 1073/1167 tok/s;    384 sec\n",
            "[2022-02-27 22:56:31,828 INFO] Step 1340/ 6000; acc:  50.79; ppl: 18.33; xent: 2.91; lr: 1.00000; 1136/1184 tok/s;    389 sec\n",
            "[2022-02-27 22:56:38,919 INFO] Step 1360/ 6000; acc:  43.60; ppl: 28.56; xent: 3.35; lr: 1.00000; 1078/1087 tok/s;    397 sec\n",
            "[2022-02-27 22:56:44,687 INFO] Step 1380/ 6000; acc:  47.81; ppl: 21.53; xent: 3.07; lr: 1.00000; 1163/1197 tok/s;    402 sec\n",
            "[2022-02-27 22:56:49,957 INFO] Step 1400/ 6000; acc:  50.07; ppl: 19.86; xent: 2.99; lr: 1.00000; 1069/1150 tok/s;    408 sec\n",
            "[2022-02-27 22:56:56,363 INFO] Step 1420/ 6000; acc:  44.31; ppl: 27.60; xent: 3.32; lr: 1.00000; 1126/1158 tok/s;    414 sec\n",
            "[2022-02-27 22:57:02,259 INFO] Step 1440/ 6000; acc:  48.24; ppl: 19.79; xent: 2.98; lr: 1.00000; 1137/1188 tok/s;    420 sec\n",
            "[2022-02-27 22:57:07,134 INFO] Step 1460/ 6000; acc:  52.32; ppl: 16.02; xent: 2.77; lr: 1.00000; 1083/1208 tok/s;    425 sec\n",
            "[2022-02-27 22:57:13,272 INFO] Step 1480/ 6000; acc:  47.94; ppl: 21.95; xent: 3.09; lr: 1.00000; 994/1061 tok/s;    431 sec\n",
            "[2022-02-27 22:57:19,311 INFO] Step 1500/ 6000; acc:  47.11; ppl: 22.27; xent: 3.10; lr: 1.00000; 1172/1217 tok/s;    437 sec\n",
            "[2022-02-27 22:57:24,635 INFO] Step 1520/ 6000; acc:  51.61; ppl: 16.09; xent: 2.78; lr: 1.00000; 1123/1206 tok/s;    442 sec\n",
            "[2022-02-27 22:57:29,864 INFO] Step 1540/ 6000; acc:  50.72; ppl: 17.74; xent: 2.88; lr: 1.00000; 1126/1133 tok/s;    447 sec\n",
            "[2022-02-27 22:57:36,637 INFO] Step 1560/ 6000; acc:  44.40; ppl: 26.73; xent: 3.29; lr: 1.00000; 1119/1138 tok/s;    454 sec\n",
            "[2022-02-27 22:57:42,130 INFO] Step 1580/ 6000; acc:  50.50; ppl: 16.56; xent: 2.81; lr: 1.00000; 1132/1212 tok/s;    460 sec\n",
            "[2022-02-27 22:57:47,059 INFO] Step 1600/ 6000; acc:  50.85; ppl: 16.90; xent: 2.83; lr: 1.00000; 1161/1214 tok/s;    465 sec\n",
            "[2022-02-27 22:57:53,945 INFO] Step 1620/ 6000; acc:  44.28; ppl: 26.57; xent: 3.28; lr: 1.00000; 1134/1112 tok/s;    472 sec\n",
            "[2022-02-27 22:57:59,125 INFO] Step 1640/ 6000; acc:  51.74; ppl: 15.77; xent: 2.76; lr: 1.00000; 1124/1170 tok/s;    477 sec\n",
            "[2022-02-27 22:58:04,090 INFO] Step 1660/ 6000; acc:  51.79; ppl: 15.34; xent: 2.73; lr: 1.00000; 1152/1212 tok/s;    482 sec\n",
            "[2022-02-27 22:58:10,985 INFO] Step 1680/ 6000; acc:  44.58; ppl: 27.03; xent: 3.30; lr: 1.00000; 1102/1099 tok/s;    489 sec\n",
            "[2022-02-27 22:58:16,780 INFO] Step 1700/ 6000; acc:  49.78; ppl: 18.14; xent: 2.90; lr: 1.00000; 1160/1150 tok/s;    494 sec\n",
            "[2022-02-27 22:58:21,868 INFO] Step 1720/ 6000; acc:  52.57; ppl: 14.89; xent: 2.70; lr: 1.00000; 1094/1198 tok/s;    499 sec\n",
            "[2022-02-27 22:58:28,594 INFO] Step 1740/ 6000; acc:  45.83; ppl: 22.60; xent: 3.12; lr: 1.00000; 1094/1113 tok/s;    506 sec\n",
            "[2022-02-27 22:58:34,585 INFO] Step 1760/ 6000; acc:  49.50; ppl: 17.52; xent: 2.86; lr: 1.00000; 1145/1210 tok/s;    512 sec\n",
            "[2022-02-27 22:58:39,428 INFO] Step 1780/ 6000; acc:  53.66; ppl: 14.11; xent: 2.65; lr: 1.00000; 1117/1227 tok/s;    517 sec\n",
            "[2022-02-27 22:58:45,535 INFO] Step 1800/ 6000; acc:  49.91; ppl: 20.02; xent: 3.00; lr: 1.00000; 1013/1046 tok/s;    523 sec\n",
            "[2022-02-27 22:58:51,522 INFO] Step 1820/ 6000; acc:  47.90; ppl: 18.93; xent: 2.94; lr: 1.00000; 1169/1224 tok/s;    529 sec\n",
            "[2022-02-27 22:58:56,582 INFO] Step 1840/ 6000; acc:  52.70; ppl: 14.60; xent: 2.68; lr: 1.00000; 1151/1242 tok/s;    534 sec\n",
            "[2022-02-27 22:59:00,769 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-27 22:59:01,912 INFO] Step 1860/ 6000; acc:  52.70; ppl: 14.35; xent: 2.66; lr: 1.00000; 1080/1129 tok/s;    540 sec\n",
            "[2022-02-27 22:59:09,075 INFO] Validation perplexity: 10.0799\n",
            "[2022-02-27 22:59:09,076 INFO] Validation accuracy: 59.0172\n",
            "[2022-02-27 22:59:09,076 INFO] Model is improving ppl: 13.0911 --> 10.0799.\n",
            "[2022-02-27 22:59:09,076 INFO] Model is improving acc: 55.3083 --> 59.0172.\n",
            "[2022-02-27 22:59:09,134 INFO] Saving checkpoint ./models/6000step/model_step_1875.pt\n",
            "[2022-02-27 22:59:10,798 INFO] Step 1880/ 6000; acc:  45.30; ppl: 23.81; xent: 3.17; lr: 1.00000; 830/876 tok/s;    548 sec\n",
            "[2022-02-27 22:59:16,514 INFO] Step 1900/ 6000; acc:  52.37; ppl: 15.45; xent: 2.74; lr: 1.00000; 1083/1160 tok/s;    554 sec\n",
            "[2022-02-27 22:59:21,782 INFO] Step 1920/ 6000; acc:  52.78; ppl: 14.33; xent: 2.66; lr: 1.00000; 1076/1109 tok/s;    559 sec\n",
            "[2022-02-27 22:59:29,268 INFO] Step 1940/ 6000; acc:  46.35; ppl: 21.61; xent: 3.07; lr: 1.00000; 1039/1035 tok/s;    567 sec\n",
            "[2022-02-27 22:59:34,551 INFO] Step 1960/ 6000; acc:  53.99; ppl: 13.41; xent: 2.60; lr: 1.00000; 1102/1191 tok/s;    572 sec\n",
            "[2022-02-27 22:59:39,548 INFO] Step 1980/ 6000; acc:  54.09; ppl: 13.67; xent: 2.62; lr: 1.00000; 1155/1206 tok/s;    577 sec\n",
            "[2022-02-27 22:59:46,313 INFO] Step 2000/ 6000; acc:  46.62; ppl: 22.08; xent: 3.09; lr: 1.00000; 1130/1139 tok/s;    584 sec\n",
            "[2022-02-27 22:59:52,160 INFO] Step 2020/ 6000; acc:  51.02; ppl: 16.41; xent: 2.80; lr: 1.00000; 1153/1186 tok/s;    590 sec\n",
            "[2022-02-27 22:59:57,351 INFO] Step 2040/ 6000; acc:  54.27; ppl: 14.12; xent: 2.65; lr: 1.00000; 1091/1174 tok/s;    595 sec\n",
            "[2022-02-27 23:00:03,733 INFO] Step 2060/ 6000; acc:  48.00; ppl: 19.86; xent: 2.99; lr: 1.00000; 1116/1162 tok/s;    601 sec\n",
            "[2022-02-27 23:00:09,568 INFO] Step 2080/ 6000; acc:  51.06; ppl: 15.22; xent: 2.72; lr: 1.00000; 1142/1200 tok/s;    607 sec\n",
            "[2022-02-27 23:00:14,487 INFO] Step 2100/ 6000; acc:  57.23; ppl: 11.22; xent: 2.42; lr: 1.00000; 1079/1172 tok/s;    612 sec\n",
            "[2022-02-27 23:00:20,644 INFO] Step 2120/ 6000; acc:  50.64; ppl: 16.63; xent: 2.81; lr: 1.00000; 994/1083 tok/s;    618 sec\n",
            "[2022-02-27 23:00:26,672 INFO] Step 2140/ 6000; acc:  50.20; ppl: 16.09; xent: 2.78; lr: 1.00000; 1164/1200 tok/s;    624 sec\n",
            "[2022-02-27 23:00:31,959 INFO] Step 2160/ 6000; acc:  54.90; ppl: 13.28; xent: 2.59; lr: 1.00000; 1107/1202 tok/s;    630 sec\n",
            "[2022-02-27 23:00:37,926 INFO] Step 2180/ 6000; acc:  54.99; ppl: 12.14; xent: 2.50; lr: 1.00000; 978/996 tok/s;    636 sec\n",
            "[2022-02-27 23:00:44,838 INFO] Step 2200/ 6000; acc:  46.31; ppl: 21.48; xent: 3.07; lr: 1.00000; 1118/1091 tok/s;    642 sec\n",
            "[2022-02-27 23:00:50,412 INFO] Step 2220/ 6000; acc:  54.24; ppl: 12.26; xent: 2.51; lr: 1.00000; 1144/1173 tok/s;    648 sec\n",
            "[2022-02-27 23:00:55,698 INFO] Step 2240/ 6000; acc:  53.98; ppl: 12.68; xent: 2.54; lr: 1.00000; 1090/1184 tok/s;    653 sec\n",
            "[2022-02-27 23:01:02,540 INFO] Step 2260/ 6000; acc:  47.80; ppl: 19.58; xent: 2.97; lr: 1.00000; 1136/1114 tok/s;    660 sec\n",
            "[2022-02-27 23:01:07,782 INFO] Step 2280/ 6000; acc:  56.10; ppl: 11.46; xent: 2.44; lr: 1.00000; 1107/1160 tok/s;    665 sec\n",
            "[2022-02-27 23:01:12,739 INFO] Step 2300/ 6000; acc:  55.62; ppl: 11.76; xent: 2.46; lr: 1.00000; 1152/1204 tok/s;    670 sec\n",
            "[2022-02-27 23:01:19,813 INFO] Step 2320/ 6000; acc:  46.99; ppl: 21.29; xent: 3.06; lr: 1.00000; 1076/1073 tok/s;    677 sec\n",
            "[2022-02-27 23:01:25,621 INFO] Step 2340/ 6000; acc:  52.44; ppl: 13.63; xent: 2.61; lr: 1.00000; 1163/1163 tok/s;    683 sec\n",
            "[2022-02-27 23:01:30,691 INFO] Step 2360/ 6000; acc:  54.32; ppl: 11.83; xent: 2.47; lr: 1.00000; 1102/1236 tok/s;    688 sec\n",
            "[2022-02-27 23:01:37,257 INFO] Step 2380/ 6000; acc:  48.82; ppl: 17.91; xent: 2.89; lr: 1.00000; 1114/1128 tok/s;    695 sec\n",
            "[2022-02-27 23:01:43,101 INFO] Step 2400/ 6000; acc:  52.45; ppl: 13.77; xent: 2.62; lr: 1.00000; 1160/1191 tok/s;    701 sec\n",
            "[2022-02-27 23:01:48,023 INFO] Step 2420/ 6000; acc:  57.10; ppl: 10.16; xent: 2.32; lr: 1.00000; 1079/1202 tok/s;    706 sec\n",
            "[2022-02-27 23:01:54,145 INFO] Step 2440/ 6000; acc:  51.85; ppl: 15.42; xent: 2.74; lr: 1.00000; 1004/1071 tok/s;    712 sec\n",
            "[2022-02-27 23:02:00,258 INFO] Step 2460/ 6000; acc:  51.33; ppl: 14.45; xent: 2.67; lr: 1.00000; 1149/1196 tok/s;    718 sec\n",
            "[2022-02-27 23:02:05,487 INFO] Step 2480/ 6000; acc:  55.74; ppl: 11.27; xent: 2.42; lr: 1.00000; 1119/1209 tok/s;    723 sec\n",
            "[2022-02-27 23:02:09,560 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 5\n",
            "[2022-02-27 23:02:10,779 INFO] Step 2500/ 6000; acc:  55.59; ppl: 11.15; xent: 2.41; lr: 1.00000; 1093/1144 tok/s;    728 sec\n",
            "[2022-02-27 23:02:12,341 INFO] Validation perplexity: 8.24879\n",
            "[2022-02-27 23:02:12,341 INFO] Validation accuracy: 62.1929\n",
            "[2022-02-27 23:02:12,341 INFO] Model is improving ppl: 10.0799 --> 8.24879.\n",
            "[2022-02-27 23:02:12,341 INFO] Model is improving acc: 59.0172 --> 62.1929.\n",
            "[2022-02-27 23:02:12,409 INFO] Saving checkpoint ./models/6000step/model_step_2500.pt\n",
            "[2022-02-27 23:02:20,186 INFO] Step 2520/ 6000; acc:  47.75; ppl: 18.18; xent: 2.90; lr: 1.00000; 796/842 tok/s;    738 sec\n",
            "[2022-02-27 23:02:25,969 INFO] Step 2540/ 6000; acc:  54.94; ppl: 12.04; xent: 2.49; lr: 1.00000; 1076/1127 tok/s;    744 sec\n",
            "[2022-02-27 23:02:31,034 INFO] Step 2560/ 6000; acc:  55.48; ppl: 11.52; xent: 2.44; lr: 1.00000; 1125/1161 tok/s;    749 sec\n",
            "[2022-02-27 23:02:38,013 INFO] Step 2580/ 6000; acc:  48.75; ppl: 18.09; xent: 2.90; lr: 1.00000; 1113/1110 tok/s;    756 sec\n",
            "[2022-02-27 23:02:43,394 INFO] Step 2600/ 6000; acc:  56.39; ppl: 10.86; xent: 2.38; lr: 1.00000; 1091/1187 tok/s;    761 sec\n",
            "[2022-02-27 23:02:48,522 INFO] Step 2620/ 6000; acc:  57.45; ppl: 10.78; xent: 2.38; lr: 1.00000; 1136/1166 tok/s;    766 sec\n",
            "[2022-02-27 23:02:55,044 INFO] Step 2640/ 6000; acc:  49.81; ppl: 16.67; xent: 2.81; lr: 1.00000; 1152/1168 tok/s;    773 sec\n",
            "[2022-02-27 23:03:00,939 INFO] Step 2660/ 6000; acc:  54.73; ppl: 12.38; xent: 2.52; lr: 1.00000; 1121/1155 tok/s;    779 sec\n",
            "[2022-02-27 23:03:06,041 INFO] Step 2680/ 6000; acc:  56.40; ppl: 10.73; xent: 2.37; lr: 1.00000; 1086/1205 tok/s;    784 sec\n",
            "[2022-02-27 23:03:12,641 INFO] Step 2700/ 6000; acc:  50.78; ppl: 15.22; xent: 2.72; lr: 1.00000; 1085/1127 tok/s;    790 sec\n",
            "[2022-02-27 23:03:18,481 INFO] Step 2720/ 6000; acc:  54.17; ppl: 12.37; xent: 2.52; lr: 1.00000; 1145/1196 tok/s;    796 sec\n",
            "[2022-02-27 23:03:23,508 INFO] Step 2740/ 6000; acc:  59.12; ppl:  9.35; xent: 2.24; lr: 1.00000; 1060/1159 tok/s;    801 sec\n",
            "[2022-02-27 23:03:29,503 INFO] Step 2760/ 6000; acc:  53.62; ppl: 12.97; xent: 2.56; lr: 1.00000; 1032/1093 tok/s;    807 sec\n",
            "[2022-02-27 23:03:35,537 INFO] Step 2780/ 6000; acc:  52.42; ppl: 13.26; xent: 2.59; lr: 1.00000; 1173/1217 tok/s;    813 sec\n",
            "[2022-02-27 23:03:40,871 INFO] Step 2800/ 6000; acc:  57.82; ppl: 10.21; xent: 2.32; lr: 1.00000; 1106/1193 tok/s;    818 sec\n",
            "[2022-02-27 23:03:46,072 INFO] Step 2820/ 6000; acc:  57.00; ppl: 10.69; xent: 2.37; lr: 1.00000; 1129/1148 tok/s;    824 sec\n",
            "[2022-02-27 23:03:52,700 INFO] Step 2840/ 6000; acc:  49.60; ppl: 16.76; xent: 2.82; lr: 1.00000; 1161/1136 tok/s;    830 sec\n",
            "[2022-02-27 23:03:58,329 INFO] Step 2860/ 6000; acc:  57.11; ppl:  9.62; xent: 2.26; lr: 1.00000; 1126/1150 tok/s;    836 sec\n",
            "[2022-02-27 23:04:03,471 INFO] Step 2880/ 6000; acc:  56.74; ppl:  9.95; xent: 2.30; lr: 1.00000; 1114/1196 tok/s;    841 sec\n",
            "[2022-02-27 23:04:10,229 INFO] Step 2900/ 6000; acc:  49.19; ppl: 17.17; xent: 2.84; lr: 1.00000; 1171/1135 tok/s;    848 sec\n",
            "[2022-02-27 23:04:15,403 INFO] Step 2920/ 6000; acc:  58.24; ppl:  9.02; xent: 2.20; lr: 1.00000; 1138/1185 tok/s;    853 sec\n",
            "[2022-02-27 23:04:20,672 INFO] Step 2940/ 6000; acc:  57.65; ppl:  9.81; xent: 2.28; lr: 1.00000; 1094/1153 tok/s;    858 sec\n",
            "[2022-02-27 23:04:27,714 INFO] Step 2960/ 6000; acc:  50.48; ppl: 14.43; xent: 2.67; lr: 1.00000; 1061/1079 tok/s;    865 sec\n",
            "[2022-02-27 23:04:33,496 INFO] Step 2980/ 6000; acc:  54.98; ppl: 10.76; xent: 2.38; lr: 1.00000; 1127/1208 tok/s;    871 sec\n",
            "[2022-02-27 23:04:38,422 INFO] Step 3000/ 6000; acc:  57.63; ppl: 10.01; xent: 2.30; lr: 1.00000; 1122/1181 tok/s;    876 sec\n",
            "[2022-02-27 23:04:45,020 INFO] Step 3020/ 6000; acc:  50.89; ppl: 15.26; xent: 2.73; lr: 1.00000; 1099/1129 tok/s;    883 sec\n",
            "[2022-02-27 23:04:50,713 INFO] Step 3040/ 6000; acc:  54.63; ppl: 10.98; xent: 2.40; lr: 1.00000; 1190/1232 tok/s;    888 sec\n",
            "[2022-02-27 23:04:55,685 INFO] Step 3060/ 6000; acc:  61.24; ppl:  7.89; xent: 2.07; lr: 1.00000; 1074/1187 tok/s;    893 sec\n",
            "[2022-02-27 23:04:58,982 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 6\n",
            "[2022-02-27 23:05:01,773 INFO] Step 3080/ 6000; acc:  53.17; ppl: 12.90; xent: 2.56; lr: 1.00000; 1018/1061 tok/s;    899 sec\n",
            "[2022-02-27 23:05:07,972 INFO] Step 3100/ 6000; acc:  53.24; ppl: 12.30; xent: 2.51; lr: 1.00000; 1141/1212 tok/s;    906 sec\n",
            "[2022-02-27 23:05:13,245 INFO] Step 3120/ 6000; acc:  58.42; ppl:  9.09; xent: 2.21; lr: 1.00000; 1119/1191 tok/s;    911 sec\n",
            "[2022-02-27 23:05:16,003 INFO] Validation perplexity: 7.2556\n",
            "[2022-02-27 23:05:16,004 INFO] Validation accuracy: 64.0705\n",
            "[2022-02-27 23:05:16,004 INFO] Model is improving ppl: 8.24879 --> 7.2556.\n",
            "[2022-02-27 23:05:16,004 INFO] Model is improving acc: 62.1929 --> 64.0705.\n",
            "[2022-02-27 23:05:16,062 INFO] Saving checkpoint ./models/6000step/model_step_3125.pt\n",
            "[2022-02-27 23:05:20,507 INFO] Step 3140/ 6000; acc:  57.93; ppl:  9.46; xent: 2.25; lr: 1.00000; 802/819 tok/s;    918 sec\n",
            "[2022-02-27 23:05:27,918 INFO] Step 3160/ 6000; acc:  51.60; ppl: 13.57; xent: 2.61; lr: 1.00000; 998/1052 tok/s;    926 sec\n",
            "[2022-02-27 23:05:33,386 INFO] Step 3180/ 6000; acc:  56.79; ppl:  9.63; xent: 2.27; lr: 1.00000; 1132/1199 tok/s;    931 sec\n",
            "[2022-02-27 23:05:38,422 INFO] Step 3200/ 6000; acc:  58.81; ppl:  9.27; xent: 2.23; lr: 1.00000; 1128/1163 tok/s;    936 sec\n",
            "[2022-02-27 23:05:45,337 INFO] Step 3220/ 6000; acc:  49.60; ppl: 15.55; xent: 2.74; lr: 1.00000; 1147/1140 tok/s;    943 sec\n",
            "[2022-02-27 23:05:50,695 INFO] Step 3240/ 6000; acc:  57.77; ppl:  9.21; xent: 2.22; lr: 1.00000; 1104/1184 tok/s;    948 sec\n",
            "[2022-02-27 23:05:55,896 INFO] Step 3260/ 6000; acc:  59.44; ppl:  9.05; xent: 2.20; lr: 1.00000; 1123/1169 tok/s;    953 sec\n",
            "[2022-02-27 23:06:02,365 INFO] Step 3280/ 6000; acc:  52.10; ppl: 13.97; xent: 2.64; lr: 1.00000; 1148/1150 tok/s;    960 sec\n",
            "[2022-02-27 23:06:08,151 INFO] Step 3300/ 6000; acc:  56.12; ppl:  9.95; xent: 2.30; lr: 1.00000; 1112/1216 tok/s;    966 sec\n",
            "[2022-02-27 23:06:13,227 INFO] Step 3320/ 6000; acc:  60.19; ppl:  8.46; xent: 2.13; lr: 1.00000; 1072/1189 tok/s;    971 sec\n",
            "[2022-02-27 23:06:20,016 INFO] Step 3340/ 6000; acc:  52.75; ppl: 12.42; xent: 2.52; lr: 1.00000; 1054/1077 tok/s;    978 sec\n",
            "[2022-02-27 23:06:25,769 INFO] Step 3360/ 6000; acc:  56.60; ppl:  9.93; xent: 2.30; lr: 1.00000; 1169/1225 tok/s;    983 sec\n",
            "[2022-02-27 23:06:30,637 INFO] Step 3380/ 6000; acc:  62.14; ppl:  7.27; xent: 1.98; lr: 1.00000; 1107/1184 tok/s;    988 sec\n",
            "[2022-02-27 23:06:36,683 INFO] Step 3400/ 6000; acc:  55.35; ppl: 11.07; xent: 2.40; lr: 1.00000; 1041/1087 tok/s;    994 sec\n",
            "[2022-02-27 23:06:42,793 INFO] Step 3420/ 6000; acc:  54.73; ppl: 10.85; xent: 2.38; lr: 1.00000; 1171/1222 tok/s;   1000 sec\n",
            "[2022-02-27 23:06:48,036 INFO] Step 3440/ 6000; acc:  59.13; ppl:  8.38; xent: 2.13; lr: 1.00000; 1139/1216 tok/s;   1006 sec\n",
            "[2022-02-27 23:06:53,305 INFO] Step 3460/ 6000; acc:  58.82; ppl:  8.79; xent: 2.17; lr: 1.00000; 1121/1128 tok/s;   1011 sec\n",
            "[2022-02-27 23:07:00,141 INFO] Step 3480/ 6000; acc:  51.48; ppl: 13.38; xent: 2.59; lr: 1.00000; 1100/1076 tok/s;   1018 sec\n",
            "[2022-02-27 23:07:05,776 INFO] Step 3500/ 6000; acc:  59.16; ppl:  8.18; xent: 2.10; lr: 1.00000; 1108/1160 tok/s;   1023 sec\n",
            "[2022-02-27 23:07:10,917 INFO] Step 3520/ 6000; acc:  59.69; ppl:  7.87; xent: 2.06; lr: 1.00000; 1108/1168 tok/s;   1029 sec\n",
            "[2022-02-27 23:07:17,830 INFO] Step 3540/ 6000; acc:  50.11; ppl: 15.43; xent: 2.74; lr: 1.00000; 1172/1147 tok/s;   1035 sec\n",
            "[2022-02-27 23:07:23,149 INFO] Step 3560/ 6000; acc:  59.81; ppl:  7.77; xent: 2.05; lr: 1.00000; 1122/1167 tok/s;   1041 sec\n",
            "[2022-02-27 23:07:28,363 INFO] Step 3580/ 6000; acc:  60.38; ppl:  7.82; xent: 2.06; lr: 1.00000; 1117/1178 tok/s;   1046 sec\n",
            "[2022-02-27 23:07:35,144 INFO] Step 3600/ 6000; acc:  52.35; ppl: 12.78; xent: 2.55; lr: 1.00000; 1093/1104 tok/s;   1053 sec\n",
            "[2022-02-27 23:07:41,112 INFO] Step 3620/ 6000; acc:  56.94; ppl:  8.94; xent: 2.19; lr: 1.00000; 1086/1182 tok/s;   1059 sec\n",
            "[2022-02-27 23:07:45,907 INFO] Step 3640/ 6000; acc:  59.72; ppl:  7.87; xent: 2.06; lr: 1.00000; 1147/1239 tok/s;   1064 sec\n",
            "[2022-02-27 23:07:52,578 INFO] Step 3660/ 6000; acc:  53.13; ppl: 12.83; xent: 2.55; lr: 1.00000; 1091/1084 tok/s;   1070 sec\n",
            "[2022-02-27 23:07:58,374 INFO] Step 3680/ 6000; acc:  57.04; ppl:  9.06; xent: 2.20; lr: 1.00000; 1159/1203 tok/s;   1076 sec\n",
            "[2022-02-27 23:08:03,168 INFO] Step 3700/ 6000; acc:  63.12; ppl:  6.53; xent: 1.88; lr: 1.00000; 1100/1225 tok/s;   1081 sec\n",
            "[2022-02-27 23:08:06,414 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 7\n",
            "[2022-02-27 23:08:09,145 INFO] Step 3720/ 6000; acc:  55.80; ppl: 10.31; xent: 2.33; lr: 1.00000; 1017/1073 tok/s;   1087 sec\n",
            "[2022-02-27 23:08:15,480 INFO] Step 3740/ 6000; acc:  55.02; ppl: 10.54; xent: 2.35; lr: 1.00000; 1125/1177 tok/s;   1093 sec\n",
            "[2022-02-27 23:08:19,767 INFO] Validation perplexity: 6.63977\n",
            "[2022-02-27 23:08:19,767 INFO] Validation accuracy: 65.809\n",
            "[2022-02-27 23:08:19,768 INFO] Model is improving ppl: 7.2556 --> 6.63977.\n",
            "[2022-02-27 23:08:19,768 INFO] Model is improving acc: 64.0705 --> 65.809.\n",
            "[2022-02-27 23:08:19,828 INFO] Saving checkpoint ./models/6000step/model_step_3750.pt\n",
            "[2022-02-27 23:08:22,921 INFO] Step 3760/ 6000; acc:  59.26; ppl:  8.24; xent: 2.11; lr: 1.00000; 799/855 tok/s;   1101 sec\n",
            "[2022-02-27 23:08:28,298 INFO] Step 3780/ 6000; acc:  58.67; ppl:  8.50; xent: 2.14; lr: 1.00000; 1089/1139 tok/s;   1106 sec\n",
            "[2022-02-27 23:08:35,575 INFO] Step 3800/ 6000; acc:  53.07; ppl: 11.79; xent: 2.47; lr: 1.00000; 1034/1046 tok/s;   1113 sec\n",
            "[2022-02-27 23:08:41,394 INFO] Step 3820/ 6000; acc:  59.15; ppl:  7.99; xent: 2.08; lr: 1.00000; 1077/1133 tok/s;   1119 sec\n",
            "[2022-02-27 23:08:46,576 INFO] Step 3840/ 6000; acc:  60.90; ppl:  7.62; xent: 2.03; lr: 1.00000; 1103/1162 tok/s;   1124 sec\n",
            "[2022-02-27 23:08:53,725 INFO] Step 3860/ 6000; acc:  51.85; ppl: 13.69; xent: 2.62; lr: 1.00000; 1117/1094 tok/s;   1131 sec\n",
            "[2022-02-27 23:08:59,228 INFO] Step 3880/ 6000; acc:  60.35; ppl:  7.55; xent: 2.02; lr: 1.00000; 1067/1163 tok/s;   1137 sec\n",
            "[2022-02-27 23:09:04,694 INFO] Step 3900/ 6000; acc:  59.59; ppl:  8.02; xent: 2.08; lr: 1.00000; 1064/1114 tok/s;   1142 sec\n",
            "[2022-02-27 23:09:11,235 INFO] Step 3920/ 6000; acc:  53.76; ppl: 11.74; xent: 2.46; lr: 1.00000; 1109/1135 tok/s;   1149 sec\n",
            "[2022-02-27 23:09:16,962 INFO] Step 3940/ 6000; acc:  59.17; ppl:  7.70; xent: 2.04; lr: 1.00000; 1103/1192 tok/s;   1155 sec\n",
            "[2022-02-27 23:09:22,047 INFO] Step 3960/ 6000; acc:  62.10; ppl:  6.90; xent: 1.93; lr: 1.00000; 1056/1139 tok/s;   1160 sec\n",
            "[2022-02-27 23:09:28,781 INFO] Step 3980/ 6000; acc:  55.00; ppl: 10.59; xent: 2.36; lr: 1.00000; 1054/1105 tok/s;   1166 sec\n",
            "[2022-02-27 23:09:34,780 INFO] Step 4000/ 6000; acc:  56.61; ppl:  9.01; xent: 2.20; lr: 1.00000; 1130/1199 tok/s;   1172 sec\n",
            "[2022-02-27 23:09:39,800 INFO] Step 4020/ 6000; acc:  63.16; ppl:  6.16; xent: 1.82; lr: 1.00000; 1079/1178 tok/s;   1177 sec\n",
            "[2022-02-27 23:09:45,866 INFO] Step 4040/ 6000; acc:  56.67; ppl:  9.93; xent: 2.30; lr: 1.00000; 1054/1053 tok/s;   1183 sec\n",
            "[2022-02-27 23:09:52,058 INFO] Step 4060/ 6000; acc:  56.68; ppl:  9.38; xent: 2.24; lr: 1.00000; 1172/1189 tok/s;   1190 sec\n",
            "[2022-02-27 23:09:57,442 INFO] Step 4080/ 6000; acc:  61.13; ppl:  6.99; xent: 1.94; lr: 1.00000; 1118/1205 tok/s;   1195 sec\n",
            "[2022-02-27 23:10:02,710 INFO] Step 4100/ 6000; acc:  61.30; ppl:  7.18; xent: 1.97; lr: 1.00000; 1127/1148 tok/s;   1200 sec\n",
            "[2022-02-27 23:10:09,519 INFO] Step 4120/ 6000; acc:  52.46; ppl: 12.15; xent: 2.50; lr: 1.00000; 1117/1109 tok/s;   1207 sec\n",
            "[2022-02-27 23:10:15,135 INFO] Step 4140/ 6000; acc:  60.51; ppl:  6.97; xent: 1.94; lr: 1.00000; 1115/1156 tok/s;   1213 sec\n",
            "[2022-02-27 23:10:20,319 INFO] Step 4160/ 6000; acc:  60.86; ppl:  7.17; xent: 1.97; lr: 1.00000; 1102/1160 tok/s;   1218 sec\n",
            "[2022-02-27 23:10:27,451 INFO] Step 4180/ 6000; acc:  52.20; ppl: 13.00; xent: 2.57; lr: 1.00000; 1116/1079 tok/s;   1225 sec\n",
            "[2022-02-27 23:10:32,819 INFO] Step 4200/ 6000; acc:  61.59; ppl:  6.83; xent: 1.92; lr: 1.00000; 1093/1163 tok/s;   1230 sec\n",
            "[2022-02-27 23:10:37,913 INFO] Step 4220/ 6000; acc:  61.93; ppl:  6.63; xent: 1.89; lr: 1.00000; 1131/1185 tok/s;   1236 sec\n",
            "[2022-02-27 23:10:44,664 INFO] Step 4240/ 6000; acc:  54.60; ppl: 11.00; xent: 2.40; lr: 1.00000; 1097/1128 tok/s;   1242 sec\n",
            "[2022-02-27 23:10:50,524 INFO] Step 4260/ 6000; acc:  59.39; ppl:  7.51; xent: 2.02; lr: 1.00000; 1100/1206 tok/s;   1248 sec\n",
            "[2022-02-27 23:10:55,446 INFO] Step 4280/ 6000; acc:  62.66; ppl:  6.51; xent: 1.87; lr: 1.00000; 1117/1189 tok/s;   1253 sec\n",
            "[2022-02-27 23:11:02,231 INFO] Step 4300/ 6000; acc:  54.79; ppl: 10.93; xent: 2.39; lr: 1.00000; 1060/1070 tok/s;   1260 sec\n",
            "[2022-02-27 23:11:08,042 INFO] Step 4320/ 6000; acc:  58.34; ppl:  7.96; xent: 2.07; lr: 1.00000; 1149/1208 tok/s;   1266 sec\n",
            "[2022-02-27 23:11:12,706 INFO] Step 4340/ 6000; acc:  64.16; ppl:  5.71; xent: 1.74; lr: 1.00000; 1130/1234 tok/s;   1270 sec\n",
            "[2022-02-27 23:11:15,995 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 8\n",
            "[2022-02-27 23:11:18,914 INFO] Step 4360/ 6000; acc:  57.32; ppl:  9.08; xent: 2.21; lr: 1.00000; 992/1041 tok/s;   1277 sec\n",
            "[2022-02-27 23:11:25,626 INFO] Validation perplexity: 6.24988\n",
            "[2022-02-27 23:11:25,628 INFO] Validation accuracy: 66.3653\n",
            "[2022-02-27 23:11:25,629 INFO] Model is improving ppl: 6.63977 --> 6.24988.\n",
            "[2022-02-27 23:11:25,629 INFO] Model is improving acc: 65.809 --> 66.3653.\n",
            "[2022-02-27 23:11:25,691 INFO] Saving checkpoint ./models/6000step/model_step_4375.pt\n",
            "[2022-02-27 23:11:27,302 INFO] Step 4380/ 6000; acc:  56.60; ppl:  9.25; xent: 2.23; lr: 1.00000; 857/889 tok/s;   1285 sec\n",
            "[2022-02-27 23:11:35,626 INFO] Step 4400/ 6000; acc:  61.17; ppl:  6.99; xent: 1.94; lr: 1.00000; 721/774 tok/s;   1293 sec\n",
            "[2022-02-27 23:11:41,375 INFO] Step 4420/ 6000; acc:  60.31; ppl:  7.28; xent: 1.99; lr: 1.00000; 1025/1076 tok/s;   1299 sec\n",
            "[2022-02-27 23:11:48,928 INFO] Step 4440/ 6000; acc:  54.17; ppl: 11.05; xent: 2.40; lr: 1.00000; 1003/1007 tok/s;   1307 sec\n",
            "[2022-02-27 23:11:54,887 INFO] Step 4460/ 6000; acc:  60.01; ppl:  7.33; xent: 1.99; lr: 1.00000; 1057/1114 tok/s;   1312 sec\n",
            "[2022-02-27 23:12:00,485 INFO] Step 4480/ 6000; acc:  62.38; ppl:  6.63; xent: 1.89; lr: 1.00000; 1027/1088 tok/s;   1318 sec\n",
            "[2022-02-27 23:12:07,812 INFO] Step 4500/ 6000; acc:  53.04; ppl: 11.79; xent: 2.47; lr: 1.00000; 1088/1068 tok/s;   1325 sec\n",
            "[2022-02-27 23:12:13,702 INFO] Step 4520/ 6000; acc:  62.43; ppl:  6.51; xent: 1.87; lr: 1.00000; 1001/1092 tok/s;   1331 sec\n",
            "[2022-02-27 23:12:19,363 INFO] Step 4540/ 6000; acc:  61.90; ppl:  6.61; xent: 1.89; lr: 1.00000; 1031/1083 tok/s;   1337 sec\n",
            "[2022-02-27 23:12:26,239 INFO] Step 4560/ 6000; acc:  55.31; ppl: 10.00; xent: 2.30; lr: 1.00000; 1039/1071 tok/s;   1344 sec\n",
            "[2022-02-27 23:12:32,087 INFO] Step 4580/ 6000; acc:  62.06; ppl:  6.64; xent: 1.89; lr: 1.00000; 1073/1158 tok/s;   1350 sec\n",
            "[2022-02-27 23:12:37,344 INFO] Step 4600/ 6000; acc:  64.65; ppl:  5.81; xent: 1.76; lr: 1.00000; 1022/1088 tok/s;   1355 sec\n",
            "[2022-02-27 23:12:44,459 INFO] Step 4620/ 6000; acc:  55.98; ppl:  9.35; xent: 2.24; lr: 1.00000; 999/1068 tok/s;   1362 sec\n",
            "[2022-02-27 23:12:50,634 INFO] Step 4640/ 6000; acc:  58.43; ppl:  7.71; xent: 2.04; lr: 1.00000; 1089/1140 tok/s;   1368 sec\n",
            "[2022-02-27 23:12:55,854 INFO] Step 4660/ 6000; acc:  65.18; ppl:  5.77; xent: 1.75; lr: 1.00000; 1019/1115 tok/s;   1373 sec\n",
            "[2022-02-27 23:13:02,401 INFO] Step 4680/ 6000; acc:  58.03; ppl:  8.59; xent: 2.15; lr: 1.00000; 972/994 tok/s;   1380 sec\n",
            "[2022-02-27 23:13:08,859 INFO] Step 4700/ 6000; acc:  57.07; ppl:  8.95; xent: 2.19; lr: 1.00000; 1147/1133 tok/s;   1386 sec\n",
            "[2022-02-27 23:13:14,609 INFO] Step 4720/ 6000; acc:  62.53; ppl:  6.22; xent: 1.83; lr: 1.00000; 1074/1099 tok/s;   1392 sec\n",
            "[2022-02-27 23:13:20,441 INFO] Step 4740/ 6000; acc:  61.57; ppl:  6.64; xent: 1.89; lr: 1.00000; 1026/1063 tok/s;   1398 sec\n",
            "[2022-02-27 23:13:27,631 INFO] Step 4760/ 6000; acc:  54.86; ppl: 10.18; xent: 2.32; lr: 1.00000; 1052/1054 tok/s;   1405 sec\n",
            "[2022-02-27 23:13:33,453 INFO] Step 4780/ 6000; acc:  61.19; ppl:  6.30; xent: 1.84; lr: 1.00000; 1074/1120 tok/s;   1411 sec\n",
            "[2022-02-27 23:13:38,772 INFO] Step 4800/ 6000; acc:  63.80; ppl:  6.24; xent: 1.83; lr: 1.00000; 1073/1119 tok/s;   1416 sec\n",
            "[2022-02-27 23:13:46,110 INFO] Step 4820/ 6000; acc:  53.00; ppl: 11.56; xent: 2.45; lr: 1.00000; 1086/1055 tok/s;   1424 sec\n",
            "[2022-02-27 23:13:51,695 INFO] Step 4840/ 6000; acc:  62.98; ppl:  5.91; xent: 1.78; lr: 1.00000; 1054/1132 tok/s;   1429 sec\n",
            "[2022-02-27 23:13:57,110 INFO] Step 4860/ 6000; acc:  62.16; ppl:  6.25; xent: 1.83; lr: 1.00000; 1066/1147 tok/s;   1435 sec\n",
            "[2022-02-27 23:14:04,154 INFO] Step 4880/ 6000; acc:  55.26; ppl:  9.78; xent: 2.28; lr: 1.00000; 1042/1057 tok/s;   1442 sec\n",
            "[2022-02-27 23:14:10,109 INFO] Step 4900/ 6000; acc:  60.96; ppl:  6.89; xent: 1.93; lr: 1.00000; 1075/1134 tok/s;   1448 sec\n",
            "[2022-02-27 23:14:15,323 INFO] Step 4920/ 6000; acc:  64.75; ppl:  5.41; xent: 1.69; lr: 1.00000; 1036/1118 tok/s;   1453 sec\n",
            "[2022-02-27 23:14:22,625 INFO] Step 4940/ 6000; acc:  55.73; ppl:  9.62; xent: 2.26; lr: 1.00000; 977/1026 tok/s;   1460 sec\n",
            "[2022-02-27 23:14:28,823 INFO] Step 4960/ 6000; acc:  60.21; ppl:  6.95; xent: 1.94; lr: 1.00000; 1081/1138 tok/s;   1466 sec\n",
            "[2022-02-27 23:14:33,968 INFO] Step 4980/ 6000; acc:  65.49; ppl:  5.23; xent: 1.65; lr: 1.00000; 1030/1130 tok/s;   1472 sec\n",
            "[2022-02-27 23:14:37,329 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 9\n",
            "[2022-02-27 23:14:40,785 INFO] Step 5000/ 6000; acc:  58.90; ppl:  7.99; xent: 2.08; lr: 1.00000; 918/972 tok/s;   1478 sec\n",
            "[2022-02-27 23:14:42,456 INFO] Validation perplexity: 5.91553\n",
            "[2022-02-27 23:14:42,456 INFO] Validation accuracy: 66.8521\n",
            "[2022-02-27 23:14:42,456 INFO] Model is improving ppl: 6.24988 --> 5.91553.\n",
            "[2022-02-27 23:14:42,456 INFO] Model is improving acc: 66.3653 --> 66.8521.\n",
            "[2022-02-27 23:14:42,515 INFO] Saving checkpoint ./models/6000step/model_step_5000.pt\n",
            "[2022-02-27 23:14:49,584 INFO] Step 5020/ 6000; acc:  58.10; ppl:  7.98; xent: 2.08; lr: 1.00000; 819/841 tok/s;   1487 sec\n",
            "[2022-02-27 23:14:55,460 INFO] Step 5040/ 6000; acc:  62.53; ppl:  6.37; xent: 1.85; lr: 1.00000; 1026/1080 tok/s;   1493 sec\n",
            "[2022-02-27 23:15:01,192 INFO] Step 5060/ 6000; acc:  61.04; ppl:  6.74; xent: 1.91; lr: 1.00000; 1032/1084 tok/s;   1499 sec\n",
            "[2022-02-27 23:15:08,336 INFO] Step 5080/ 6000; acc:  54.61; ppl: 10.01; xent: 2.30; lr: 1.00000; 1059/1071 tok/s;   1506 sec\n",
            "[2022-02-27 23:15:14,406 INFO] Step 5100/ 6000; acc:  61.88; ppl:  6.36; xent: 1.85; lr: 1.00000; 1051/1088 tok/s;   1512 sec\n",
            "[2022-02-27 23:15:20,021 INFO] Step 5120/ 6000; acc:  63.36; ppl:  5.98; xent: 1.79; lr: 1.00000; 1032/1082 tok/s;   1518 sec\n",
            "[2022-02-27 23:15:27,233 INFO] Step 5140/ 6000; acc:  55.54; ppl:  9.83; xent: 2.29; lr: 1.00000; 1078/1080 tok/s;   1525 sec\n",
            "[2022-02-27 23:15:33,000 INFO] Step 5160/ 6000; acc:  64.79; ppl:  5.58; xent: 1.72; lr: 1.00000; 1007/1079 tok/s;   1531 sec\n",
            "[2022-02-27 23:15:38,521 INFO] Step 5180/ 6000; acc:  63.76; ppl:  5.82; xent: 1.76; lr: 1.00000; 1037/1117 tok/s;   1536 sec\n",
            "[2022-02-27 23:15:45,391 INFO] Step 5200/ 6000; acc:  56.09; ppl:  8.92; xent: 2.19; lr: 1.00000; 1047/1085 tok/s;   1543 sec\n",
            "[2022-02-27 23:15:51,186 INFO] Step 5220/ 6000; acc:  62.29; ppl:  6.36; xent: 1.85; lr: 1.00000; 1087/1180 tok/s;   1549 sec\n",
            "[2022-02-27 23:15:56,350 INFO] Step 5240/ 6000; acc:  65.62; ppl:  5.15; xent: 1.64; lr: 1.00000; 1047/1110 tok/s;   1554 sec\n",
            "[2022-02-27 23:16:03,334 INFO] Step 5260/ 6000; acc:  57.60; ppl:  8.31; xent: 2.12; lr: 1.00000; 1028/1082 tok/s;   1561 sec\n",
            "[2022-02-27 23:16:09,364 INFO] Step 5280/ 6000; acc:  60.33; ppl:  7.10; xent: 1.96; lr: 1.00000; 1125/1172 tok/s;   1567 sec\n",
            "[2022-02-27 23:16:14,615 INFO] Step 5300/ 6000; acc:  66.50; ppl:  5.01; xent: 1.61; lr: 1.00000; 1018/1115 tok/s;   1572 sec\n",
            "[2022-02-27 23:16:20,897 INFO] Step 5320/ 6000; acc:  58.91; ppl:  7.73; xent: 2.04; lr: 1.00000; 1018/1030 tok/s;   1578 sec\n",
            "[2022-02-27 23:16:27,302 INFO] Step 5340/ 6000; acc:  58.51; ppl:  7.80; xent: 2.05; lr: 1.00000; 1150/1129 tok/s;   1585 sec\n",
            "[2022-02-27 23:16:33,197 INFO] Step 5360/ 6000; acc:  63.75; ppl:  5.36; xent: 1.68; lr: 1.00000; 1040/1069 tok/s;   1591 sec\n",
            "[2022-02-27 23:16:38,854 INFO] Step 5380/ 6000; acc:  63.85; ppl:  5.77; xent: 1.75; lr: 1.00000; 1051/1099 tok/s;   1596 sec\n",
            "[2022-02-27 23:16:45,752 INFO] Step 5400/ 6000; acc:  54.93; ppl: 10.43; xent: 2.34; lr: 1.00000; 1118/1081 tok/s;   1603 sec\n",
            "[2022-02-27 23:16:51,747 INFO] Step 5420/ 6000; acc:  62.90; ppl:  5.70; xent: 1.74; lr: 1.00000; 1059/1089 tok/s;   1609 sec\n",
            "[2022-02-27 23:16:57,170 INFO] Step 5440/ 6000; acc:  64.63; ppl:  5.44; xent: 1.69; lr: 1.00000; 1062/1141 tok/s;   1615 sec\n",
            "[2022-02-27 23:17:05,047 INFO] Step 5460/ 6000; acc:  55.57; ppl:  9.28; xent: 2.23; lr: 1.00000; 977/1000 tok/s;   1623 sec\n",
            "[2022-02-27 23:17:10,557 INFO] Step 5480/ 6000; acc:  64.79; ppl:  5.31; xent: 1.67; lr: 1.00000; 1042/1140 tok/s;   1628 sec\n",
            "[2022-02-27 23:17:15,938 INFO] Step 5500/ 6000; acc:  63.56; ppl:  5.41; xent: 1.69; lr: 1.00000; 1062/1080 tok/s;   1634 sec\n",
            "[2022-02-27 23:17:23,218 INFO] Step 5520/ 6000; acc:  56.43; ppl:  8.98; xent: 2.20; lr: 1.00000; 998/1042 tok/s;   1641 sec\n",
            "[2022-02-27 23:17:29,087 INFO] Step 5540/ 6000; acc:  61.99; ppl:  6.01; xent: 1.79; lr: 1.00000; 1088/1158 tok/s;   1647 sec\n",
            "[2022-02-27 23:17:34,366 INFO] Step 5560/ 6000; acc:  66.84; ppl:  4.66; xent: 1.54; lr: 1.00000; 1030/1101 tok/s;   1652 sec\n",
            "[2022-02-27 23:17:36,869 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 10\n",
            "[2022-02-27 23:17:41,526 INFO] Step 5580/ 6000; acc:  56.04; ppl:  9.38; xent: 2.24; lr: 1.00000; 1009/1045 tok/s;   1659 sec\n",
            "[2022-02-27 23:17:47,813 INFO] Step 5600/ 6000; acc:  60.93; ppl:  6.47; xent: 1.87; lr: 1.00000; 1079/1130 tok/s;   1665 sec\n",
            "[2022-02-27 23:17:52,957 INFO] Step 5620/ 6000; acc:  66.22; ppl:  4.78; xent: 1.57; lr: 1.00000; 1035/1144 tok/s;   1671 sec\n",
            "[2022-02-27 23:17:55,816 INFO] Validation perplexity: 5.87522\n",
            "[2022-02-27 23:17:55,816 INFO] Validation accuracy: 67.5475\n",
            "[2022-02-27 23:17:55,816 INFO] Model is improving ppl: 5.91553 --> 5.87522.\n",
            "[2022-02-27 23:17:55,817 INFO] Model is improving acc: 66.8521 --> 67.5475.\n",
            "[2022-02-27 23:17:55,877 INFO] Saving checkpoint ./models/6000step/model_step_5625.pt\n",
            "[2022-02-27 23:18:01,662 INFO] Step 5640/ 6000; acc:  60.59; ppl:  6.90; xent: 1.93; lr: 1.00000; 722/739 tok/s;   1679 sec\n",
            "[2022-02-27 23:18:08,608 INFO] Step 5660/ 6000; acc:  59.40; ppl:  7.06; xent: 1.95; lr: 1.00000; 1027/1054 tok/s;   1686 sec\n",
            "[2022-02-27 23:18:14,378 INFO] Step 5680/ 6000; acc:  63.71; ppl:  5.72; xent: 1.74; lr: 1.00000; 1041/1115 tok/s;   1692 sec\n",
            "[2022-02-27 23:18:20,156 INFO] Step 5700/ 6000; acc:  63.65; ppl:  5.61; xent: 1.72; lr: 1.00000; 1022/1060 tok/s;   1698 sec\n",
            "[2022-02-27 23:18:27,457 INFO] Step 5720/ 6000; acc:  55.35; ppl:  9.70; xent: 2.27; lr: 1.00000; 1060/1063 tok/s;   1705 sec\n",
            "[2022-02-27 23:18:33,485 INFO] Step 5740/ 6000; acc:  62.95; ppl:  5.91; xent: 1.78; lr: 1.00000; 1062/1097 tok/s;   1711 sec\n",
            "[2022-02-27 23:18:39,147 INFO] Step 5760/ 6000; acc:  64.15; ppl:  5.43; xent: 1.69; lr: 1.00000; 1028/1089 tok/s;   1717 sec\n",
            "[2022-02-27 23:18:46,355 INFO] Step 5780/ 6000; acc:  56.36; ppl:  8.83; xent: 2.18; lr: 1.00000; 1053/1084 tok/s;   1724 sec\n",
            "[2022-02-27 23:18:52,032 INFO] Step 5800/ 6000; acc:  65.21; ppl:  4.97; xent: 1.60; lr: 1.00000; 1002/1115 tok/s;   1730 sec\n",
            "[2022-02-27 23:18:57,588 INFO] Step 5820/ 6000; acc:  65.68; ppl:  5.17; xent: 1.64; lr: 1.00000; 1020/1058 tok/s;   1735 sec\n",
            "[2022-02-27 23:19:04,788 INFO] Step 5840/ 6000; acc:  58.15; ppl:  7.74; xent: 2.05; lr: 1.00000; 999/1043 tok/s;   1742 sec\n",
            "[2022-02-27 23:19:10,667 INFO] Step 5860/ 6000; acc:  62.68; ppl:  5.90; xent: 1.77; lr: 1.00000; 1075/1165 tok/s;   1748 sec\n",
            "[2022-02-27 23:19:15,926 INFO] Step 5880/ 6000; acc:  67.91; ppl:  4.51; xent: 1.51; lr: 1.00000; 1038/1081 tok/s;   1754 sec\n",
            "[2022-02-27 23:19:23,247 INFO] Step 5900/ 6000; acc:  58.51; ppl:  7.98; xent: 2.08; lr: 1.00000; 1000/1016 tok/s;   1761 sec\n",
            "[2022-02-27 23:19:29,612 INFO] Step 5920/ 6000; acc:  60.97; ppl:  6.36; xent: 1.85; lr: 1.00000; 1074/1143 tok/s;   1767 sec\n",
            "[2022-02-27 23:19:34,855 INFO] Step 5940/ 6000; acc:  67.66; ppl:  4.41; xent: 1.48; lr: 1.00000; 1033/1128 tok/s;   1772 sec\n",
            "[2022-02-27 23:19:40,920 INFO] Step 5960/ 6000; acc:  60.26; ppl:  7.18; xent: 1.97; lr: 1.00000; 1043/1040 tok/s;   1779 sec\n",
            "[2022-02-27 23:19:47,332 INFO] Step 5980/ 6000; acc:  60.54; ppl:  6.74; xent: 1.91; lr: 1.00000; 1140/1105 tok/s;   1785 sec\n",
            "[2022-02-27 23:19:53,146 INFO] Step 6000/ 6000; acc:  65.78; ppl:  4.92; xent: 1.59; lr: 1.00000; 1041/1092 tok/s;   1791 sec\n",
            "[2022-02-27 23:19:53,216 INFO] Saving checkpoint ./models/6000step/model_step_6000.pt\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:19:56,244 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:20:00,602 INFO] PRED AVG SCORE: -1.1664, PRED PPL: 3.2104\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:20:02,624 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:20:07,059 INFO] PRED AVG SCORE: -1.0799, PRED PPL: 2.9444\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:20:09,123 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:20:14,123 INFO] PRED AVG SCORE: -1.0042, PRED PPL: 2.7298\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_translate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/translate.py\", line 54, in main\n",
            "    translate(opt)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/translate.py\", line 16, in translate\n",
            "    translator = build_translator(opt, logger=logger, report_score=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/translate/translator.py\", line 32, in build_translator\n",
            "    fields, model, model_opt = load_test_model(opt)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/model_builder.py\", line 85, in load_test_model\n",
            "    map_location=lambda storage, loc: storage)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'model_step_2000.pt'\n",
            "BLEU = 10.03, 50.7/18.8/10.4/3.8 (BP=0.717, ratio=0.751, hyp_len=2858, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 14.06, 52.4/21.5/12.3/5.5 (BP=0.846, ratio=0.856, hyp_len=3261, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 17.59, 53.2/23.7/13.9/6.8 (BP=0.947, ratio=0.949, hyp_len=3613, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "Use of uninitialized value $length_reference in numeric eq (==) at multi-bleu.perl line 148.\n",
            "BLEU = 0, 0/0/0/0 (BP=0, ratio=0, hyp_len=0, ref_len=0)\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-45bbcd06ebfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cd drive/MyDrive/tp_nmt/\\nonmt_train -config ./config-6000step.yaml -early_stopping 2\\ncd models/6000step\\nonmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\\nonmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\\nonmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\\nonmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\\ncd ../..\\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred625.txt\\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1250.txt\\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1875.txt\\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred2000.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cd drive/MyDrive/tp_nmt/\nonmt_train -config ./config-6000step.yaml -early_stopping 2\ncd models/6000step\nonmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\nonmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\nonmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\nonmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\ncd ../..\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred625.txt\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1250.txt\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1875.txt\nperl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred2000.txt' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-6000step.yaml -early_stopping 2\n",
        "cd models/6000step\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tHdrfWQfppQ",
        "outputId": "27afb461-7989-4505-d9d8-e267e6f61f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:24:45,551 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:24:49,658 INFO] PRED AVG SCORE: -1.1664, PRED PPL: 3.2104\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:24:51,656 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:24:56,106 INFO] PRED AVG SCORE: -1.0799, PRED PPL: 2.9444\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:24:58,154 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:03,083 INFO] PRED AVG SCORE: -1.0042, PRED PPL: 2.7298\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:05,250 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:10,111 INFO] PRED AVG SCORE: -0.8822, PRED PPL: 2.4161\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:12,295 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:17,006 INFO] PRED AVG SCORE: -0.7913, PRED PPL: 2.2063\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:19,131 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:24,240 INFO] PRED AVG SCORE: -0.7912, PRED PPL: 2.2061\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:26,458 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:31,369 INFO] PRED AVG SCORE: -0.7035, PRED PPL: 2.0208\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:33,579 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:38,783 INFO] PRED AVG SCORE: -0.6950, PRED PPL: 2.0037\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:40,822 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:45,600 INFO] PRED AVG SCORE: -0.6317, PRED PPL: 1.8808\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:25:47,613 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:25:52,370 INFO] PRED AVG SCORE: -0.5984, PRED PPL: 1.8193\n",
            "BLEU = 10.03, 50.7/18.8/10.4/3.8 (BP=0.717, ratio=0.751, hyp_len=2858, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 14.06, 52.4/21.5/12.3/5.5 (BP=0.846, ratio=0.856, hyp_len=3261, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 17.59, 53.2/23.7/13.9/6.8 (BP=0.947, ratio=0.949, hyp_len=3613, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 21.83, 56.3/27.7/17.6/9.7 (BP=0.960, ratio=0.961, hyp_len=3658, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 26.03, 62.6/34.3/23.0/14.2 (BP=0.899, ratio=0.904, hyp_len=3441, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 28.26, 60.2/33.6/22.9/14.8 (BP=0.982, ratio=0.982, hyp_len=3741, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 28.27, 64.2/36.5/24.7/15.9 (BP=0.913, ratio=0.916, hyp_len=3489, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 29.93, 62.4/36.2/24.6/16.1 (BP=0.972, ratio=0.973, hyp_len=3704, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 30.26, 66.0/38.9/26.8/18.0 (BP=0.907, ratio=0.911, hyp_len=3468, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 31.78, 66.6/40.3/28.1/19.0 (BP=0.917, ratio=0.921, hyp_len=3506, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/models/6000step\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2500.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2500.txt\n",
        "onmt_translate -model model_step_3125.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred3125.txt\n",
        "onmt_translate -model model_step_3750.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred3750.txt\n",
        "onmt_translate -model model_step_4375.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred4375.txt\n",
        "onmt_translate -model model_step_5000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred5000.txt\n",
        "onmt_translate -model model_step_5625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred5625.txt\n",
        "onmt_translate -model model_step_6000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred6000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred2500.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred3125.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred3750.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred4375.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred5000.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/6000step/pred5625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/6000step/pred6000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnt8OlxUi1w4"
      },
      "source": [
        "Le score bleu augmente avec le nombre de train_steps, mais l'execution du training prends énormement de temps, on a donc décider de ne pas augmenter plus le nombre maximal de train_steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acYU8WG0Hvje"
      },
      "source": [
        "Q11: Nous n'obtenons pas de meilleurs résultats avec plus de couche \n",
        "\n",
        "<span style=\"color:red\">Pourquoi? en tout cas, il aurait fallu utiliser l'option early_stopping pour bien repondre à cette question (ainsi que les deux prochaines). En effet, si l'on utilise pas cette option, on risque de comparer des modèles qui sont sous-entrainé (c'est votre cas) ou sur-entrainés (overfitted),  sans donc pouvoir en tirer des conclusions. En effet, le nombre d'étapes d'entrainement nécessaires à l'optimization du modèle varie selon la quantité de paramétres (poids) à optimizer, ce qui est le cas lorsqu'on varie le nombre de couches ou leur taille.  </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c6V8u--ivPj",
        "outputId": "8658743d-c8bc-4abd-c53b-094034900369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 18:57:50,687 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-27 18:57:50,689 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 18:57:50,690 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-27 18:57:50,690 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-27 18:57:50,691 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-27 18:57:50,691 INFO] Loading vocab from text file...\n",
            "[2022-02-27 18:57:50,692 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-27 18:57:50,718 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-27 18:57:50,723 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-27 18:57:50,743 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-27 18:57:50,748 INFO] Building fields with vocab in counters...\n",
            "[2022-02-27 18:57:50,759 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-27 18:57:50,774 INFO]  * src vocab size: 9980.\n",
            "[2022-02-27 18:57:50,775 INFO]  * src vocab size = 9980\n",
            "[2022-02-27 18:57:50,775 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-27 18:57:50,779 INFO] Building model...\n",
            "[2022-02-27 18:57:51,005 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, num_layers=2, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "        (1): LSTMCell(256, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-27 18:57:51,006 INFO] encoder: 6030384\n",
            "[2022-02-27 18:57:51,007 INFO] decoder: 7770558\n",
            "[2022-02-27 18:57:51,007 INFO] * number of parameters: 13800942\n",
            "[2022-02-27 18:57:51,009 INFO] Starting training on CPU, could be very slow\n",
            "[2022-02-27 18:57:51,009 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-27 18:57:51,010 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 18:57:51,010 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "[2022-02-27 18:57:57,430 INFO] Step 20/ 2000; acc:   9.60; ppl: 2002.06; xent: 7.60; lr: 1.00000; 883/938 tok/s;      6 sec\n",
            "[2022-02-27 18:58:07,442 INFO] Step 40/ 2000; acc:   7.43; ppl: 846.72; xent: 6.74; lr: 1.00000; 820/833 tok/s;     16 sec\n",
            "[2022-02-27 18:58:13,357 INFO] Step 60/ 2000; acc:  10.74; ppl: 501.75; xent: 6.22; lr: 1.00000; 852/946 tok/s;     22 sec\n",
            "[2022-02-27 18:58:20,090 INFO] Step 80/ 2000; acc:  10.88; ppl: 351.23; xent: 5.86; lr: 1.00000; 938/952 tok/s;     29 sec\n",
            "[2022-02-27 18:58:29,422 INFO] Step 100/ 2000; acc:  12.84; ppl: 369.15; xent: 5.91; lr: 1.00000; 922/904 tok/s;     38 sec\n",
            "[2022-02-27 18:58:35,285 INFO] Step 120/ 2000; acc:  21.04; ppl: 174.62; xent: 5.16; lr: 1.00000; 854/937 tok/s;     44 sec\n",
            "[2022-02-27 18:58:42,102 INFO] Step 140/ 2000; acc:  18.87; ppl: 189.32; xent: 5.24; lr: 1.00000; 927/970 tok/s;     51 sec\n",
            "[2022-02-27 18:58:49,064 INFO] Step 160/ 2000; acc:  21.57; ppl: 164.10; xent: 5.10; lr: 1.00000; 938/982 tok/s;     58 sec\n",
            "[2022-02-27 18:58:56,572 INFO] Step 180/ 2000; acc:  20.47; ppl: 197.09; xent: 5.28; lr: 1.00000; 890/930 tok/s;     66 sec\n",
            "[2022-02-27 18:59:03,104 INFO] Step 200/ 2000; acc:  23.76; ppl: 132.42; xent: 4.89; lr: 1.00000; 964/1018 tok/s;     72 sec\n",
            "[2022-02-27 18:59:09,374 INFO] Step 220/ 2000; acc:  29.06; ppl: 98.12; xent: 4.59; lr: 1.00000; 892/1008 tok/s;     78 sec\n",
            "[2022-02-27 18:59:17,954 INFO] Step 240/ 2000; acc:  25.17; ppl: 136.16; xent: 4.91; lr: 1.00000; 833/859 tok/s;     87 sec\n",
            "[2022-02-27 18:59:24,663 INFO] Step 260/ 2000; acc:  29.16; ppl: 92.66; xent: 4.53; lr: 1.00000; 927/952 tok/s;     94 sec\n",
            "[2022-02-27 18:59:30,594 INFO] Step 280/ 2000; acc:  33.25; ppl: 71.49; xent: 4.27; lr: 1.00000; 884/1007 tok/s;    100 sec\n",
            "[2022-02-27 18:59:39,461 INFO] Step 300/ 2000; acc:  24.85; ppl: 123.35; xent: 4.82; lr: 1.00000; 894/918 tok/s;    108 sec\n",
            "[2022-02-27 18:59:45,962 INFO] Step 320/ 2000; acc:  33.92; ppl: 70.36; xent: 4.25; lr: 1.00000; 909/928 tok/s;    115 sec\n",
            "[2022-02-27 18:59:52,278 INFO] Step 340/ 2000; acc:  34.63; ppl: 62.53; xent: 4.14; lr: 1.00000; 924/930 tok/s;    121 sec\n",
            "[2022-02-27 19:00:01,574 INFO] Step 360/ 2000; acc:  27.36; ppl: 98.19; xent: 4.59; lr: 1.00000; 893/885 tok/s;    131 sec\n",
            "[2022-02-27 19:00:07,562 INFO] Step 380/ 2000; acc:  35.66; ppl: 58.76; xent: 4.07; lr: 1.00000; 866/939 tok/s;    137 sec\n",
            "[2022-02-27 19:00:14,031 INFO] Step 400/ 2000; acc:  34.88; ppl: 57.71; xent: 4.06; lr: 1.00000; 973/956 tok/s;    143 sec\n",
            "[2022-02-27 19:00:23,426 INFO] Step 420/ 2000; acc:  26.37; ppl: 100.73; xent: 4.61; lr: 1.00000; 928/904 tok/s;    152 sec\n",
            "[2022-02-27 19:00:29,429 INFO] Step 440/ 2000; acc:  37.29; ppl: 48.83; xent: 3.89; lr: 1.00000; 825/921 tok/s;    158 sec\n",
            "[2022-02-27 19:00:36,671 INFO] Step 460/ 2000; acc:  34.95; ppl: 53.71; xent: 3.98; lr: 1.00000; 867/915 tok/s;    166 sec\n",
            "[2022-02-27 19:00:44,133 INFO] Step 480/ 2000; acc:  34.23; ppl: 59.38; xent: 4.08; lr: 1.00000; 877/889 tok/s;    173 sec\n",
            "[2022-02-27 19:00:52,037 INFO] Step 500/ 2000; acc:  30.76; ppl: 72.50; xent: 4.28; lr: 1.00000; 832/875 tok/s;    181 sec\n",
            "[2022-02-27 19:00:58,728 INFO] Step 520/ 2000; acc:  35.77; ppl: 52.34; xent: 3.96; lr: 1.00000; 941/992 tok/s;    188 sec\n",
            "[2022-02-27 19:01:04,946 INFO] Step 540/ 2000; acc:  40.22; ppl: 42.39; xent: 3.75; lr: 1.00000; 906/987 tok/s;    194 sec\n",
            "[2022-02-27 19:01:13,490 INFO] Step 560/ 2000; acc:  32.08; ppl: 73.78; xent: 4.30; lr: 1.00000; 864/899 tok/s;    202 sec\n",
            "[2022-02-27 19:01:18,832 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-27 19:01:20,148 INFO] Step 580/ 2000; acc:  37.21; ppl: 48.97; xent: 3.89; lr: 1.00000; 938/954 tok/s;    209 sec\n",
            "[2022-02-27 19:01:26,213 INFO] Step 600/ 2000; acc:  41.08; ppl: 36.09; xent: 3.59; lr: 1.00000; 867/969 tok/s;    215 sec\n",
            "[2022-02-27 19:01:35,212 INFO] Step 620/ 2000; acc:  31.07; ppl: 68.70; xent: 4.23; lr: 1.00000; 871/904 tok/s;    224 sec\n",
            "[2022-02-27 19:01:36,625 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-27 19:01:38,551 INFO] Validation perplexity: 27.0833\n",
            "[2022-02-27 19:01:38,551 INFO] Validation accuracy: 44.089\n",
            "[2022-02-27 19:01:38,611 INFO] Saving checkpoint ./models/2couches/model_step_625.pt\n",
            "[2022-02-27 19:01:43,986 INFO] Step 640/ 2000; acc:  38.85; ppl: 44.23; xent: 3.79; lr: 1.00000; 672/693 tok/s;    233 sec\n",
            "[2022-02-27 19:01:50,502 INFO] Step 660/ 2000; acc:  40.97; ppl: 36.54; xent: 3.60; lr: 1.00000; 864/945 tok/s;    239 sec\n",
            "[2022-02-27 19:02:00,356 INFO] Step 680/ 2000; acc:  33.09; ppl: 61.75; xent: 4.12; lr: 1.00000; 822/827 tok/s;    249 sec\n",
            "[2022-02-27 19:02:06,201 INFO] Step 700/ 2000; acc:  41.87; ppl: 35.04; xent: 3.56; lr: 1.00000; 867/944 tok/s;    255 sec\n",
            "[2022-02-27 19:02:13,248 INFO] Step 720/ 2000; acc:  39.58; ppl: 39.96; xent: 3.69; lr: 1.00000; 897/913 tok/s;    262 sec\n",
            "[2022-02-27 19:02:22,940 INFO] Step 740/ 2000; acc:  31.26; ppl: 69.23; xent: 4.24; lr: 1.00000; 906/891 tok/s;    272 sec\n",
            "[2022-02-27 19:02:28,917 INFO] Step 760/ 2000; acc:  43.47; ppl: 32.83; xent: 3.49; lr: 1.00000; 844/903 tok/s;    278 sec\n",
            "[2022-02-27 19:02:36,050 INFO] Step 780/ 2000; acc:  40.59; ppl: 37.53; xent: 3.63; lr: 1.00000; 876/954 tok/s;    285 sec\n",
            "[2022-02-27 19:02:43,138 INFO] Step 800/ 2000; acc:  41.45; ppl: 39.11; xent: 3.67; lr: 1.00000; 904/932 tok/s;    292 sec\n",
            "[2022-02-27 19:02:50,851 INFO] Step 820/ 2000; acc:  36.47; ppl: 52.40; xent: 3.96; lr: 1.00000; 841/906 tok/s;    300 sec\n",
            "[2022-02-27 19:02:57,666 INFO] Step 840/ 2000; acc:  41.49; ppl: 35.01; xent: 3.56; lr: 1.00000; 916/960 tok/s;    307 sec\n",
            "[2022-02-27 19:03:04,152 INFO] Step 860/ 2000; acc:  43.55; ppl: 29.83; xent: 3.40; lr: 1.00000; 872/967 tok/s;    313 sec\n",
            "[2022-02-27 19:03:12,864 INFO] Step 880/ 2000; acc:  36.59; ppl: 47.47; xent: 3.86; lr: 1.00000; 832/850 tok/s;    322 sec\n",
            "[2022-02-27 19:03:19,562 INFO] Step 900/ 2000; acc:  42.51; ppl: 34.38; xent: 3.54; lr: 1.00000; 934/959 tok/s;    329 sec\n",
            "[2022-02-27 19:03:25,751 INFO] Step 920/ 2000; acc:  44.67; ppl: 27.73; xent: 3.32; lr: 1.00000; 865/944 tok/s;    335 sec\n",
            "[2022-02-27 19:03:35,023 INFO] Step 940/ 2000; acc:  34.29; ppl: 54.78; xent: 4.00; lr: 1.00000; 870/890 tok/s;    344 sec\n",
            "[2022-02-27 19:03:41,277 INFO] Step 960/ 2000; acc:  44.26; ppl: 30.44; xent: 3.42; lr: 1.00000; 953/976 tok/s;    350 sec\n",
            "[2022-02-27 19:03:47,697 INFO] Step 980/ 2000; acc:  44.56; ppl: 27.73; xent: 3.32; lr: 1.00000; 898/906 tok/s;    357 sec\n",
            "[2022-02-27 19:03:56,547 INFO] Step 1000/ 2000; acc:  35.97; ppl: 45.48; xent: 3.82; lr: 1.00000; 917/912 tok/s;    366 sec\n",
            "[2022-02-27 19:04:02,516 INFO] Step 1020/ 2000; acc:  45.44; ppl: 27.15; xent: 3.30; lr: 1.00000; 861/931 tok/s;    372 sec\n",
            "[2022-02-27 19:04:09,072 INFO] Step 1040/ 2000; acc:  43.31; ppl: 30.43; xent: 3.42; lr: 1.00000; 962/959 tok/s;    378 sec\n",
            "[2022-02-27 19:04:19,142 INFO] Step 1060/ 2000; acc:  34.86; ppl: 51.69; xent: 3.95; lr: 1.00000; 887/862 tok/s;    388 sec\n",
            "[2022-02-27 19:04:25,148 INFO] Step 1080/ 2000; acc:  47.29; ppl: 24.48; xent: 3.20; lr: 1.00000; 837/902 tok/s;    394 sec\n",
            "[2022-02-27 19:04:32,151 INFO] Step 1100/ 2000; acc:  41.73; ppl: 31.52; xent: 3.45; lr: 1.00000; 897/981 tok/s;    401 sec\n",
            "[2022-02-27 19:04:39,138 INFO] Step 1120/ 2000; acc:  42.75; ppl: 31.46; xent: 3.45; lr: 1.00000; 930/958 tok/s;    408 sec\n",
            "[2022-02-27 19:04:46,893 INFO] Step 1140/ 2000; acc:  39.09; ppl: 39.94; xent: 3.69; lr: 1.00000; 839/890 tok/s;    416 sec\n",
            "[2022-02-27 19:04:53,451 INFO] Step 1160/ 2000; acc:  43.65; ppl: 30.88; xent: 3.43; lr: 1.00000; 958/966 tok/s;    422 sec\n",
            "[2022-02-27 19:04:59,716 INFO] Step 1180/ 2000; acc:  46.62; ppl: 24.08; xent: 3.18; lr: 1.00000; 883/991 tok/s;    429 sec\n",
            "[2022-02-27 19:05:09,066 INFO] Step 1200/ 2000; acc:  37.68; ppl: 41.65; xent: 3.73; lr: 1.00000; 783/823 tok/s;    438 sec\n",
            "[2022-02-27 19:05:14,462 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-27 19:05:15,805 INFO] Step 1220/ 2000; acc:  44.35; ppl: 29.71; xent: 3.39; lr: 1.00000; 921/934 tok/s;    445 sec\n",
            "[2022-02-27 19:05:21,881 INFO] Step 1240/ 2000; acc:  46.88; ppl: 22.62; xent: 3.12; lr: 1.00000; 876/972 tok/s;    451 sec\n",
            "[2022-02-27 19:05:29,600 INFO] Validation perplexity: 17.4369\n",
            "[2022-02-27 19:05:29,600 INFO] Validation accuracy: 50.6027\n",
            "[2022-02-27 19:05:29,660 INFO] Saving checkpoint ./models/2couches/model_step_1250.pt\n",
            "[2022-02-27 19:05:33,437 INFO] Step 1260/ 2000; acc:  38.40; ppl: 40.65; xent: 3.70; lr: 1.00000; 678/704 tok/s;    462 sec\n",
            "[2022-02-27 19:05:39,988 INFO] Step 1280/ 2000; acc:  44.64; ppl: 27.50; xent: 3.31; lr: 1.00000; 907/934 tok/s;    469 sec\n",
            "[2022-02-27 19:05:46,351 INFO] Step 1300/ 2000; acc:  48.15; ppl: 21.87; xent: 3.09; lr: 1.00000; 902/930 tok/s;    475 sec\n",
            "[2022-02-27 19:05:55,763 INFO] Step 1320/ 2000; acc:  38.47; ppl: 37.92; xent: 3.64; lr: 1.00000; 870/876 tok/s;    485 sec\n",
            "[2022-02-27 19:06:01,569 INFO] Step 1340/ 2000; acc:  47.61; ppl: 22.36; xent: 3.11; lr: 1.00000; 881/976 tok/s;    491 sec\n",
            "[2022-02-27 19:06:08,233 INFO] Step 1360/ 2000; acc:  45.66; ppl: 26.36; xent: 3.27; lr: 1.00000; 948/963 tok/s;    497 sec\n",
            "[2022-02-27 19:06:17,663 INFO] Step 1380/ 2000; acc:  37.17; ppl: 44.61; xent: 3.80; lr: 1.00000; 931/915 tok/s;    507 sec\n",
            "[2022-02-27 19:06:23,825 INFO] Step 1400/ 2000; acc:  48.13; ppl: 22.13; xent: 3.10; lr: 1.00000; 813/890 tok/s;    513 sec\n",
            "[2022-02-27 19:06:30,852 INFO] Step 1420/ 2000; acc:  45.56; ppl: 24.60; xent: 3.20; lr: 1.00000; 876/955 tok/s;    520 sec\n",
            "[2022-02-27 19:06:38,218 INFO] Step 1440/ 2000; acc:  45.95; ppl: 24.64; xent: 3.20; lr: 1.00000; 850/909 tok/s;    527 sec\n",
            "[2022-02-27 19:06:46,055 INFO] Step 1460/ 2000; acc:  42.01; ppl: 32.22; xent: 3.47; lr: 1.00000; 809/861 tok/s;    535 sec\n",
            "[2022-02-27 19:06:53,052 INFO] Step 1480/ 2000; acc:  45.92; ppl: 23.06; xent: 3.14; lr: 1.00000; 882/955 tok/s;    542 sec\n",
            "[2022-02-27 19:06:59,877 INFO] Step 1500/ 2000; acc:  48.18; ppl: 20.54; xent: 3.02; lr: 1.00000; 839/904 tok/s;    549 sec\n",
            "[2022-02-27 19:07:08,656 INFO] Step 1520/ 2000; acc:  40.53; ppl: 33.41; xent: 3.51; lr: 1.00000; 833/851 tok/s;    558 sec\n",
            "[2022-02-27 19:07:15,612 INFO] Step 1540/ 2000; acc:  46.32; ppl: 24.82; xent: 3.21; lr: 1.00000; 908/934 tok/s;    565 sec\n",
            "[2022-02-27 19:07:21,711 INFO] Step 1560/ 2000; acc:  49.85; ppl: 18.50; xent: 2.92; lr: 1.00000; 895/941 tok/s;    571 sec\n",
            "[2022-02-27 19:07:31,009 INFO] Step 1580/ 2000; acc:  39.41; ppl: 36.40; xent: 3.59; lr: 1.00000; 876/877 tok/s;    580 sec\n",
            "[2022-02-27 19:07:37,642 INFO] Step 1600/ 2000; acc:  47.70; ppl: 21.05; xent: 3.05; lr: 1.00000; 904/946 tok/s;    587 sec\n",
            "[2022-02-27 19:07:44,120 INFO] Step 1620/ 2000; acc:  48.71; ppl: 19.91; xent: 2.99; lr: 1.00000; 891/894 tok/s;    593 sec\n",
            "[2022-02-27 19:07:53,375 INFO] Step 1640/ 2000; acc:  39.52; ppl: 33.69; xent: 3.52; lr: 1.00000; 891/886 tok/s;    602 sec\n",
            "[2022-02-27 19:07:59,482 INFO] Step 1660/ 2000; acc:  48.79; ppl: 19.53; xent: 2.97; lr: 1.00000; 841/917 tok/s;    608 sec\n",
            "[2022-02-27 19:08:06,151 INFO] Step 1680/ 2000; acc:  47.40; ppl: 21.42; xent: 3.06; lr: 1.00000; 941/939 tok/s;    615 sec\n",
            "[2022-02-27 19:08:16,172 INFO] Step 1700/ 2000; acc:  39.03; ppl: 35.97; xent: 3.58; lr: 1.00000; 873/852 tok/s;    625 sec\n",
            "[2022-02-27 19:08:22,191 INFO] Step 1720/ 2000; acc:  50.51; ppl: 18.23; xent: 2.90; lr: 1.00000; 818/919 tok/s;    631 sec\n",
            "[2022-02-27 19:08:29,306 INFO] Step 1740/ 2000; acc:  47.13; ppl: 21.16; xent: 3.05; lr: 1.00000; 875/949 tok/s;    638 sec\n",
            "[2022-02-27 19:08:36,472 INFO] Step 1760/ 2000; acc:  46.32; ppl: 22.81; xent: 3.13; lr: 1.00000; 902/932 tok/s;    645 sec\n",
            "[2022-02-27 19:08:44,660 INFO] Step 1780/ 2000; acc:  42.09; ppl: 30.99; xent: 3.43; lr: 1.00000; 802/843 tok/s;    654 sec\n",
            "[2022-02-27 19:08:51,561 INFO] Step 1800/ 2000; acc:  46.98; ppl: 20.38; xent: 3.01; lr: 1.00000; 909/919 tok/s;    661 sec\n",
            "[2022-02-27 19:08:58,085 INFO] Step 1820/ 2000; acc:  49.93; ppl: 18.29; xent: 2.91; lr: 1.00000; 850/948 tok/s;    667 sec\n",
            "[2022-02-27 19:09:07,115 INFO] Step 1840/ 2000; acc:  41.94; ppl: 30.05; xent: 3.40; lr: 1.00000; 802/834 tok/s;    676 sec\n",
            "[2022-02-27 19:09:12,605 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-27 19:09:14,066 INFO] Step 1860/ 2000; acc:  47.44; ppl: 20.62; xent: 3.03; lr: 1.00000; 890/926 tok/s;    683 sec\n",
            "[2022-02-27 19:09:20,929 INFO] Validation perplexity: 13.5376\n",
            "[2022-02-27 19:09:20,930 INFO] Validation accuracy: 52.828\n",
            "[2022-02-27 19:09:20,991 INFO] Saving checkpoint ./models/2couches/model_step_1875.pt\n",
            "[2022-02-27 19:09:22,829 INFO] Step 1880/ 2000; acc:  50.69; ppl: 16.79; xent: 2.82; lr: 1.00000; 613/666 tok/s;    692 sec\n",
            "[2022-02-27 19:09:32,732 INFO] Step 1900/ 2000; acc:  40.66; ppl: 31.42; xent: 3.45; lr: 1.00000; 804/831 tok/s;    702 sec\n",
            "[2022-02-27 19:09:39,237 INFO] Step 1920/ 2000; acc:  47.68; ppl: 20.07; xent: 3.00; lr: 1.00000; 913/956 tok/s;    708 sec\n",
            "[2022-02-27 19:09:45,726 INFO] Step 1940/ 2000; acc:  51.96; ppl: 15.48; xent: 2.74; lr: 1.00000; 891/903 tok/s;    715 sec\n",
            "[2022-02-27 19:09:55,613 INFO] Step 1960/ 2000; acc:  40.77; ppl: 29.79; xent: 3.39; lr: 1.00000; 831/842 tok/s;    725 sec\n",
            "[2022-02-27 19:10:01,718 INFO] Step 1980/ 2000; acc:  49.97; ppl: 18.16; xent: 2.90; lr: 1.00000; 843/937 tok/s;    731 sec\n",
            "[2022-02-27 19:10:08,712 INFO] Step 2000/ 2000; acc:  47.43; ppl: 19.98; xent: 2.99; lr: 1.00000; 907/924 tok/s;    738 sec\n",
            "[2022-02-27 19:10:08,775 INFO] Saving checkpoint ./models/2couches/model_step_2000.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-2couche.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pMV29kYjZWy",
        "outputId": "324c2990-236c-4cd2-da28-411a3d239e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 21:50:13,043 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 21:50:17,764 INFO] PRED AVG SCORE: -1.2682, PRED PPL: 3.5543\n",
            "[2022-02-27 21:50:20,045 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 21:50:25,313 INFO] PRED AVG SCORE: -1.1636, PRED PPL: 3.2014\n",
            "[2022-02-27 21:50:27,540 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 21:50:33,178 INFO] PRED AVG SCORE: -1.0768, PRED PPL: 2.9354\n",
            "[2022-02-27 21:50:35,540 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 21:50:40,872 INFO] PRED AVG SCORE: -1.0543, PRED PPL: 2.8701\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/models/2couches\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa5gP09sjf2M",
        "outputId": "89effa77-147a-4321-b6ac-ab68a8e9babc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 7.49, 44.0/14.1/7.9/1.9 (BP=0.767, ratio=0.790, hyp_len=3010, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 11.65, 48.9/19.0/10.6/4.3 (BP=0.811, ratio=0.827, hyp_len=3150, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 13.79, 49.3/20.0/11.0/4.1 (BP=0.954, ratio=0.955, hyp_len=3636, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 14.79, 51.3/22.1/12.5/5.1 (BP=0.901, ratio=0.906, hyp_len=3449, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/2couches/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/2couches/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/2couches/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/2couches/pred2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40OdFI9NkXYn",
        "outputId": "fcc26b36-b679-4ff5-8323-746cc4ca67e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 21:52:48,477 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-27 21:52:48,482 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 21:52:48,482 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-27 21:52:48,483 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-27 21:52:48,484 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-27 21:52:48,484 INFO] Loading vocab from text file...\n",
            "[2022-02-27 21:52:48,484 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-27 21:52:48,515 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-27 21:52:48,520 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-27 21:52:48,541 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-27 21:52:48,545 INFO] Building fields with vocab in counters...\n",
            "[2022-02-27 21:52:48,557 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-27 21:52:48,573 INFO]  * src vocab size: 9980.\n",
            "[2022-02-27 21:52:48,574 INFO]  * src vocab size = 9980\n",
            "[2022-02-27 21:52:48,575 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-27 21:52:48,578 INFO] Building model...\n",
            "[2022-02-27 21:52:48,885 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, num_layers=3, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "        (1): LSTMCell(256, 256)\n",
            "        (2): LSTMCell(256, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-27 21:52:48,886 INFO] encoder: 6425648\n",
            "[2022-02-27 21:52:48,887 INFO] decoder: 8296894\n",
            "[2022-02-27 21:52:48,887 INFO] * number of parameters: 14722542\n",
            "[2022-02-27 21:52:48,889 INFO] Starting training on CPU, could be very slow\n",
            "[2022-02-27 21:52:48,890 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-27 21:52:48,890 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 21:52:48,890 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "[2022-02-27 21:52:58,462 INFO] Step 20/ 2000; acc:   6.64; ppl: 3319.06; xent: 8.11; lr: 1.00000; 700/738 tok/s;     10 sec\n",
            "[2022-02-27 21:53:06,738 INFO] Step 40/ 2000; acc:  10.18; ppl: 654.32; xent: 6.48; lr: 1.00000; 709/744 tok/s;     18 sec\n",
            "[2022-02-27 21:53:15,616 INFO] Step 60/ 2000; acc:   9.66; ppl: 436.28; xent: 6.08; lr: 1.00000; 706/734 tok/s;     27 sec\n",
            "[2022-02-27 21:53:26,902 INFO] Step 80/ 2000; acc:   6.82; ppl: 507.96; xent: 6.23; lr: 1.00000; 685/699 tok/s;     38 sec\n",
            "[2022-02-27 21:53:35,248 INFO] Step 100/ 2000; acc:  10.93; ppl: 325.13; xent: 5.78; lr: 1.00000; 727/730 tok/s;     46 sec\n",
            "[2022-02-27 21:53:43,301 INFO] Step 120/ 2000; acc:  11.16; ppl: 310.66; xent: 5.74; lr: 1.00000; 738/799 tok/s;     54 sec\n",
            "[2022-02-27 21:53:53,662 INFO] Step 140/ 2000; acc:   9.90; ppl: 398.67; xent: 5.99; lr: 1.00000; 741/749 tok/s;     65 sec\n",
            "[2022-02-27 21:54:02,354 INFO] Step 160/ 2000; acc:  12.19; ppl: 271.13; xent: 5.60; lr: 1.00000; 724/773 tok/s;     73 sec\n",
            "[2022-02-27 21:54:09,882 INFO] Step 180/ 2000; acc:  19.94; ppl: 186.28; xent: 5.23; lr: 1.00000; 746/813 tok/s;     81 sec\n",
            "[2022-02-27 21:54:19,283 INFO] Step 200/ 2000; acc:  17.16; ppl: 243.37; xent: 5.49; lr: 1.00000; 771/767 tok/s;     90 sec\n",
            "[2022-02-27 21:54:27,980 INFO] Step 220/ 2000; acc:  19.51; ppl: 170.18; xent: 5.14; lr: 1.00000; 727/769 tok/s;     99 sec\n",
            "[2022-02-27 21:54:35,225 INFO] Step 240/ 2000; acc:  22.50; ppl: 140.20; xent: 4.94; lr: 1.00000; 735/787 tok/s;    106 sec\n",
            "[2022-02-27 21:54:45,837 INFO] Step 260/ 2000; acc:  18.16; ppl: 187.92; xent: 5.24; lr: 1.00000; 663/706 tok/s;    117 sec\n",
            "[2022-02-27 21:54:54,057 INFO] Step 280/ 2000; acc:  21.54; ppl: 144.81; xent: 4.98; lr: 1.00000; 783/817 tok/s;    125 sec\n",
            "[2022-02-27 21:55:01,579 INFO] Step 300/ 2000; acc:  25.50; ppl: 120.53; xent: 4.79; lr: 1.00000; 718/776 tok/s;    133 sec\n",
            "[2022-02-27 21:55:11,326 INFO] Step 320/ 2000; acc:  23.96; ppl: 132.04; xent: 4.88; lr: 1.00000; 694/737 tok/s;    142 sec\n",
            "[2022-02-27 21:55:20,625 INFO] Step 340/ 2000; acc:  24.88; ppl: 115.65; xent: 4.75; lr: 1.00000; 735/762 tok/s;    152 sec\n",
            "[2022-02-27 21:55:28,475 INFO] Step 360/ 2000; acc:  29.25; ppl: 97.02; xent: 4.57; lr: 1.00000; 760/784 tok/s;    160 sec\n",
            "[2022-02-27 21:55:36,554 INFO] Step 380/ 2000; acc:  28.50; ppl: 95.45; xent: 4.56; lr: 1.00000; 790/770 tok/s;    168 sec\n",
            "[2022-02-27 21:55:47,097 INFO] Step 400/ 2000; acc:  24.26; ppl: 128.55; xent: 4.86; lr: 1.00000; 739/755 tok/s;    178 sec\n",
            "[2022-02-27 21:55:55,346 INFO] Step 420/ 2000; acc:  30.23; ppl: 80.64; xent: 4.39; lr: 1.00000; 737/741 tok/s;    186 sec\n",
            "[2022-02-27 21:56:03,178 INFO] Step 440/ 2000; acc:  31.16; ppl: 76.78; xent: 4.34; lr: 1.00000; 760/787 tok/s;    194 sec\n",
            "[2022-02-27 21:56:13,631 INFO] Step 460/ 2000; acc:  25.48; ppl: 119.00; xent: 4.78; lr: 1.00000; 731/749 tok/s;    205 sec\n",
            "[2022-02-27 21:56:22,439 INFO] Step 480/ 2000; acc:  30.11; ppl: 76.65; xent: 4.34; lr: 1.00000; 712/764 tok/s;    214 sec\n",
            "[2022-02-27 21:56:29,978 INFO] Step 500/ 2000; acc:  34.44; ppl: 61.88; xent: 4.13; lr: 1.00000; 745/778 tok/s;    221 sec\n",
            "[2022-02-27 21:56:40,122 INFO] Step 520/ 2000; acc:  29.85; ppl: 82.97; xent: 4.42; lr: 1.00000; 716/707 tok/s;    231 sec\n",
            "[2022-02-27 21:56:48,489 INFO] Step 540/ 2000; acc:  32.23; ppl: 67.66; xent: 4.21; lr: 1.00000; 766/804 tok/s;    240 sec\n",
            "[2022-02-27 21:56:55,624 INFO] Step 560/ 2000; acc:  35.06; ppl: 54.45; xent: 4.00; lr: 1.00000; 753/816 tok/s;    247 sec\n",
            "[2022-02-27 21:57:03,766 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-27 21:57:05,881 INFO] Step 580/ 2000; acc:  29.81; ppl: 76.81; xent: 4.34; lr: 1.00000; 691/735 tok/s;    257 sec\n",
            "[2022-02-27 21:57:14,573 INFO] Step 600/ 2000; acc:  33.57; ppl: 61.40; xent: 4.12; lr: 1.00000; 739/768 tok/s;    266 sec\n",
            "[2022-02-27 21:57:22,226 INFO] Step 620/ 2000; acc:  34.58; ppl: 57.33; xent: 4.05; lr: 1.00000; 702/771 tok/s;    273 sec\n",
            "[2022-02-27 21:57:24,449 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-27 21:57:26,805 INFO] Validation perplexity: 35.194\n",
            "[2022-02-27 21:57:26,805 INFO] Validation accuracy: 40.751\n",
            "[2022-02-27 21:57:26,867 INFO] Saving checkpoint ./models/3couches/model_step_625.pt\n",
            "[2022-02-27 21:57:34,850 INFO] Step 640/ 2000; acc:  32.53; ppl: 65.55; xent: 4.18; lr: 1.00000; 534/559 tok/s;    286 sec\n",
            "[2022-02-27 21:57:43,842 INFO] Step 660/ 2000; acc:  31.75; ppl: 65.17; xent: 4.18; lr: 1.00000; 742/770 tok/s;    295 sec\n",
            "[2022-02-27 21:57:51,679 INFO] Step 680/ 2000; acc:  35.27; ppl: 53.49; xent: 3.98; lr: 1.00000; 745/792 tok/s;    303 sec\n",
            "[2022-02-27 21:57:59,857 INFO] Step 700/ 2000; acc:  35.86; ppl: 47.27; xent: 3.86; lr: 1.00000; 762/790 tok/s;    311 sec\n",
            "[2022-02-27 21:58:11,101 INFO] Step 720/ 2000; acc:  30.94; ppl: 65.43; xent: 4.18; lr: 1.00000; 694/705 tok/s;    322 sec\n",
            "[2022-02-27 21:58:19,005 INFO] Step 740/ 2000; acc:  36.79; ppl: 49.27; xent: 3.90; lr: 1.00000; 775/774 tok/s;    330 sec\n",
            "[2022-02-27 21:58:26,958 INFO] Step 760/ 2000; acc:  36.46; ppl: 47.27; xent: 3.86; lr: 1.00000; 751/810 tok/s;    338 sec\n",
            "[2022-02-27 21:58:37,223 INFO] Step 780/ 2000; acc:  31.53; ppl: 69.92; xent: 4.25; lr: 1.00000; 744/760 tok/s;    348 sec\n",
            "[2022-02-27 21:58:45,606 INFO] Step 800/ 2000; acc:  35.36; ppl: 50.40; xent: 3.92; lr: 1.00000; 732/790 tok/s;    357 sec\n",
            "[2022-02-27 21:58:52,795 INFO] Step 820/ 2000; acc:  38.77; ppl: 40.10; xent: 3.69; lr: 1.00000; 773/818 tok/s;    364 sec\n",
            "[2022-02-27 21:59:02,376 INFO] Step 840/ 2000; acc:  34.67; ppl: 56.25; xent: 4.03; lr: 1.00000; 742/772 tok/s;    373 sec\n",
            "[2022-02-27 21:59:10,682 INFO] Step 860/ 2000; acc:  36.33; ppl: 45.65; xent: 3.82; lr: 1.00000; 767/817 tok/s;    382 sec\n",
            "[2022-02-27 21:59:17,804 INFO] Step 880/ 2000; acc:  40.63; ppl: 38.29; xent: 3.65; lr: 1.00000; 750/821 tok/s;    389 sec\n",
            "[2022-02-27 21:59:28,198 INFO] Step 900/ 2000; acc:  33.60; ppl: 53.92; xent: 3.99; lr: 1.00000; 691/714 tok/s;    399 sec\n",
            "[2022-02-27 21:59:36,588 INFO] Step 920/ 2000; acc:  37.25; ppl: 41.58; xent: 3.73; lr: 1.00000; 769/834 tok/s;    408 sec\n",
            "[2022-02-27 21:59:43,938 INFO] Step 940/ 2000; acc:  40.86; ppl: 37.90; xent: 3.63; lr: 1.00000; 747/783 tok/s;    415 sec\n",
            "[2022-02-27 21:59:53,225 INFO] Step 960/ 2000; acc:  36.35; ppl: 49.94; xent: 3.91; lr: 1.00000; 747/749 tok/s;    424 sec\n",
            "[2022-02-27 22:00:02,158 INFO] Step 980/ 2000; acc:  36.18; ppl: 42.82; xent: 3.76; lr: 1.00000; 758/780 tok/s;    433 sec\n",
            "[2022-02-27 22:00:10,027 INFO] Step 1000/ 2000; acc:  39.35; ppl: 37.21; xent: 3.62; lr: 1.00000; 749/773 tok/s;    441 sec\n",
            "[2022-02-27 22:00:18,240 INFO] Step 1020/ 2000; acc:  38.82; ppl: 39.12; xent: 3.67; lr: 1.00000; 769/762 tok/s;    449 sec\n",
            "[2022-02-27 22:00:28,762 INFO] Step 1040/ 2000; acc:  33.48; ppl: 53.87; xent: 3.99; lr: 1.00000; 742/744 tok/s;    460 sec\n",
            "[2022-02-27 22:00:36,471 INFO] Step 1060/ 2000; acc:  39.63; ppl: 36.55; xent: 3.60; lr: 1.00000; 800/789 tok/s;    468 sec\n",
            "[2022-02-27 22:00:44,239 INFO] Step 1080/ 2000; acc:  39.82; ppl: 35.66; xent: 3.57; lr: 1.00000; 772/818 tok/s;    475 sec\n",
            "[2022-02-27 22:00:54,728 INFO] Step 1100/ 2000; acc:  32.85; ppl: 57.18; xent: 4.05; lr: 1.00000; 737/760 tok/s;    486 sec\n",
            "[2022-02-27 22:01:03,235 INFO] Step 1120/ 2000; acc:  38.26; ppl: 39.38; xent: 3.67; lr: 1.00000; 733/795 tok/s;    494 sec\n",
            "[2022-02-27 22:01:10,673 INFO] Step 1140/ 2000; acc:  41.36; ppl: 33.58; xent: 3.51; lr: 1.00000; 751/801 tok/s;    502 sec\n",
            "[2022-02-27 22:01:20,187 INFO] Step 1160/ 2000; acc:  37.40; ppl: 43.65; xent: 3.78; lr: 1.00000; 752/748 tok/s;    511 sec\n",
            "[2022-02-27 22:01:28,668 INFO] Step 1180/ 2000; acc:  39.74; ppl: 35.58; xent: 3.57; lr: 1.00000; 748/780 tok/s;    520 sec\n",
            "[2022-02-27 22:01:35,747 INFO] Step 1200/ 2000; acc:  42.76; ppl: 32.01; xent: 3.47; lr: 1.00000; 754/792 tok/s;    527 sec\n",
            "[2022-02-27 22:01:43,705 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-27 22:01:45,799 INFO] Step 1220/ 2000; acc:  36.51; ppl: 43.28; xent: 3.77; lr: 1.00000; 699/775 tok/s;    537 sec\n",
            "[2022-02-27 22:01:54,263 INFO] Step 1240/ 2000; acc:  38.55; ppl: 38.98; xent: 3.66; lr: 1.00000; 765/801 tok/s;    545 sec\n",
            "[2022-02-27 22:02:00,805 INFO] Validation perplexity: 20.7121\n",
            "[2022-02-27 22:02:00,806 INFO] Validation accuracy: 47.7747\n",
            "[2022-02-27 22:02:00,864 INFO] Saving checkpoint ./models/3couches/model_step_1250.pt\n",
            "[2022-02-27 22:02:04,658 INFO] Step 1260/ 2000; acc:  41.45; ppl: 34.15; xent: 3.53; lr: 1.00000; 523/549 tok/s;    556 sec\n",
            "[2022-02-27 22:02:14,283 INFO] Step 1280/ 2000; acc:  38.02; ppl: 38.83; xent: 3.66; lr: 1.00000; 701/740 tok/s;    565 sec\n",
            "[2022-02-27 22:02:23,110 INFO] Step 1300/ 2000; acc:  39.18; ppl: 36.33; xent: 3.59; lr: 1.00000; 765/783 tok/s;    574 sec\n",
            "[2022-02-27 22:02:30,960 INFO] Step 1320/ 2000; acc:  41.70; ppl: 31.58; xent: 3.45; lr: 1.00000; 755/787 tok/s;    582 sec\n",
            "[2022-02-27 22:02:39,465 INFO] Step 1340/ 2000; acc:  40.63; ppl: 33.28; xent: 3.50; lr: 1.00000; 742/767 tok/s;    591 sec\n",
            "[2022-02-27 22:02:50,883 INFO] Step 1360/ 2000; acc:  35.54; ppl: 45.97; xent: 3.83; lr: 1.00000; 680/698 tok/s;    602 sec\n",
            "[2022-02-27 22:02:59,022 INFO] Step 1380/ 2000; acc:  43.10; ppl: 30.70; xent: 3.42; lr: 1.00000; 748/752 tok/s;    610 sec\n",
            "[2022-02-27 22:03:06,819 INFO] Step 1400/ 2000; acc:  42.18; ppl: 30.49; xent: 3.42; lr: 1.00000; 761/829 tok/s;    618 sec\n",
            "[2022-02-27 22:03:16,727 INFO] Step 1420/ 2000; acc:  37.26; ppl: 43.31; xent: 3.77; lr: 1.00000; 766/767 tok/s;    628 sec\n",
            "[2022-02-27 22:03:25,074 INFO] Step 1440/ 2000; acc:  40.81; ppl: 32.07; xent: 3.47; lr: 1.00000; 723/781 tok/s;    636 sec\n",
            "[2022-02-27 22:03:32,453 INFO] Step 1460/ 2000; acc:  44.43; ppl: 26.67; xent: 3.28; lr: 1.00000; 740/793 tok/s;    644 sec\n",
            "[2022-02-27 22:03:42,120 INFO] Step 1480/ 2000; acc:  39.47; ppl: 36.16; xent: 3.59; lr: 1.00000; 724/782 tok/s;    653 sec\n",
            "[2022-02-27 22:03:50,567 INFO] Step 1500/ 2000; acc:  41.49; ppl: 31.53; xent: 3.45; lr: 1.00000; 762/816 tok/s;    662 sec\n",
            "[2022-02-27 22:03:57,797 INFO] Step 1520/ 2000; acc:  45.31; ppl: 25.46; xent: 3.24; lr: 1.00000; 743/819 tok/s;    669 sec\n",
            "[2022-02-27 22:04:07,914 INFO] Step 1540/ 2000; acc:  39.12; ppl: 37.93; xent: 3.64; lr: 1.00000; 719/711 tok/s;    679 sec\n",
            "[2022-02-27 22:04:16,491 INFO] Step 1560/ 2000; acc:  41.17; ppl: 30.11; xent: 3.40; lr: 1.00000; 764/796 tok/s;    688 sec\n",
            "[2022-02-27 22:04:23,724 INFO] Step 1580/ 2000; acc:  45.13; ppl: 25.09; xent: 3.22; lr: 1.00000; 764/814 tok/s;    695 sec\n",
            "[2022-02-27 22:04:33,745 INFO] Step 1600/ 2000; acc:  39.56; ppl: 35.00; xent: 3.56; lr: 1.00000; 698/707 tok/s;    705 sec\n",
            "[2022-02-27 22:04:42,617 INFO] Step 1620/ 2000; acc:  40.90; ppl: 30.79; xent: 3.43; lr: 1.00000; 767/795 tok/s;    714 sec\n",
            "[2022-02-27 22:04:50,528 INFO] Step 1640/ 2000; acc:  43.40; ppl: 27.17; xent: 3.30; lr: 1.00000; 747/770 tok/s;    722 sec\n",
            "[2022-02-27 22:04:58,457 INFO] Step 1660/ 2000; acc:  42.93; ppl: 27.97; xent: 3.33; lr: 1.00000; 800/790 tok/s;    730 sec\n",
            "[2022-02-27 22:05:08,943 INFO] Step 1680/ 2000; acc:  38.30; ppl: 38.18; xent: 3.64; lr: 1.00000; 741/757 tok/s;    740 sec\n",
            "[2022-02-27 22:05:16,842 INFO] Step 1700/ 2000; acc:  43.99; ppl: 27.08; xent: 3.30; lr: 1.00000; 770/776 tok/s;    748 sec\n",
            "[2022-02-27 22:05:24,507 INFO] Step 1720/ 2000; acc:  44.85; ppl: 25.02; xent: 3.22; lr: 1.00000; 776/802 tok/s;    756 sec\n",
            "[2022-02-27 22:05:34,959 INFO] Step 1740/ 2000; acc:  37.30; ppl: 40.86; xent: 3.71; lr: 1.00000; 730/738 tok/s;    766 sec\n",
            "[2022-02-27 22:05:43,281 INFO] Step 1760/ 2000; acc:  43.06; ppl: 26.96; xent: 3.29; lr: 1.00000; 747/817 tok/s;    774 sec\n",
            "[2022-02-27 22:05:50,547 INFO] Step 1780/ 2000; acc:  44.73; ppl: 25.30; xent: 3.23; lr: 1.00000; 766/817 tok/s;    782 sec\n",
            "[2022-02-27 22:06:00,407 INFO] Step 1800/ 2000; acc:  40.43; ppl: 32.47; xent: 3.48; lr: 1.00000; 724/733 tok/s;    792 sec\n",
            "[2022-02-27 22:06:08,690 INFO] Step 1820/ 2000; acc:  43.05; ppl: 25.69; xent: 3.25; lr: 1.00000; 764/801 tok/s;    800 sec\n",
            "[2022-02-27 22:06:15,758 INFO] Step 1840/ 2000; acc:  46.62; ppl: 23.51; xent: 3.16; lr: 1.00000; 755/795 tok/s;    807 sec\n",
            "[2022-02-27 22:06:23,613 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-27 22:06:25,853 INFO] Step 1860/ 2000; acc:  39.85; ppl: 33.55; xent: 3.51; lr: 1.00000; 691/764 tok/s;    817 sec\n",
            "[2022-02-27 22:06:34,357 INFO] Validation perplexity: 16.2718\n",
            "[2022-02-27 22:06:34,358 INFO] Validation accuracy: 50.7418\n",
            "[2022-02-27 22:06:34,416 INFO] Saving checkpoint ./models/3couches/model_step_1875.pt\n",
            "[2022-02-27 22:06:37,085 INFO] Step 1880/ 2000; acc:  41.99; ppl: 28.34; xent: 3.34; lr: 1.00000; 579/607 tok/s;    828 sec\n",
            "[2022-02-27 22:06:44,820 INFO] Step 1900/ 2000; acc:  45.13; ppl: 24.90; xent: 3.21; lr: 1.00000; 705/750 tok/s;    836 sec\n",
            "[2022-02-27 22:06:54,581 INFO] Step 1920/ 2000; acc:  41.16; ppl: 30.18; xent: 3.41; lr: 1.00000; 704/730 tok/s;    846 sec\n",
            "[2022-02-27 22:07:03,587 INFO] Step 1940/ 2000; acc:  41.85; ppl: 29.90; xent: 3.40; lr: 1.00000; 756/776 tok/s;    855 sec\n",
            "[2022-02-27 22:07:11,430 INFO] Step 1960/ 2000; acc:  45.73; ppl: 23.32; xent: 3.15; lr: 1.00000; 758/794 tok/s;    863 sec\n",
            "[2022-02-27 22:07:19,479 INFO] Step 1980/ 2000; acc:  43.63; ppl: 25.14; xent: 3.22; lr: 1.00000; 789/810 tok/s;    871 sec\n",
            "[2022-02-27 22:07:30,850 INFO] Step 2000/ 2000; acc:  39.14; ppl: 33.60; xent: 3.51; lr: 1.00000; 684/707 tok/s;    882 sec\n",
            "[2022-02-27 22:07:30,910 INFO] Saving checkpoint ./models/3couches/model_step_2000.pt\n",
            "[2022-02-27 22:07:33,559 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:07:37,782 INFO] PRED AVG SCORE: -1.3378, PRED PPL: 3.8107\n",
            "[2022-02-27 22:07:40,038 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:07:45,891 INFO] PRED AVG SCORE: -1.2814, PRED PPL: 3.6018\n",
            "[2022-02-27 22:07:47,941 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:07:54,257 INFO] PRED AVG SCORE: -1.0694, PRED PPL: 2.9135\n",
            "[2022-02-27 22:07:56,766 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:08:05,449 INFO] PRED AVG SCORE: -1.1404, PRED PPL: 3.1279\n",
            "BLEU = 4.42, 45.8/12.1/6.8/1.7 (BP=0.493, ratio=0.586, hyp_len=2230, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 8.76, 41.8/13.4/7.4/2.4 (BP=0.877, ratio=0.884, hyp_len=3365, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 10.79, 44.6/16.1/8.4/2.8 (BP=0.944, ratio=0.946, hyp_len=3602, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 11.23, 50.1/19.3/10.9/4.2 (BP=0.773, ratio=0.795, hyp_len=3028, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-3couche.yaml\n",
        "cd models/3couches\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/3couches/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/3couches/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/3couches/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/3couches/pred2000.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SvCvazrjI_o"
      },
      "source": [
        "**Q12:** En augmentant le nombre d'unités pour l'encodeur et le décodeur, on obtient de meilleurs résultats. Ces résultats varient d'une execution à l'autre mais on remarqué de meilleurs résultats pour 384 unités. <span style=\"color:red\"> Pourquoi? </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doj2EVqdLAOp",
        "outputId": "ab889961-3eff-40d9-fedc-7af1f994adb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 22:12:33,981 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-27 22:12:33,983 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 22:12:33,983 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-27 22:12:33,984 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-27 22:12:33,984 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-27 22:12:33,984 INFO] Loading vocab from text file...\n",
            "[2022-02-27 22:12:33,984 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-27 22:12:34,009 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-27 22:12:34,015 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-27 22:12:34,035 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-27 22:12:34,039 INFO] Building fields with vocab in counters...\n",
            "[2022-02-27 22:12:34,050 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-27 22:12:34,065 INFO]  * src vocab size: 9980.\n",
            "[2022-02-27 22:12:34,066 INFO]  * src vocab size = 9980\n",
            "[2022-02-27 22:12:34,066 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-27 22:12:34,070 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:12:34,317 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 192, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(884, 384)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-27 22:12:34,318 INFO] encoder: 6055984\n",
            "[2022-02-27 22:12:34,318 INFO] decoder: 9205950\n",
            "[2022-02-27 22:12:34,318 INFO] * number of parameters: 15261934\n",
            "[2022-02-27 22:12:34,320 INFO] Starting training on CPU, could be very slow\n",
            "[2022-02-27 22:12:34,321 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-27 22:12:34,321 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 22:12:34,322 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "[2022-02-27 22:12:41,708 INFO] Step 20/ 2000; acc:  10.37; ppl: 2266.06; xent: 7.73; lr: 1.00000; 755/793 tok/s;      7 sec\n",
            "[2022-02-27 22:12:51,723 INFO] Step 40/ 2000; acc:  14.58; ppl: 507.40; xent: 6.23; lr: 1.00000; 744/771 tok/s;     17 sec\n",
            "[2022-02-27 22:13:00,342 INFO] Step 60/ 2000; acc:  17.75; ppl: 311.18; xent: 5.74; lr: 1.00000; 722/762 tok/s;     26 sec\n",
            "[2022-02-27 22:13:08,030 INFO] Step 80/ 2000; acc:  20.84; ppl: 195.75; xent: 5.28; lr: 1.00000; 764/823 tok/s;     34 sec\n",
            "[2022-02-27 22:13:17,235 INFO] Step 100/ 2000; acc:  20.14; ppl: 201.38; xent: 5.31; lr: 1.00000; 780/805 tok/s;     43 sec\n",
            "[2022-02-27 22:13:25,619 INFO] Step 120/ 2000; acc:  22.97; ppl: 166.07; xent: 5.11; lr: 1.00000; 809/813 tok/s;     51 sec\n",
            "[2022-02-27 22:13:33,341 INFO] Step 140/ 2000; acc:  27.20; ppl: 121.70; xent: 4.80; lr: 1.00000; 767/815 tok/s;     59 sec\n",
            "[2022-02-27 22:13:40,596 INFO] Step 160/ 2000; acc:  29.28; ppl: 99.00; xent: 4.60; lr: 1.00000; 806/867 tok/s;     66 sec\n",
            "[2022-02-27 22:13:50,281 INFO] Step 180/ 2000; acc:  25.18; ppl: 142.29; xent: 4.96; lr: 1.00000; 802/816 tok/s;     76 sec\n",
            "[2022-02-27 22:13:57,875 INFO] Step 200/ 2000; acc:  32.84; ppl: 85.01; xent: 4.44; lr: 1.00000; 786/799 tok/s;     84 sec\n",
            "[2022-02-27 22:14:05,252 INFO] Step 220/ 2000; acc:  33.81; ppl: 74.89; xent: 4.32; lr: 1.00000; 795/811 tok/s;     91 sec\n",
            "[2022-02-27 22:14:15,128 INFO] Step 240/ 2000; acc:  28.74; ppl: 98.51; xent: 4.59; lr: 1.00000; 746/788 tok/s;    101 sec\n",
            "[2022-02-27 22:14:22,654 INFO] Step 260/ 2000; acc:  35.71; ppl: 65.57; xent: 4.18; lr: 1.00000; 754/816 tok/s;    108 sec\n",
            "[2022-02-27 22:14:29,845 INFO] Step 280/ 2000; acc:  38.19; ppl: 54.60; xent: 4.00; lr: 1.00000; 757/811 tok/s;    116 sec\n",
            "[2022-02-27 22:14:39,692 INFO] Step 300/ 2000; acc:  30.72; ppl: 88.81; xent: 4.49; lr: 1.00000; 779/787 tok/s;    125 sec\n",
            "[2022-02-27 22:14:47,601 INFO] Step 320/ 2000; acc:  33.52; ppl: 64.81; xent: 4.17; lr: 1.00000; 771/867 tok/s;    133 sec\n",
            "[2022-02-27 22:14:54,938 INFO] Step 340/ 2000; acc:  37.97; ppl: 49.24; xent: 3.90; lr: 1.00000; 771/827 tok/s;    141 sec\n",
            "[2022-02-27 22:15:04,744 INFO] Step 360/ 2000; acc:  31.88; ppl: 73.34; xent: 4.30; lr: 1.00000; 771/778 tok/s;    150 sec\n",
            "[2022-02-27 22:15:12,545 INFO] Step 380/ 2000; acc:  35.84; ppl: 57.43; xent: 4.05; lr: 1.00000; 820/808 tok/s;    158 sec\n",
            "[2022-02-27 22:15:20,011 INFO] Step 400/ 2000; acc:  39.15; ppl: 43.22; xent: 3.77; lr: 1.00000; 790/828 tok/s;    166 sec\n",
            "[2022-02-27 22:15:29,193 INFO] Step 420/ 2000; acc:  34.88; ppl: 63.53; xent: 4.15; lr: 1.00000; 794/805 tok/s;    175 sec\n",
            "[2022-02-27 22:15:37,354 INFO] Step 440/ 2000; acc:  37.28; ppl: 52.61; xent: 3.96; lr: 1.00000; 831/815 tok/s;    183 sec\n",
            "[2022-02-27 22:15:44,973 INFO] Step 460/ 2000; acc:  39.13; ppl: 45.42; xent: 3.82; lr: 1.00000; 772/810 tok/s;    191 sec\n",
            "[2022-02-27 22:15:52,285 INFO] Step 480/ 2000; acc:  39.72; ppl: 42.54; xent: 3.75; lr: 1.00000; 798/834 tok/s;    198 sec\n",
            "[2022-02-27 22:16:02,617 INFO] Step 500/ 2000; acc:  33.90; ppl: 63.76; xent: 4.16; lr: 1.00000; 744/752 tok/s;    208 sec\n",
            "[2022-02-27 22:16:10,541 INFO] Step 520/ 2000; acc:  40.09; ppl: 42.40; xent: 3.75; lr: 1.00000; 745/797 tok/s;    216 sec\n",
            "[2022-02-27 22:16:18,590 INFO] Step 540/ 2000; acc:  41.29; ppl: 40.67; xent: 3.71; lr: 1.00000; 742/770 tok/s;    224 sec\n",
            "[2022-02-27 22:16:28,462 INFO] Step 560/ 2000; acc:  36.20; ppl: 55.13; xent: 4.01; lr: 1.00000; 763/784 tok/s;    234 sec\n",
            "[2022-02-27 22:16:34,803 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-27 22:16:36,426 INFO] Step 580/ 2000; acc:  41.39; ppl: 35.30; xent: 3.56; lr: 1.00000; 709/801 tok/s;    242 sec\n",
            "[2022-02-27 22:16:43,736 INFO] Step 600/ 2000; acc:  44.81; ppl: 31.01; xent: 3.43; lr: 1.00000; 744/791 tok/s;    249 sec\n",
            "[2022-02-27 22:16:53,807 INFO] Step 620/ 2000; acc:  36.49; ppl: 54.91; xent: 4.01; lr: 1.00000; 757/754 tok/s;    259 sec\n",
            "[2022-02-27 22:16:55,908 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-27 22:16:58,273 INFO] Validation perplexity: 21.1506\n",
            "[2022-02-27 22:16:58,273 INFO] Validation accuracy: 48.841\n",
            "[2022-02-27 22:16:58,333 INFO] Saving checkpoint ./models/384unit/model_step_625.pt\n",
            "[2022-02-27 22:17:04,894 INFO] Step 640/ 2000; acc:  39.93; ppl: 38.77; xent: 3.66; lr: 1.00000; 551/606 tok/s;    271 sec\n",
            "[2022-02-27 22:17:12,038 INFO] Step 660/ 2000; acc:  46.13; ppl: 29.32; xent: 3.38; lr: 1.00000; 781/814 tok/s;    278 sec\n",
            "[2022-02-27 22:17:22,300 INFO] Step 680/ 2000; acc:  37.21; ppl: 45.96; xent: 3.83; lr: 1.00000; 718/735 tok/s;    288 sec\n",
            "[2022-02-27 22:17:30,695 INFO] Step 700/ 2000; acc:  41.84; ppl: 34.46; xent: 3.54; lr: 1.00000; 739/781 tok/s;    296 sec\n",
            "[2022-02-27 22:17:38,372 INFO] Step 720/ 2000; acc:  43.67; ppl: 29.49; xent: 3.38; lr: 1.00000; 768/834 tok/s;    304 sec\n",
            "[2022-02-27 22:17:47,700 INFO] Step 740/ 2000; acc:  39.63; ppl: 42.57; xent: 3.75; lr: 1.00000; 787/808 tok/s;    313 sec\n",
            "[2022-02-27 22:17:56,058 INFO] Step 760/ 2000; acc:  41.24; ppl: 37.25; xent: 3.62; lr: 1.00000; 819/813 tok/s;    322 sec\n",
            "[2022-02-27 22:18:03,827 INFO] Step 780/ 2000; acc:  45.64; ppl: 28.24; xent: 3.34; lr: 1.00000; 755/804 tok/s;    330 sec\n",
            "[2022-02-27 22:18:11,180 INFO] Step 800/ 2000; acc:  46.20; ppl: 27.48; xent: 3.31; lr: 1.00000; 781/824 tok/s;    337 sec\n",
            "[2022-02-27 22:18:20,831 INFO] Step 820/ 2000; acc:  39.09; ppl: 43.50; xent: 3.77; lr: 1.00000; 785/808 tok/s;    347 sec\n",
            "[2022-02-27 22:18:28,331 INFO] Step 840/ 2000; acc:  46.04; ppl: 25.88; xent: 3.25; lr: 1.00000; 781/839 tok/s;    354 sec\n",
            "[2022-02-27 22:18:35,586 INFO] Step 860/ 2000; acc:  47.52; ppl: 26.69; xent: 3.28; lr: 1.00000; 818/820 tok/s;    361 sec\n",
            "[2022-02-27 22:18:45,608 INFO] Step 880/ 2000; acc:  40.51; ppl: 35.05; xent: 3.56; lr: 1.00000; 744/775 tok/s;    371 sec\n",
            "[2022-02-27 22:18:52,976 INFO] Step 900/ 2000; acc:  46.92; ppl: 24.23; xent: 3.19; lr: 1.00000; 775/858 tok/s;    379 sec\n",
            "[2022-02-27 22:18:59,903 INFO] Step 920/ 2000; acc:  47.85; ppl: 23.35; xent: 3.15; lr: 1.00000; 795/849 tok/s;    386 sec\n",
            "[2022-02-27 22:19:09,734 INFO] Step 940/ 2000; acc:  39.80; ppl: 38.92; xent: 3.66; lr: 1.00000; 794/804 tok/s;    395 sec\n",
            "[2022-02-27 22:19:18,043 INFO] Step 960/ 2000; acc:  45.84; ppl: 25.28; xent: 3.23; lr: 1.00000; 745/801 tok/s;    404 sec\n",
            "[2022-02-27 22:19:25,570 INFO] Step 980/ 2000; acc:  48.58; ppl: 21.38; xent: 3.06; lr: 1.00000; 746/788 tok/s;    411 sec\n",
            "[2022-02-27 22:19:35,042 INFO] Step 1000/ 2000; acc:  41.96; ppl: 30.58; xent: 3.42; lr: 1.00000; 778/804 tok/s;    421 sec\n",
            "[2022-02-27 22:19:43,209 INFO] Step 1020/ 2000; acc:  45.75; ppl: 25.87; xent: 3.25; lr: 1.00000; 776/761 tok/s;    429 sec\n",
            "[2022-02-27 22:19:50,908 INFO] Step 1040/ 2000; acc:  48.32; ppl: 22.21; xent: 3.10; lr: 1.00000; 767/812 tok/s;    437 sec\n",
            "[2022-02-27 22:20:00,361 INFO] Step 1060/ 2000; acc:  42.06; ppl: 32.77; xent: 3.49; lr: 1.00000; 790/787 tok/s;    446 sec\n",
            "[2022-02-27 22:20:08,615 INFO] Step 1080/ 2000; acc:  45.26; ppl: 25.89; xent: 3.25; lr: 1.00000; 833/820 tok/s;    454 sec\n",
            "[2022-02-27 22:20:16,327 INFO] Step 1100/ 2000; acc:  47.84; ppl: 21.94; xent: 3.09; lr: 1.00000; 766/818 tok/s;    462 sec\n",
            "[2022-02-27 22:20:23,618 INFO] Step 1120/ 2000; acc:  48.28; ppl: 22.52; xent: 3.11; lr: 1.00000; 800/828 tok/s;    469 sec\n",
            "[2022-02-27 22:20:33,369 INFO] Step 1140/ 2000; acc:  41.21; ppl: 33.06; xent: 3.50; lr: 1.00000; 776/805 tok/s;    479 sec\n",
            "[2022-02-27 22:20:41,011 INFO] Step 1160/ 2000; acc:  49.94; ppl: 19.79; xent: 2.99; lr: 1.00000; 766/821 tok/s;    487 sec\n",
            "[2022-02-27 22:20:48,779 INFO] Step 1180/ 2000; acc:  48.23; ppl: 21.48; xent: 3.07; lr: 1.00000; 752/815 tok/s;    494 sec\n",
            "[2022-02-27 22:20:58,430 INFO] Step 1200/ 2000; acc:  43.17; ppl: 29.78; xent: 3.39; lr: 1.00000; 777/792 tok/s;    504 sec\n",
            "[2022-02-27 22:21:04,361 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-27 22:21:05,890 INFO] Step 1220/ 2000; acc:  49.03; ppl: 19.63; xent: 2.98; lr: 1.00000; 762/805 tok/s;    512 sec\n",
            "[2022-02-27 22:21:13,036 INFO] Step 1240/ 2000; acc:  50.68; ppl: 18.87; xent: 2.94; lr: 1.00000; 767/831 tok/s;    519 sec\n",
            "[2022-02-27 22:21:19,290 INFO] Validation perplexity: 12.8962\n",
            "[2022-02-27 22:21:19,290 INFO] Validation accuracy: 55.5633\n",
            "[2022-02-27 22:21:19,349 INFO] Saving checkpoint ./models/384unit/model_step_1250.pt\n",
            "[2022-02-27 22:21:25,616 INFO] Step 1260/ 2000; acc:  43.06; ppl: 30.08; xent: 3.40; lr: 1.00000; 605/606 tok/s;    531 sec\n",
            "[2022-02-27 22:21:33,858 INFO] Step 1280/ 2000; acc:  48.17; ppl: 20.45; xent: 3.02; lr: 1.00000; 747/820 tok/s;    540 sec\n",
            "[2022-02-27 22:21:41,065 INFO] Step 1300/ 2000; acc:  51.96; ppl: 17.23; xent: 2.85; lr: 1.00000; 778/824 tok/s;    547 sec\n",
            "[2022-02-27 22:21:51,042 INFO] Step 1320/ 2000; acc:  44.28; ppl: 26.14; xent: 3.26; lr: 1.00000; 744/774 tok/s;    557 sec\n",
            "[2022-02-27 22:21:58,958 INFO] Step 1340/ 2000; acc:  48.96; ppl: 19.70; xent: 2.98; lr: 1.00000; 799/810 tok/s;    565 sec\n",
            "[2022-02-27 22:22:06,586 INFO] Step 1360/ 2000; acc:  49.65; ppl: 18.70; xent: 2.93; lr: 1.00000; 775/827 tok/s;    572 sec\n",
            "[2022-02-27 22:22:15,716 INFO] Step 1380/ 2000; acc:  45.58; ppl: 25.98; xent: 3.26; lr: 1.00000; 805/825 tok/s;    581 sec\n",
            "[2022-02-27 22:22:23,832 INFO] Step 1400/ 2000; acc:  47.55; ppl: 22.38; xent: 3.11; lr: 1.00000; 838/840 tok/s;    590 sec\n",
            "[2022-02-27 22:22:31,339 INFO] Step 1420/ 2000; acc:  51.18; ppl: 17.29; xent: 2.85; lr: 1.00000; 771/826 tok/s;    597 sec\n",
            "[2022-02-27 22:22:38,461 INFO] Step 1440/ 2000; acc:  52.94; ppl: 16.50; xent: 2.80; lr: 1.00000; 792/830 tok/s;    604 sec\n",
            "[2022-02-27 22:22:48,093 INFO] Step 1460/ 2000; acc:  45.33; ppl: 24.51; xent: 3.20; lr: 1.00000; 769/804 tok/s;    614 sec\n",
            "[2022-02-27 22:22:55,490 INFO] Step 1480/ 2000; acc:  52.05; ppl: 15.60; xent: 2.75; lr: 1.00000; 780/870 tok/s;    621 sec\n",
            "[2022-02-27 22:23:02,875 INFO] Step 1500/ 2000; acc:  51.78; ppl: 17.19; xent: 2.84; lr: 1.00000; 806/820 tok/s;    629 sec\n",
            "[2022-02-27 22:23:12,873 INFO] Step 1520/ 2000; acc:  45.85; ppl: 22.96; xent: 3.13; lr: 1.00000; 756/764 tok/s;    639 sec\n",
            "[2022-02-27 22:23:20,415 INFO] Step 1540/ 2000; acc:  52.77; ppl: 15.49; xent: 2.74; lr: 1.00000; 762/854 tok/s;    646 sec\n",
            "[2022-02-27 22:23:27,469 INFO] Step 1560/ 2000; acc:  52.37; ppl: 15.05; xent: 2.71; lr: 1.00000; 789/854 tok/s;    653 sec\n",
            "[2022-02-27 22:23:37,475 INFO] Step 1580/ 2000; acc:  44.89; ppl: 24.56; xent: 3.20; lr: 1.00000; 783/807 tok/s;    663 sec\n",
            "[2022-02-27 22:23:45,334 INFO] Step 1600/ 2000; acc:  51.63; ppl: 16.21; xent: 2.79; lr: 1.00000; 807/796 tok/s;    671 sec\n",
            "[2022-02-27 22:23:52,403 INFO] Step 1620/ 2000; acc:  53.19; ppl: 14.42; xent: 2.67; lr: 1.00000; 798/837 tok/s;    678 sec\n",
            "[2022-02-27 22:24:01,930 INFO] Step 1640/ 2000; acc:  46.02; ppl: 22.03; xent: 3.09; lr: 1.00000; 785/809 tok/s;    688 sec\n",
            "[2022-02-27 22:24:09,915 INFO] Step 1660/ 2000; acc:  50.36; ppl: 16.66; xent: 2.81; lr: 1.00000; 795/786 tok/s;    696 sec\n",
            "[2022-02-27 22:24:17,478 INFO] Step 1680/ 2000; acc:  53.90; ppl: 14.28; xent: 2.66; lr: 1.00000; 778/807 tok/s;    703 sec\n",
            "[2022-02-27 22:24:27,313 INFO] Step 1700/ 2000; acc:  46.89; ppl: 21.32; xent: 3.06; lr: 1.00000; 740/763 tok/s;    713 sec\n",
            "[2022-02-27 22:24:35,489 INFO] Step 1720/ 2000; acc:  49.46; ppl: 18.25; xent: 2.90; lr: 1.00000; 828/820 tok/s;    721 sec\n",
            "[2022-02-27 22:24:43,017 INFO] Step 1740/ 2000; acc:  51.74; ppl: 15.73; xent: 2.76; lr: 1.00000; 780/821 tok/s;    729 sec\n",
            "[2022-02-27 22:24:50,290 INFO] Step 1760/ 2000; acc:  53.06; ppl: 14.48; xent: 2.67; lr: 1.00000; 802/818 tok/s;    736 sec\n",
            "[2022-02-27 22:25:00,250 INFO] Step 1780/ 2000; acc:  46.05; ppl: 21.84; xent: 3.08; lr: 1.00000; 764/788 tok/s;    746 sec\n",
            "[2022-02-27 22:25:07,992 INFO] Step 1800/ 2000; acc:  53.22; ppl: 13.89; xent: 2.63; lr: 1.00000; 748/841 tok/s;    754 sec\n",
            "[2022-02-27 22:25:15,542 INFO] Step 1820/ 2000; acc:  53.44; ppl: 13.97; xent: 2.64; lr: 1.00000; 774/819 tok/s;    761 sec\n",
            "[2022-02-27 22:25:25,171 INFO] Step 1840/ 2000; acc:  47.46; ppl: 19.90; xent: 2.99; lr: 1.00000; 769/792 tok/s;    771 sec\n",
            "[2022-02-27 22:25:31,144 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-27 22:25:32,407 INFO] Step 1860/ 2000; acc:  54.34; ppl: 13.20; xent: 2.58; lr: 1.00000; 788/827 tok/s;    778 sec\n",
            "[2022-02-27 22:25:40,140 INFO] Validation perplexity: 9.41938\n",
            "[2022-02-27 22:25:40,141 INFO] Validation accuracy: 60.0834\n",
            "[2022-02-27 22:25:40,199 INFO] Saving checkpoint ./models/384unit/model_step_1875.pt\n",
            "[2022-02-27 22:25:42,372 INFO] Step 1880/ 2000; acc:  55.38; ppl: 12.68; xent: 2.54; lr: 1.00000; 550/602 tok/s;    788 sec\n",
            "[2022-02-27 22:25:52,393 INFO] Step 1900/ 2000; acc:  46.09; ppl: 22.07; xent: 3.09; lr: 1.00000; 771/783 tok/s;    798 sec\n",
            "[2022-02-27 22:26:01,000 INFO] Step 1920/ 2000; acc:  52.60; ppl: 13.65; xent: 2.61; lr: 1.00000; 722/776 tok/s;    807 sec\n",
            "[2022-02-27 22:26:08,170 INFO] Step 1940/ 2000; acc:  56.09; ppl: 12.30; xent: 2.51; lr: 1.00000; 786/845 tok/s;    814 sec\n",
            "[2022-02-27 22:26:18,377 INFO] Step 1960/ 2000; acc:  47.39; ppl: 19.94; xent: 2.99; lr: 1.00000; 732/750 tok/s;    824 sec\n",
            "[2022-02-27 22:26:26,038 INFO] Step 1980/ 2000; acc:  53.68; ppl: 13.53; xent: 2.60; lr: 1.00000; 829/845 tok/s;    832 sec\n",
            "[2022-02-27 22:26:33,706 INFO] Step 2000/ 2000; acc:  54.29; ppl: 13.00; xent: 2.56; lr: 1.00000; 773/839 tok/s;    839 sec\n",
            "[2022-02-27 22:26:33,765 INFO] Saving checkpoint ./models/384unit/model_step_2000.pt\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:26:36,596 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:26:45,325 INFO] PRED AVG SCORE: -1.1412, PRED PPL: 3.1306\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:26:48,326 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:26:55,803 INFO] PRED AVG SCORE: -1.1651, PRED PPL: 3.2064\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:26:57,947 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:27:04,907 INFO] PRED AVG SCORE: -0.9908, PRED PPL: 2.6933\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:27:07,009 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:27:13,956 INFO] PRED AVG SCORE: -0.9170, PRED PPL: 2.5017\n",
            "BLEU = 9.35, 53.6/19.3/10.6/3.8 (BP=0.654, ratio=0.702, hyp_len=2672, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 15.57, 48.5/20.6/11.6/5.1 (BP=0.999, ratio=0.999, hyp_len=3805, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 19.79, 54.8/26.1/16.4/8.5 (BP=0.936, ratio=0.938, hyp_len=3570, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.64, 56.6/26.2/15.9/8.0 (BP=0.894, ratio=0.899, hyp_len=3423, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-384unit.yaml\n",
        "cd models/384unit\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/384unit/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/384unit/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/384unit/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/384unit/pred2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90nqCIkkLVqb",
        "outputId": "ea79fbef-c181-400b-83c1-8bc818f0abcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-27 22:29:13,578 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-27 22:29:13,581 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-27 22:29:13,581 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-27 22:29:13,582 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-27 22:29:13,583 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-27 22:29:13,583 INFO] Loading vocab from text file...\n",
            "[2022-02-27 22:29:13,583 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-27 22:29:13,615 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-27 22:29:13,621 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-27 22:29:13,642 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-27 22:29:13,646 INFO] Building fields with vocab in counters...\n",
            "[2022-02-27 22:29:13,657 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-27 22:29:13,672 INFO]  * src vocab size: 9980.\n",
            "[2022-02-27 22:29:13,673 INFO]  * src vocab size = 9980\n",
            "[2022-02-27 22:29:13,673 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-27 22:29:13,680 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:29:13,972 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 256, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1012, 512)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-27 22:29:13,972 INFO] encoder: 6542384\n",
            "[2022-02-27 22:29:13,972 INFO] decoder: 11429822\n",
            "[2022-02-27 22:29:13,973 INFO] * number of parameters: 17972206\n",
            "[2022-02-27 22:29:13,974 INFO] Starting training on CPU, could be very slow\n",
            "[2022-02-27 22:29:13,974 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-27 22:29:13,974 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-27 22:29:13,975 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "[2022-02-27 22:29:26,133 INFO] Step 20/ 2000; acc:   8.21; ppl: 7439.43; xent: 8.91; lr: 1.00000; 537/543 tok/s;     12 sec\n",
            "[2022-02-27 22:29:39,680 INFO] Step 40/ 2000; acc:  12.58; ppl: 661.79; xent: 6.49; lr: 1.00000; 510/533 tok/s;     26 sec\n",
            "[2022-02-27 22:29:49,777 INFO] Step 60/ 2000; acc:  18.57; ppl: 283.62; xent: 5.65; lr: 1.00000; 564/610 tok/s;     36 sec\n",
            "[2022-02-27 22:30:01,749 INFO] Step 80/ 2000; acc:  18.74; ppl: 242.47; xent: 5.49; lr: 1.00000; 583/624 tok/s;     48 sec\n",
            "[2022-02-27 22:30:14,071 INFO] Step 100/ 2000; acc:  20.05; ppl: 213.89; xent: 5.37; lr: 1.00000; 545/554 tok/s;     60 sec\n",
            "[2022-02-27 22:30:24,615 INFO] Step 120/ 2000; acc:  25.89; ppl: 139.06; xent: 4.93; lr: 1.00000; 542/561 tok/s;     71 sec\n",
            "[2022-02-27 22:30:36,476 INFO] Step 140/ 2000; acc:  23.19; ppl: 147.02; xent: 4.99; lr: 1.00000; 592/589 tok/s;     83 sec\n",
            "[2022-02-27 22:30:48,704 INFO] Step 160/ 2000; acc:  23.38; ppl: 149.27; xent: 5.01; lr: 1.00000; 550/578 tok/s;     95 sec\n",
            "[2022-02-27 22:30:59,946 INFO] Step 180/ 2000; acc:  27.61; ppl: 111.74; xent: 4.72; lr: 1.00000; 563/590 tok/s;    106 sec\n",
            "[2022-02-27 22:31:11,538 INFO] Step 200/ 2000; acc:  30.23; ppl: 90.72; xent: 4.51; lr: 1.00000; 572/604 tok/s;    118 sec\n",
            "[2022-02-27 22:31:24,167 INFO] Step 220/ 2000; acc:  30.36; ppl: 103.94; xent: 4.64; lr: 1.00000; 489/537 tok/s;    130 sec\n",
            "[2022-02-27 22:31:34,876 INFO] Step 240/ 2000; acc:  33.35; ppl: 72.69; xent: 4.29; lr: 1.00000; 551/596 tok/s;    141 sec\n",
            "[2022-02-27 22:31:45,699 INFO] Step 260/ 2000; acc:  33.41; ppl: 75.44; xent: 4.32; lr: 1.00000; 607/612 tok/s;    152 sec\n",
            "[2022-02-27 22:31:56,166 INFO] Step 280/ 2000; acc:  34.46; ppl: 67.05; xent: 4.21; lr: 1.00000; 556/597 tok/s;    162 sec\n",
            "[2022-02-27 22:32:09,285 INFO] Step 300/ 2000; acc:  32.06; ppl: 79.85; xent: 4.38; lr: 1.00000; 531/559 tok/s;    175 sec\n",
            "[2022-02-27 22:32:19,754 INFO] Step 320/ 2000; acc:  35.70; ppl: 58.67; xent: 4.07; lr: 1.00000; 576/607 tok/s;    186 sec\n",
            "[2022-02-27 22:32:31,041 INFO] Step 340/ 2000; acc:  35.60; ppl: 56.95; xent: 4.04; lr: 1.00000; 584/598 tok/s;    197 sec\n",
            "[2022-02-27 22:32:44,025 INFO] Step 360/ 2000; acc:  32.74; ppl: 75.49; xent: 4.32; lr: 1.00000; 545/552 tok/s;    210 sec\n",
            "[2022-02-27 22:32:54,013 INFO] Step 380/ 2000; acc:  39.10; ppl: 49.98; xent: 3.91; lr: 1.00000; 580/596 tok/s;    220 sec\n",
            "[2022-02-27 22:33:05,836 INFO] Step 400/ 2000; acc:  35.02; ppl: 58.19; xent: 4.06; lr: 1.00000; 592/595 tok/s;    232 sec\n",
            "[2022-02-27 22:33:18,049 INFO] Step 420/ 2000; acc:  35.23; ppl: 64.42; xent: 4.17; lr: 1.00000; 551/563 tok/s;    244 sec\n",
            "[2022-02-27 22:33:28,245 INFO] Step 440/ 2000; acc:  38.82; ppl: 49.32; xent: 3.90; lr: 1.00000; 569/579 tok/s;    254 sec\n",
            "[2022-02-27 22:33:40,332 INFO] Step 460/ 2000; acc:  36.68; ppl: 49.90; xent: 3.91; lr: 1.00000; 580/569 tok/s;    266 sec\n",
            "[2022-02-27 22:33:52,748 INFO] Step 480/ 2000; acc:  35.75; ppl: 56.99; xent: 4.04; lr: 1.00000; 534/559 tok/s;    279 sec\n",
            "[2022-02-27 22:34:04,424 INFO] Step 500/ 2000; acc:  37.76; ppl: 46.99; xent: 3.85; lr: 1.00000; 537/572 tok/s;    290 sec\n",
            "[2022-02-27 22:34:16,073 INFO] Step 520/ 2000; acc:  39.15; ppl: 45.57; xent: 3.82; lr: 1.00000; 569/601 tok/s;    302 sec\n",
            "[2022-02-27 22:34:28,186 INFO] Step 540/ 2000; acc:  37.76; ppl: 51.89; xent: 3.95; lr: 1.00000; 520/565 tok/s;    314 sec\n",
            "[2022-02-27 22:34:39,026 INFO] Step 560/ 2000; acc:  40.81; ppl: 39.70; xent: 3.68; lr: 1.00000; 555/591 tok/s;    325 sec\n",
            "[2022-02-27 22:34:47,648 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-27 22:34:50,091 INFO] Step 580/ 2000; acc:  40.65; ppl: 40.56; xent: 3.70; lr: 1.00000; 599/610 tok/s;    336 sec\n",
            "[2022-02-27 22:35:00,818 INFO] Step 600/ 2000; acc:  41.04; ppl: 37.86; xent: 3.63; lr: 1.00000; 541/578 tok/s;    347 sec\n",
            "[2022-02-27 22:35:13,735 INFO] Step 620/ 2000; acc:  37.98; ppl: 45.63; xent: 3.82; lr: 1.00000; 534/561 tok/s;    360 sec\n",
            "[2022-02-27 22:35:15,736 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-27 22:35:18,966 INFO] Validation perplexity: 20.9635\n",
            "[2022-02-27 22:35:18,966 INFO] Validation accuracy: 50.1623\n",
            "[2022-02-27 22:35:19,026 INFO] Saving checkpoint ./models/512unit/model_step_625.pt\n",
            "[2022-02-27 22:35:28,431 INFO] Step 640/ 2000; acc:  41.55; ppl: 36.78; xent: 3.60; lr: 1.00000; 410/433 tok/s;    374 sec\n",
            "[2022-02-27 22:35:39,868 INFO] Step 660/ 2000; acc:  41.80; ppl: 36.64; xent: 3.60; lr: 1.00000; 569/570 tok/s;    386 sec\n",
            "[2022-02-27 22:35:53,407 INFO] Step 680/ 2000; acc:  38.94; ppl: 40.50; xent: 3.70; lr: 1.00000; 506/529 tok/s;    399 sec\n",
            "[2022-02-27 22:36:03,949 INFO] Step 700/ 2000; acc:  43.55; ppl: 31.34; xent: 3.44; lr: 1.00000; 537/578 tok/s;    410 sec\n",
            "[2022-02-27 22:36:16,285 INFO] Step 720/ 2000; acc:  39.33; ppl: 38.30; xent: 3.65; lr: 1.00000; 567/607 tok/s;    422 sec\n",
            "[2022-02-27 22:36:28,651 INFO] Step 740/ 2000; acc:  40.08; ppl: 39.88; xent: 3.69; lr: 1.00000; 551/566 tok/s;    435 sec\n",
            "[2022-02-27 22:36:39,227 INFO] Step 760/ 2000; acc:  44.72; ppl: 31.46; xent: 3.45; lr: 1.00000; 549/562 tok/s;    445 sec\n",
            "[2022-02-27 22:36:51,128 INFO] Step 780/ 2000; acc:  42.65; ppl: 31.27; xent: 3.44; lr: 1.00000; 584/594 tok/s;    457 sec\n",
            "[2022-02-27 22:37:02,807 INFO] Step 800/ 2000; acc:  41.81; ppl: 39.17; xent: 3.67; lr: 1.00000; 563/590 tok/s;    469 sec\n",
            "[2022-02-27 22:37:14,206 INFO] Step 820/ 2000; acc:  43.75; ppl: 30.69; xent: 3.42; lr: 1.00000; 541/582 tok/s;    480 sec\n",
            "[2022-02-27 22:37:25,483 INFO] Step 840/ 2000; acc:  44.80; ppl: 27.83; xent: 3.33; lr: 1.00000; 585/609 tok/s;    492 sec\n",
            "[2022-02-27 22:37:37,816 INFO] Step 860/ 2000; acc:  42.57; ppl: 32.58; xent: 3.48; lr: 1.00000; 509/543 tok/s;    504 sec\n",
            "[2022-02-27 22:37:48,402 INFO] Step 880/ 2000; acc:  46.60; ppl: 25.34; xent: 3.23; lr: 1.00000; 562/605 tok/s;    514 sec\n",
            "[2022-02-27 22:37:59,524 INFO] Step 900/ 2000; acc:  43.93; ppl: 29.34; xent: 3.38; lr: 1.00000; 594/611 tok/s;    526 sec\n",
            "[2022-02-27 22:38:09,841 INFO] Step 920/ 2000; acc:  44.87; ppl: 28.59; xent: 3.35; lr: 1.00000; 573/595 tok/s;    536 sec\n",
            "[2022-02-27 22:38:23,116 INFO] Step 940/ 2000; acc:  42.34; ppl: 34.04; xent: 3.53; lr: 1.00000; 538/556 tok/s;    549 sec\n",
            "[2022-02-27 22:38:33,945 INFO] Step 960/ 2000; acc:  47.41; ppl: 22.81; xent: 3.13; lr: 1.00000; 559/596 tok/s;    560 sec\n",
            "[2022-02-27 22:38:45,140 INFO] Step 980/ 2000; acc:  45.09; ppl: 25.68; xent: 3.25; lr: 1.00000; 584/587 tok/s;    571 sec\n",
            "[2022-02-27 22:38:57,843 INFO] Step 1000/ 2000; acc:  43.79; ppl: 30.51; xent: 3.42; lr: 1.00000; 545/545 tok/s;    584 sec\n",
            "[2022-02-27 22:39:08,093 INFO] Step 1020/ 2000; acc:  48.47; ppl: 22.37; xent: 3.11; lr: 1.00000; 560/581 tok/s;    594 sec\n",
            "[2022-02-27 22:39:20,533 INFO] Step 1040/ 2000; acc:  44.70; ppl: 25.40; xent: 3.23; lr: 1.00000; 563/593 tok/s;    607 sec\n",
            "[2022-02-27 22:39:33,291 INFO] Step 1060/ 2000; acc:  43.48; ppl: 31.10; xent: 3.44; lr: 1.00000; 540/549 tok/s;    619 sec\n",
            "[2022-02-27 22:39:43,334 INFO] Step 1080/ 2000; acc:  47.58; ppl: 23.65; xent: 3.16; lr: 1.00000; 585/590 tok/s;    629 sec\n",
            "[2022-02-27 22:39:55,099 INFO] Step 1100/ 2000; acc:  46.01; ppl: 24.56; xent: 3.20; lr: 1.00000; 600/587 tok/s;    641 sec\n",
            "[2022-02-27 22:40:07,082 INFO] Step 1120/ 2000; acc:  44.17; ppl: 28.39; xent: 3.35; lr: 1.00000; 548/575 tok/s;    653 sec\n",
            "[2022-02-27 22:40:18,553 INFO] Step 1140/ 2000; acc:  46.04; ppl: 23.97; xent: 3.18; lr: 1.00000; 544/586 tok/s;    665 sec\n",
            "[2022-02-27 22:40:30,168 INFO] Step 1160/ 2000; acc:  46.68; ppl: 23.29; xent: 3.15; lr: 1.00000; 567/601 tok/s;    676 sec\n",
            "[2022-02-27 22:40:42,236 INFO] Step 1180/ 2000; acc:  45.54; ppl: 26.59; xent: 3.28; lr: 1.00000; 519/558 tok/s;    688 sec\n",
            "[2022-02-27 22:40:52,885 INFO] Step 1200/ 2000; acc:  48.66; ppl: 20.37; xent: 3.01; lr: 1.00000; 556/598 tok/s;    699 sec\n",
            "[2022-02-27 22:41:01,992 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-27 22:41:04,587 INFO] Step 1220/ 2000; acc:  48.24; ppl: 21.88; xent: 3.09; lr: 1.00000; 562/579 tok/s;    711 sec\n",
            "[2022-02-27 22:41:15,355 INFO] Step 1240/ 2000; acc:  48.03; ppl: 21.60; xent: 3.07; lr: 1.00000; 544/575 tok/s;    721 sec\n",
            "[2022-02-27 22:41:26,189 INFO] Validation perplexity: 12.7931\n",
            "[2022-02-27 22:41:26,190 INFO] Validation accuracy: 56.421\n",
            "[2022-02-27 22:41:26,248 INFO] Saving checkpoint ./models/512unit/model_step_1250.pt\n",
            "[2022-02-27 22:41:32,133 INFO] Step 1260/ 2000; acc:  44.95; ppl: 26.13; xent: 3.26; lr: 1.00000; 412/433 tok/s;    738 sec\n",
            "[2022-02-27 22:41:42,966 INFO] Step 1280/ 2000; acc:  48.18; ppl: 20.33; xent: 3.01; lr: 1.00000; 559/591 tok/s;    749 sec\n",
            "[2022-02-27 22:41:54,595 INFO] Step 1300/ 2000; acc:  47.57; ppl: 21.62; xent: 3.07; lr: 1.00000; 564/576 tok/s;    761 sec\n",
            "[2022-02-27 22:42:08,227 INFO] Step 1320/ 2000; acc:  46.41; ppl: 21.95; xent: 3.09; lr: 1.00000; 509/522 tok/s;    774 sec\n",
            "[2022-02-27 22:42:18,497 INFO] Step 1340/ 2000; acc:  51.15; ppl: 17.82; xent: 2.88; lr: 1.00000; 560/589 tok/s;    785 sec\n",
            "[2022-02-27 22:42:30,758 INFO] Step 1360/ 2000; acc:  46.57; ppl: 21.49; xent: 3.07; lr: 1.00000; 568/607 tok/s;    797 sec\n",
            "[2022-02-27 22:42:43,420 INFO] Step 1380/ 2000; acc:  47.29; ppl: 23.68; xent: 3.16; lr: 1.00000; 541/543 tok/s;    809 sec\n",
            "[2022-02-27 22:42:53,787 INFO] Step 1400/ 2000; acc:  50.22; ppl: 19.47; xent: 2.97; lr: 1.00000; 557/581 tok/s;    820 sec\n",
            "[2022-02-27 22:43:05,722 INFO] Step 1420/ 2000; acc:  48.53; ppl: 18.95; xent: 2.94; lr: 1.00000; 571/601 tok/s;    832 sec\n",
            "[2022-02-27 22:43:17,873 INFO] Step 1440/ 2000; acc:  47.12; ppl: 22.57; xent: 3.12; lr: 1.00000; 529/568 tok/s;    844 sec\n",
            "[2022-02-27 22:43:29,007 INFO] Step 1460/ 2000; acc:  50.67; ppl: 18.18; xent: 2.90; lr: 1.00000; 543/587 tok/s;    855 sec\n",
            "[2022-02-27 22:43:40,061 INFO] Step 1480/ 2000; acc:  51.08; ppl: 17.01; xent: 2.83; lr: 1.00000; 597/604 tok/s;    866 sec\n",
            "[2022-02-27 22:43:52,426 INFO] Step 1500/ 2000; acc:  48.59; ppl: 20.22; xent: 3.01; lr: 1.00000; 512/538 tok/s;    878 sec\n",
            "[2022-02-27 22:44:03,160 INFO] Step 1520/ 2000; acc:  51.90; ppl: 15.94; xent: 2.77; lr: 1.00000; 559/602 tok/s;    889 sec\n",
            "[2022-02-27 22:44:14,725 INFO] Step 1540/ 2000; acc:  50.20; ppl: 17.22; xent: 2.85; lr: 1.00000; 573/610 tok/s;    901 sec\n",
            "[2022-02-27 22:44:25,233 INFO] Step 1560/ 2000; acc:  51.26; ppl: 16.75; xent: 2.82; lr: 1.00000; 569/587 tok/s;    911 sec\n",
            "[2022-02-27 22:44:37,975 INFO] Step 1580/ 2000; acc:  47.18; ppl: 22.76; xent: 3.13; lr: 1.00000; 572/559 tok/s;    924 sec\n",
            "[2022-02-27 22:44:48,795 INFO] Step 1600/ 2000; acc:  51.52; ppl: 15.03; xent: 2.71; lr: 1.00000; 563/607 tok/s;    935 sec\n",
            "[2022-02-27 22:44:59,951 INFO] Step 1620/ 2000; acc:  50.23; ppl: 16.86; xent: 2.82; lr: 1.00000; 588/589 tok/s;    946 sec\n",
            "[2022-02-27 22:45:12,573 INFO] Step 1640/ 2000; acc:  47.32; ppl: 21.16; xent: 3.05; lr: 1.00000; 556/559 tok/s;    959 sec\n",
            "[2022-02-27 22:45:22,873 INFO] Step 1660/ 2000; acc:  53.04; ppl: 14.78; xent: 2.69; lr: 1.00000; 557/588 tok/s;    969 sec\n",
            "[2022-02-27 22:45:34,734 INFO] Step 1680/ 2000; acc:  49.76; ppl: 17.18; xent: 2.84; lr: 1.00000; 589/602 tok/s;    981 sec\n",
            "[2022-02-27 22:45:47,595 INFO] Step 1700/ 2000; acc:  48.25; ppl: 19.80; xent: 2.99; lr: 1.00000; 523/543 tok/s;    994 sec\n",
            "[2022-02-27 22:45:57,853 INFO] Step 1720/ 2000; acc:  52.18; ppl: 14.80; xent: 2.69; lr: 1.00000; 566/570 tok/s;   1004 sec\n",
            "[2022-02-27 22:46:09,841 INFO] Step 1740/ 2000; acc:  49.66; ppl: 17.45; xent: 2.86; lr: 1.00000; 581/583 tok/s;   1016 sec\n",
            "[2022-02-27 22:46:22,280 INFO] Step 1760/ 2000; acc:  49.56; ppl: 18.41; xent: 2.91; lr: 1.00000; 530/554 tok/s;   1028 sec\n",
            "[2022-02-27 22:46:33,688 INFO] Step 1780/ 2000; acc:  50.62; ppl: 16.33; xent: 2.79; lr: 1.00000; 547/593 tok/s;   1040 sec\n",
            "[2022-02-27 22:46:45,055 INFO] Step 1800/ 2000; acc:  51.50; ppl: 15.71; xent: 2.75; lr: 1.00000; 578/610 tok/s;   1051 sec\n",
            "[2022-02-27 22:46:56,941 INFO] Step 1820/ 2000; acc:  49.70; ppl: 18.12; xent: 2.90; lr: 1.00000; 522/555 tok/s;   1063 sec\n",
            "[2022-02-27 22:47:07,358 INFO] Step 1840/ 2000; acc:  53.90; ppl: 13.09; xent: 2.57; lr: 1.00000; 567/609 tok/s;   1073 sec\n",
            "[2022-02-27 22:47:16,504 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-27 22:47:19,131 INFO] Step 1860/ 2000; acc:  52.04; ppl: 14.62; xent: 2.68; lr: 1.00000; 559/579 tok/s;   1085 sec\n",
            "[2022-02-27 22:47:30,597 INFO] Validation perplexity: 9.5846\n",
            "[2022-02-27 22:47:30,598 INFO] Validation accuracy: 60.2921\n",
            "[2022-02-27 22:47:30,656 INFO] Saving checkpoint ./models/512unit/model_step_1875.pt\n",
            "[2022-02-27 22:47:33,911 INFO] Step 1880/ 2000; acc:  53.59; ppl: 14.48; xent: 2.67; lr: 1.00000; 398/421 tok/s;   1100 sec\n",
            "[2022-02-27 22:47:47,342 INFO] Step 1900/ 2000; acc:  48.77; ppl: 18.14; xent: 2.90; lr: 1.00000; 526/545 tok/s;   1113 sec\n",
            "[2022-02-27 22:47:58,117 INFO] Step 1920/ 2000; acc:  52.59; ppl: 13.72; xent: 2.62; lr: 1.00000; 563/603 tok/s;   1124 sec\n",
            "[2022-02-27 22:48:09,405 INFO] Step 1940/ 2000; acc:  51.82; ppl: 14.34; xent: 2.66; lr: 1.00000; 583/606 tok/s;   1135 sec\n",
            "[2022-02-27 22:48:23,095 INFO] Step 1960/ 2000; acc:  50.35; ppl: 16.87; xent: 2.83; lr: 1.00000; 510/518 tok/s;   1149 sec\n",
            "[2022-02-27 22:48:33,468 INFO] Step 1980/ 2000; acc:  56.19; ppl: 12.42; xent: 2.52; lr: 1.00000; 557/588 tok/s;   1159 sec\n",
            "[2022-02-27 22:48:45,636 INFO] Step 2000/ 2000; acc:  51.31; ppl: 14.89; xent: 2.70; lr: 1.00000; 575/605 tok/s;   1172 sec\n",
            "[2022-02-27 22:48:45,695 INFO] Saving checkpoint ./models/512unit/model_step_2000.pt\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:48:48,330 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:48:55,375 INFO] PRED AVG SCORE: -1.1985, PRED PPL: 3.3152\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:48:57,452 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:49:06,826 INFO] PRED AVG SCORE: -1.0352, PRED PPL: 2.8158\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:49:08,906 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:49:17,500 INFO] PRED AVG SCORE: -0.8385, PRED PPL: 2.3129\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:49:19,627 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:49:28,984 INFO] PRED AVG SCORE: -0.8914, PRED PPL: 2.4385\n",
            "BLEU = 8.97, 56.0/20.3/10.9/4.5 (BP=0.584, ratio=0.650, hyp_len=2477, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 14.87, 49.3/20.7/11.8/5.1 (BP=0.943, ratio=0.944, hyp_len=3596, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.73, 56.5/26.9/16.7/8.6 (BP=0.867, ratio=0.876, hyp_len=3334, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 19.36, 54.9/26.3/16.0/8.4 (BP=0.922, ratio=0.925, hyp_len=3523, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-512unit.yaml\n",
        "cd models/512unit\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/512unit/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/512unit/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/512unit/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/512unit/pred2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m4oHhUzjjyJ"
      },
      "source": [
        "**Q13:** On n'observe pas d'amélioration de résultats en ajoutant un méchaniqme d'attention. <span style=\"color:red\"> où sont les logs? Vous n'observé pas d'amélioration car votre modèle est sous entrainé (vous entrainé pour 2000 steps seulements). En effet, en rajoutant le mechanisme d'attention on rajoute des paramètres au modèle, ce qui implique que notre modèle aura besoin de plus de temps d'entrainement. </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXTnhUSFLftZ"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "onmt_train -config ./config-mlp.yaml\n",
        "cd models/mlp\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/mlp/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/mlp/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/mlp/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/mlp/pred2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqf6IlxwXhkV"
      },
      "source": [
        "**Q14:** Le beam search est un algorithme de recherche heuristique qui explore un graphe en ne considérant qu'un ensemble limité de fils de chaque nœud. On observe une amélioration des résultats quand on augmente le beam_size. Par défault, le beam_size est de 5.\n",
        "\n",
        "<span style=\"color:red\"> C'est quoi les noeuds et les fils dans notre cas ? --> les tokens de chaque étape de décodage de la phrase cible, c.à.d. de la traduction </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otgW0Aq2kTE2"
      },
      "source": [
        "**Q15:** On obtiens des résultats inférieurs à la base en fixant le beam_size à 1 et de meilleurs résultats quand on le fixe à 10. <span style=\"color:red\"> Pourquoi? </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBGLHnTqLqYe",
        "outputId": "fdcdd315-856c-4ddc-96b7-573d917578c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:10:17,879 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:10:19,442 INFO] PRED AVG SCORE: -1.5322, PRED PPL: 4.6283\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:10:21,707 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:10:23,715 INFO] PRED AVG SCORE: -1.3424, PRED PPL: 3.8280\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:10:25,914 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:10:27,679 INFO] PRED AVG SCORE: -1.1224, PRED PPL: 3.0722\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:10:29,920 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:10:31,595 INFO] PRED AVG SCORE: -1.0917, PRED PPL: 2.9793\n",
            "BLEU = 8.08, 42.8/13.7/6.6/1.8 (BP=0.879, ratio=0.886, hyp_len=3373, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 10.81, 41.8/15.8/8.0/2.6 (BP=1.000, ratio=1.149, hyp_len=4376, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 15.70, 50.1/21.2/11.9/4.8 (BP=1.000, ratio=1.023, hyp_len=3895, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 17.72, 52.1/23.1/13.6/6.5 (BP=0.981, ratio=0.981, hyp_len=3736, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "cd models/base\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt -beam_size 1\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt -beam_size 1\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt -beam_size 1\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt -beam_size 1\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/base/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/base/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/base/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/base/pred2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xkU7DCmL7a5",
        "outputId": "98033535-f5d1-4a72-8a5c-a1bbf7558c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:11:11,119 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:11:18,490 INFO] PRED AVG SCORE: -1.3008, PRED PPL: 3.6724\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:11:20,518 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:11:30,007 INFO] PRED AVG SCORE: -1.1142, PRED PPL: 3.0471\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:11:32,040 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:11:40,455 INFO] PRED AVG SCORE: -0.9712, PRED PPL: 2.6411\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 22:11:42,512 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 22:11:50,726 INFO] PRED AVG SCORE: -0.9362, PRED PPL: 2.5504\n",
            "BLEU = 9.06, 52.5/19.5/10.4/3.5 (BP=0.652, ratio=0.700, hyp_len=2667, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 12.81, 46.4/18.3/9.6/3.4 (BP=0.991, ratio=0.991, hyp_len=3772, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.30, 57.0/26.5/16.2/7.9 (BP=0.872, ratio=0.879, hyp_len=3349, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.72, 59.0/27.9/17.3/9.2 (BP=0.828, ratio=0.841, hyp_len=3202, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/\n",
        "cd models/base\n",
        "onmt_translate -model model_step_625.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred625.txt -beam_size 10\n",
        "onmt_translate -model model_step_1250.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1250.txt -beam_size 10\n",
        "onmt_translate -model model_step_1875.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred1875.txt -beam_size 10\n",
        "onmt_translate -model model_step_2000.pt -src ../../BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output pred2000.txt -beam_size 10\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/base/pred625.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/base/pred1250.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < models/base/pred1875.txt\n",
        "perl multi-bleu.perl ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt< models/base/pred2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xIhc-FtkbJq"
      },
      "source": [
        "**Q16:** Le meilleur score BLEU obtenu est de 31.76, pour le modèle avec 6000 train_steps. On va maintenant tester ce modèle sur les testsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSWMvsYyk5QS",
        "outputId": "ede1615c-dd39-4b4d-aba7-f2f79fcf3289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-27 23:49:57,643 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-27 23:50:02,189 INFO] PRED AVG SCORE: -0.6034, PRED PPL: 1.8283\n",
            "BLEU = 31.61, 64.6/39.6/27.8/18.9 (BP=0.929, ratio=0.931, hyp_len=3329, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "cd drive/MyDrive/tp_nmt/models/6000step\n",
        "onmt_translate -model model_step_6000.pt -src ../../BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output pred6000.txt\n",
        "cd ../..\n",
        "perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/6000step/pred6000.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KvwTU-Tl4i6"
      },
      "source": [
        "On obtient un score BLEU de 31.61, assez proche dur score obtenu lors de la phase dev."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maETlgCQnRWz"
      },
      "source": [
        "**Q17:** La qualité de la traduction me semble être meilleure pour le meilleur modele, par exemple la première phrase \"Help\" à été interprété par le premier modèle comme \"At it\", ce qui n'à pas trop de sense, et par le deuxième modèle par \"Stop thief\", toujour pas exactement correct mais ça a plus de sens. Cependant d'autres phrases comme \"Could you drive more slowly, please?\" on été interprété par le premier modèle comme \"Can you take me, please ?\" et par le deuxième comme \"Can you speak more slowly, please ?\". Ici le sens est plus présent pour le premier modèle mais la grammaire est maladroite, alors que le deuxième modèle à fait un erreur sur un seul mot."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tp2_BAHIJS_BOURAADA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
