{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjf5pW7H0Guf"
      },
      "source": [
        "# TP Relation Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kuQK4zd1kbL"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efVai6Q31Cbh"
      },
      "source": [
        "Dans ce TP nous allons:\n",
        "\n",
        "1. Découvrir la tache d’extraction de relation par réseaux neuronaux  (« neural relation extraction », NRE)\n",
        "2.  Comprendre le fonctionnement d’un modèle d’extraction de relation avec un encodeur BERT\n",
        "3. Découvrir la tache de reconnaissance d'entités nommées (« named-entity recognition », NER)\n",
        "4. Coder une pipeline d’extraction de relation par réseaux neuronaux pour des jeux de données textuels\n",
        "\n",
        "Avec les outils suivants:\n",
        "1. La libraire [OpenNRE](https://github.com/thunlp/OpenNRE), basée sur Pytorch et HuggingFace’s Transformers, pour la tache d’extraction de relation par réseaux neuronaux\n",
        "2. [HuggingFace’s Transformers](https://huggingface.co/transformers/) : une bibliothèque basée sur Pytorch pour le traitement automatique des langues et notamment les modèles neuronaux de type Transformer (comme BERT)\n",
        "3. Google Colab, qui héberge ce *Jupyter Notebook*. Avant de commencer le TP, vous pouvez consulter des pages d'introductions [à Colab](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb#scrollTo=YHI3vyhv5p85) et [aux Notebooks](https://realpython.com/jupyter-notebook-introduction/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhcmAWU7FISp"
      },
      "source": [
        "Contrairement au dernier TP, cette fois-ci nous n’aurons pas besoin d’utiliser une GPU car nous n’allons pas entraîner des nouveaux modèles ou faire de l’inférence sur des grands jeux de données: une CPU suffira. Nous pouvons vérifier si l’on est en train d’utiliser une CPU ou une GPU avec les lignes suivantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9xBd3ZGgVOB",
        "outputId": "d281897d-b60c-4f6b-a576-65e635cd8b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will work on CPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available.\")\n",
        "  device = torch.cuda.current_device()\n",
        "else:\n",
        "  print(\"Will work on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr1QqSFEGGIH"
      },
      "source": [
        "Nous avons besoin d’installer l’outil OpenNRE. Pour éviter de devoir ré-télécharger le répertoire  GitHub de OpenNRE à chaque fois qu’on ré-initialise le fichier Colab, il est convenant de monter notre répertoire Google Drive et y télécharger le répertoire OpenNRE de façon à l’avoir toujours disponible. Ainsi, à chaque fois que nous allons ré-initialiser le fichier Colab, il nous suffira de monter notre Goodle Drive pour avoir accès à la libraire OpenNRE.\n",
        "\n",
        "> ATTENTION: modifiez *tp_path_in_drive* pour pointer vers le repertoire où vous avez placé le fichier tp_re.ipynb, vous allez télécharger OpenNRE dans le meme réépertoire. Si vous êtes sur votre machine locale, vous n'avez pas besoin de monter le Drive, mais juste de faire le clonage du réépertoire OpenNRE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmWpFhonvmNH",
        "outputId": "0dc2e1ab-6170-4fa6-fadd-69d761d2f424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "OpenNRE is already present in Google Drive under /content/gdrive/My Drive/tp_entitéNommée/OpenNRE\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "tp_path_in_drive = '/content/gdrive/My Drive/tp_entitéNommée'\n",
        "opennre_path_in_drive = tp_path_in_drive + '/OpenNRE'\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if not os.path.isdir(opennre_path_in_drive):\n",
        "  # OpenNRE is not already present in Google Drive\n",
        "  if not os.path.isdir(tp_path_in_drive):\n",
        "    # make directory for the TP if necessary\n",
        "    os.makedirs(tp_path_in_drive, exist_ok=True)\n",
        "  # change directory to the TP directory\n",
        "  os.chdir(tp_path_in_drive)\n",
        "  # clone OpenNRE repo\n",
        "  print(\"Cloning repo...\")\n",
        "  os.system('git clone https://github.com/thunlp/OpenNRE.git')\n",
        "  print(\"...done!\")\n",
        "else:\n",
        "  print(\"OpenNRE is already present in Google Drive under {0}\".format(opennre_path_in_drive))\n",
        "\n",
        "# Change current dir to OpenNRE\n",
        "os.chdir(opennre_path_in_drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q67JhFW7XPcu"
      },
      "outputs": [],
      "source": [
        "# Update requirements\n",
        "!sed -i '/transformers==3.0.2/c\\transformers==3.4.0' requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CofsxJQRON4p"
      },
      "source": [
        "Nous pouvons désormais continuer avec l’installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8wFXQwfMwxp",
        "outputId": "749c07e9-8fac-4bd7-885f-c663386bcb69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: pytest==5.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (5.3.2)\n",
            "Requirement already satisfied: scikit-learn==0.22.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.22.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: nltk>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2022.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (0.9.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (0.1.96)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (0.0.47)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.2.5)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.13.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (4.11.2)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.4->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.3.2->-r requirements.txt (line 3)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.3.2->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->-r requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing opennre.egg-info/PKG-INFO\n",
            "writing dependency_links to opennre.egg-info/dependency_links.txt\n",
            "writing top-level names to opennre.egg-info/top_level.txt\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'opennre.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/opennre\n",
            "copying build/lib/opennre/__init__.py -> build/bdist.linux-x86_64/egg/opennre\n",
            "copying build/lib/opennre/pretrain.py -> build/bdist.linux-x86_64/egg/opennre\n",
            "creating build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/__init__.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/base_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/bert_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/cnn_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/pcnn_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "creating build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/__init__.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/bag_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/data_loader.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/multi_label_sentence_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/sentence_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/utils.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "creating build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/__init__.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_attention.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_average.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_one.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/base_model.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/sigmoid_nn.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/softmax_nn.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module\n",
            "copying build/lib/opennre/module/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/cnn.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/lstm.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/rnn.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/avg_pool.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/max_pool.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "creating build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/__init__.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/basic_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/bert_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/utils.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/word_piece_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/word_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/pretrain.py to pretrain.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/base_encoder.py to base_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/bert_encoder.py to bert_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/cnn_encoder.py to cnn_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/pcnn_encoder.py to pcnn_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/bag_re.py to bag_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/data_loader.py to data_loader.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/multi_label_sentence_re.py to multi_label_sentence_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/sentence_re.py to sentence_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_attention.py to bag_attention.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_average.py to bag_average.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_one.py to bag_one.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/base_model.py to base_model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/sigmoid_nn.py to sigmoid_nn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/softmax_nn.py to softmax_nn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/cnn.py to cnn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/lstm.py to lstm.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/rnn.py to rnn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/avg_pool.py to avg_pool.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/max_pool.py to max_pool.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/basic_tokenizer.py to basic_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/bert_tokenizer.py to bert_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/word_piece_tokenizer.py to word_piece_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/word_tokenizer.py to word_tokenizer.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/opennre-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing opennre-0.1-py3.7.egg\n",
            "Removing /usr/local/lib/python3.7/dist-packages/opennre-0.1-py3.7.egg\n",
            "Copying opennre-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "opennre 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/opennre-0.1-py3.7.egg\n",
            "Processing dependencies for opennre==0.1\n",
            "Finished processing dependencies for opennre==0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9RIDU84lg4g"
      },
      "source": [
        "## Les modéles pour l'extraction de relation\n",
        "\n",
        "À part offrir un cadre pour l’implémentation et l’entraînement de modelés pour l’extraction de relation, OpenNRE offre aussi des modèles déjà entraînés sur différents jeux de données, et donc capables de détecter différents types de relations entre entités. \n",
        "\n",
        "Ici, nous allons employer un modèle entraîné sur Wiki80 (dataset introduit par [le papier OpenNRE](https://www.aclweb.org/anthology/D19-3029/)), un jeu de données contenant des phrases collectées sur Wikipedia et Wikidata, ainsi que des rélations entre leurs entités. Si vous voulez en savoir plus sur Wiki80, vous pouvez le télécharger avec le script [download_wiki80.sh](https://github.com/thunlp/OpenNRE/blob/60a8ceb42e1cfacbde3c8cfb5f758fb7fe96bdc4/benchmark/download_wiki80.sh) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxT-aqw2PXZO",
        "outputId": "a3420491-9399-4ca0-e115-4acc70e306a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-14 10:54:55,976 - root - INFO - Loading BERT pre-trained checkpoint.\n"
          ]
        }
      ],
      "source": [
        "import opennre\n",
        "model = opennre.get_model('wiki80_bert_softmax')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYbZ4f4Q9Z-b"
      },
      "source": [
        "Nous pouvons utiliser ce modèle pour calculer la relation entre un mot «tête» et un mot «queue» qui sont contenus dans un texte. Il suffit de passer au modèle le texte ainsi que la position de la tête et de la queue. Le modèle retournera la relation de la queue à l’égard de la tête, ainsi que la probabilité qu’il associe à cette rélation. Par example, nous pouvons inféérer la relation entre **Áed Uaridnach** et **Máel Dúin mac Máele Fithrich** de la façon suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfpSGIJbV2gY",
        "outputId": "0eb85f97-d467-44be-eeee-c35676e292fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('child', 0.9812852144241333)\n"
          ]
        }
      ],
      "source": [
        "item = {'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (78, 91)}, 't': {'pos': (18, 46)}}\n",
        "print(model.infer(item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kacE-LI9mi1"
      },
      "source": [
        "#### Exercice 1\n",
        "\n",
        "\n",
        "\n",
        "Écrire une fonction `to_input_format(text, head, tail)` qui nous permet de trouver la position de deux mots (une tête et une queue) dans un texte, et qui retourne un dictionnaire contenant le texte et les deux positions suivant le format requis par la fonction `model.infer()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iBlnOK1HJQN4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def to_input_format(text, head, tail):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    text: a string of text\n",
        "    head: a string of text representing a word contained in text\n",
        "    tail: a string of text representing a word contained in text but different from head\n",
        "  Returns:\n",
        "    A dictionary containing the text and the position of the head and tail within it,\n",
        "    following the input format required by an OpenNRE model (see exemple above).\n",
        "  \"\"\"\n",
        "  head_pos_begin = text.find(head)\n",
        "  head_pos_end = head_pos_begin + len(head)\n",
        "\n",
        "  tail_pos_begin = text.find(tail)\n",
        "  tail_pos_end = tail_pos_begin + len(tail)\n",
        "\n",
        "  return {'text' : text, 'h' : {'pos' : (head_pos_begin, head_pos_end)}, 't': {'pos': (tail_pos_begin, tail_pos_end)}}\n",
        "  # ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX1hgPDO4VVM",
        "outputId": "41287da7-a4b8-4cb6-f297-35c0031680ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "# Test your code with this snippet\n",
        "text = 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).'\n",
        "head = 'Áed Uaridnach'\n",
        "tail = 'Máel Dúin mac Máele Fithrich'\n",
        "test_item = to_input_format(text, head, tail)\n",
        "try:\n",
        "  assert model.infer(item) == model.infer(test_item)\n",
        "  print(\"Good job!\")\n",
        "except AssertionError:\n",
        "  print(\"Something is wrong with your function, try again!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTlD2VTBlrYn",
        "outputId": "0062af07-1af3-4510-cb8b-5aaf573eeed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SoftmaxNN(\n",
            "  (sentence_encoder): BERTEncoder(\n",
            "    (bert): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=768, out_features=80, bias=True)\n",
            "  (softmax): Softmax(dim=-1)\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUeTb4e12e-T"
      },
      "source": [
        "#### Exercice 2\n",
        "\n",
        "Décrire l’architecture du modèle qu’on vient de télécharger, la logique et le fonctionnement de chacun de ses composants:\n",
        "\n",
        "1. Sentence-encoder: pourquoi l'on utilise un encodeur type BERT ?\n",
        "2. À quoi sert la couche BertPooler ?\n",
        "3. À quoi sert la couche linéaire finale (fc) ? pourquoi elle réduit à 80 la dimension du vecteur sortant de l’encodeur?\n",
        "4. À quoi sert la fonction Softmax ?\n",
        "5. À quoi sert la fonction de dropout que l'on appliuque à la sortie du réseau, ainsi que dans chaque couche de BERT ?\n",
        "\n",
        "Aide: vous pouvez inspecter un modèle Pytorch avec `print(model)`. Pour mieux le comprendre, vous pouvez aussi voir [son code](https://github.com/thunlp/OpenNRE/blob/60a8ceb42e1cfacbde3c8cfb5f758fb7fe96bdc4/opennre/model/softmax_nn.py#L5), [la déscription du modèle BERT de Huggingface](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/bert#transformers.BertModel) ainsi que [son code](https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/bert/modeling_bert.py#L848)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARQRbv_ilwY8"
      },
      "source": [
        "### Réponses \n",
        "\n",
        "1. On utilise un encoder type BERT car on réalise une sorte de classification (on cherche à prédire la relation la plus probable entre deux entités nommées). **Pourquoi BERT est le modèle de choix pour cette tache? -> Car il est capable de représenter la signification de chaque mot en fonction des mots qui l'entourent dans la même phrase (représentation contextualisée), c'est qui est très utile pour une comprehension générale de la phrase et sa classification. D'ailleurs, les représentations contextualisées sortant de BERT peuvent être utilisées pour beaucoup d'autres taches, et non-seulement la multi-classification**\n",
        "2. La couche BertPooler permet de regrouper la sortie de l’encodeur BERT avant la couche finale. BertPooler permet de garder seulement la représentation cachée du premier token qui résume le texte.\n",
        "3. La couche linéaire finale permet de réduire la dimension du vecteur sortant de l'encodeur à 80 car on a 80 classes, c'est-à-dire 80 relations possibles.\n",
        "4. La fonction Softmax permet de normaliser et d'obtenir un vecteur de probabilité de taille 80 (pour avoir la probabilité d'avoir chacune des classes/des relations).\n",
        "5. La fonction dropout que l'on applique à la sortie du réseau, ainsi que dans chaque couche de BERT permet de sélectionner qu'une partie des neurones du réseau pour éviter l'overfitting. **Pourquoi le dropout permet d'éviter le overfitting?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxlyOgFU3GQs"
      },
      "source": [
        "## Encodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWvfXCFlvf4q"
      },
      "source": [
        "#### Exercice 3\n",
        "\n",
        "Maintenant, nous allons essayer de mieux comprendre le fonctionnement de ce modèle avec un focus sur son encodeur, qui est définit dans le fichier [bert_encoder.py](https://github.com/thunlp/OpenNRE/blob/60a8ceb42e1cfacbde3c8cfb5f758fb7fe96bdc4/opennre/encoder/bert_encoder.py#L7). Ce qui est spécifique à la tache de NRE dans ce modèle n’est pas l’architecture, mais plutôt la façon dont la séquence en input est tokenisée. Avec l’objectif de bien comprendre comment cet encodeur gère son input, répondez en détail aux questions suivantes qui se référent à la méthode `tokenize`. Si vous utilisez du code pour vous aider à répondre (conseillé), veuillez le joindre à vos réponses textuelles.\n",
        "\n",
        "1. Qu’est-ce qu’elle sont les variables `sent0`, `ent0`, `sent1`, `ent1`, `sent2` ?\n",
        "2. Qu’est-ce que c’est `re_tokens` ?\n",
        "3. Dans le _forward_, le `BERTEncoder` prend en entrée exclusivement la séquence textuelle qui a étée préalablement tokenisée. Comment est-il capable de distinguer la tête et la queue en sort de (apprendre à) prédire la relation entre les deux ?\n",
        "4. Qu’est-ce que c’est le « Padding » et pourquoi est-il utile ?\n",
        "5. Qu’est-ce que c’est l’ « Attention mask » et pourquoi est-elle utile ? [Aide ici](https://huggingface.co/docs/transformers/v4.16.2/en/glossary#attention-mask)\n",
        "6. **[BONUS]** Quelle est la différence entre la classe `BERTEncoder` et la classe `BERTEntityEncoder`, définie dans le même fichier ?\n",
        "\n",
        "\n",
        "Aide : vous pouvez accéder à la méthode `tokenize` pour la tester avec `model.sentence_encoder.tokenize(item)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te30zT02s_Tt",
        "outputId": "0a6b58fc-dcee-4295-c9a7-eb4461f3def3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.4638, -0.4593,  0.2540, -0.5908, -0.0125,  0.6642,  0.9365,  0.0483,\n",
              "         -0.7273, -0.3425,  0.8914,  0.7780,  0.4055, -0.8910,  0.9383, -0.1310,\n",
              "         -0.8897,  0.8446, -0.6228, -0.7706,  0.3430,  0.8529,  0.0679,  0.4727,\n",
              "          0.6341, -0.9478,  0.8847,  0.8844,  0.1530, -0.6425,  0.7174, -0.7653,\n",
              "         -0.9471, -0.3960, -0.6971, -0.3486,  0.8312, -0.7198,  0.2090,  0.5294,\n",
              "         -0.8037,  0.1047,  0.5383, -0.5701, -0.6731, -0.0634,  0.9902, -0.8422,\n",
              "          0.8534, -0.8804, -0.9516, -0.0551,  0.1833,  0.5383,  0.2994,  0.1594,\n",
              "          0.5757, -0.4190, -0.0261,  0.7237, -0.5949,  0.7558,  0.8197,  0.2948,\n",
              "         -0.2185,  0.1810, -0.5062, -0.7662,  0.6693,  0.3892,  0.8007, -0.7500,\n",
              "          0.9371, -0.9104, -0.9323,  0.6539,  0.2205, -0.9582,  0.5214, -0.9364,\n",
              "          0.7734, -0.8636, -0.2966,  0.6222, -0.4250,  0.9869, -0.6918, -0.7965,\n",
              "         -0.9145,  0.5952,  0.9456,  0.8977, -0.3563, -0.1127, -0.7425, -0.8747,\n",
              "         -0.5934,  0.7949, -0.8073,  0.8534, -0.7254,  0.7242,  0.4229,  0.8071,\n",
              "         -0.1516,  0.1885, -0.3617,  0.6732, -0.8584, -0.6632, -0.6159, -0.2923,\n",
              "         -0.0045,  0.4515, -0.8893, -0.7253, -0.4440, -0.2624, -0.6071,  0.7691,\n",
              "          0.5716, -0.4770,  0.9467,  0.9341, -0.6685, -0.4206, -0.1385,  0.9947,\n",
              "          0.9075, -0.7483, -0.3388,  0.7236, -0.7414,  0.2856,  0.2117,  0.9277,\n",
              "         -0.0201, -0.9604, -0.8541,  0.3388,  0.5873, -0.9015, -0.7204, -0.3116,\n",
              "          0.3421,  0.5706,  0.8940, -0.7703,  0.1548, -0.6911,  0.6462, -0.9346,\n",
              "          0.8228, -0.9201,  0.7550,  0.9591,  0.3909,  0.1022, -0.5648,  0.6414,\n",
              "          0.3159,  0.4786,  0.4061, -0.8283, -0.4812, -0.5922, -0.9381,  0.4487,\n",
              "          0.0286,  0.6430, -0.5741, -0.7570, -0.4423, -0.0484, -0.3004, -0.7344,\n",
              "          0.7972, -0.7307,  0.3273, -0.9603,  0.8865, -0.6087, -0.7674, -0.4848,\n",
              "          0.4757, -0.9043,  0.0626, -0.0223, -0.4498, -0.8650,  0.6594, -0.3019,\n",
              "         -0.1274,  0.7878,  0.4378,  0.8460, -0.8554, -0.8726,  0.6090,  0.0040,\n",
              "         -0.7357,  0.7341,  0.3195, -0.5299,  0.3986, -0.8508,  0.4253, -0.6061,\n",
              "         -0.5982,  0.8403, -0.3764,  0.4147, -0.8705, -0.8055,  0.3209, -0.9181,\n",
              "         -0.7745, -0.8621, -0.6858,  0.5878,  0.2611,  0.2800, -0.9622, -0.4817,\n",
              "         -0.7857, -0.8487, -0.5039,  0.7489, -0.6728,  0.4651,  0.7168, -0.6431,\n",
              "         -0.3777, -0.8776,  0.8678, -0.8632,  0.0896,  0.1165,  0.5114, -0.8462,\n",
              "          0.2009, -0.5567,  0.7050,  0.0895,  0.8411,  0.9358, -0.6714,  0.8547,\n",
              "          0.8081, -0.7767, -0.5734, -0.8145,  0.2438,  0.8718, -0.8862, -0.3098,\n",
              "          0.8196, -0.7687, -0.9767,  0.6289, -0.8100, -0.7505, -0.4433, -0.2910,\n",
              "          0.8296, -0.5575,  0.4284,  0.7204,  0.1916, -0.5682, -0.9481,  0.1769,\n",
              "          0.9836, -0.8265,  0.8425,  0.4370,  0.6078, -0.0816, -0.8251,  0.1278,\n",
              "         -0.0446, -0.2107,  0.9790, -0.6542,  0.0105,  0.0398,  0.8762, -0.7409,\n",
              "         -0.8249,  0.8048,  0.3848,  0.6022,  0.2693, -0.2997, -0.6195, -0.6816,\n",
              "          0.9275, -0.5368, -0.8362,  0.8921, -0.8407, -0.5098,  0.5347,  0.8644,\n",
              "         -0.8511, -0.6344, -0.8561, -0.9841,  0.6221,  0.3454, -0.5811, -0.6556,\n",
              "          0.5254, -0.0190,  0.3988, -0.9480,  0.2581,  0.7715,  0.9214,  0.2078,\n",
              "         -0.2331, -0.3210, -0.1452, -0.0238,  0.6925, -0.9350, -0.7496, -0.9370,\n",
              "          0.7825, -0.1807, -0.5820, -0.7878,  0.7914, -0.0714, -0.6172,  0.2317,\n",
              "          0.9210, -0.4505, -0.4937,  0.1442,  0.3696, -0.4633,  0.9290,  0.6489,\n",
              "         -0.7450, -0.9794, -0.8714, -0.6723, -0.6157, -0.6354, -0.0308, -0.9964,\n",
              "         -0.5555, -0.4179,  0.0688,  0.6940,  0.4450,  0.7938, -0.0899,  0.2178,\n",
              "          0.9414,  0.0753,  0.6593,  0.5340,  0.9337,  0.6109, -0.9525,  0.8682,\n",
              "         -0.8548,  0.1456,  0.8170, -0.7019,  0.9544, -0.8096, -0.3731,  0.5521,\n",
              "          0.6418, -0.3893,  0.7995,  0.3571, -0.6391, -0.5930, -0.3479,  0.6662,\n",
              "         -0.4626, -0.5636,  0.7513,  0.0208,  0.8766,  0.8152,  0.6837,  0.3712,\n",
              "         -0.8356, -0.7383,  0.9492,  0.7881, -0.0685, -0.7915, -0.4259, -0.1236,\n",
              "         -0.7778,  0.4713, -0.1444, -0.0354,  0.1148,  0.2182, -0.7665, -0.2966,\n",
              "         -0.6211,  0.5360, -0.1941, -0.4495, -0.9795,  0.6755,  0.4361, -0.8200,\n",
              "         -0.1901,  0.5833,  0.7097,  0.0916,  0.3859, -0.0543, -0.2924, -0.7376,\n",
              "         -0.8919,  0.4016, -0.9737,  0.4376,  0.7916,  0.3949, -0.8652, -0.8273,\n",
              "         -0.9811, -0.9116,  0.6259, -0.4803,  0.7562, -0.9118, -0.9235, -0.8390,\n",
              "          0.4129,  0.3496,  0.3472, -0.6622,  0.2804,  0.4943,  0.2897, -0.7049,\n",
              "          0.0371,  0.8013,  0.4081, -0.7362,  0.1324, -0.2570, -0.7868, -0.1861,\n",
              "          0.8773, -0.9309,  0.1208, -0.7221, -0.9129,  0.6566, -0.6700, -0.5078,\n",
              "         -0.1384, -0.5071, -0.6195,  0.2290, -0.5975, -0.8678,  0.7666,  0.7060,\n",
              "          0.1770, -0.9989, -0.2951, -0.9660,  0.6845, -0.7784, -0.4959,  0.2639,\n",
              "         -0.7588,  0.7256,  0.8323,  0.4158, -0.4836,  0.7369,  0.7560,  0.4718,\n",
              "         -0.8674,  0.5125,  0.8057, -0.6432, -0.3124,  0.5373, -0.7669,  0.1587,\n",
              "          0.8400, -0.3673,  0.4271,  0.5168,  0.6279,  0.2244,  0.7644,  0.1793,\n",
              "         -0.2489,  0.4419, -0.7626,  0.3553,  0.7268, -0.6771, -0.9644, -0.8444,\n",
              "          0.4282, -0.2813, -0.4776, -0.2018, -0.8301,  0.8809,  0.5097, -0.8740,\n",
              "          0.1969, -0.5907,  0.4386, -0.7803,  0.6946,  0.7529,  0.7656,  0.6432,\n",
              "          0.8911,  0.3766, -0.7929, -0.5123,  0.4478,  0.7841, -0.0658, -0.8428,\n",
              "          0.4818, -0.7249, -0.8775, -0.9206,  0.9632, -0.8828,  0.2655, -0.7454,\n",
              "         -0.0715, -0.9214,  0.5317, -0.1739,  0.7885,  0.0480, -0.7362, -0.0565,\n",
              "         -0.4213, -0.9503,  0.4568,  0.9452, -0.6884, -0.8282, -0.8595,  0.6506,\n",
              "          0.5621, -0.2304,  0.9024,  0.8157, -0.9190, -0.6564, -0.7126,  0.8596,\n",
              "          0.8148, -0.6163, -0.8572,  0.1511,  0.7576, -0.4155,  0.5305,  0.2175,\n",
              "          0.1659, -0.7149,  0.6828,  0.5036, -0.6250,  0.7875, -0.2202, -0.7199,\n",
              "          0.4389,  0.8424,  0.8319, -0.6262, -0.2490,  0.6395,  0.7833, -0.4247,\n",
              "         -0.2436,  0.0493, -0.6236, -0.9812, -0.8551,  0.4109, -0.6584,  0.0822,\n",
              "         -0.3246,  0.5567,  0.7708, -0.8250,  0.3820, -0.9023, -0.5072, -0.8928,\n",
              "         -0.7954,  0.3205, -0.4590, -0.2969, -0.8648, -0.7716,  0.8678, -0.0059,\n",
              "          0.2079,  0.3665, -0.5922,  0.5460,  0.8972, -0.6409, -0.9204, -0.7395,\n",
              "         -0.4953, -0.8444,  0.3940, -0.7567, -0.9748, -0.7223, -0.4735,  0.5598,\n",
              "          0.0149,  0.7181,  0.0666, -0.5105, -0.1180,  0.0305,  0.6546,  0.7158,\n",
              "          0.3996, -0.7028,  0.9531, -0.2091,  0.1902, -0.8251, -0.8835, -0.4321,\n",
              "         -0.3817,  0.5973,  0.8035, -0.6799, -0.5913,  0.3408,  0.6471,  0.8372,\n",
              "          0.6571, -0.2080, -0.9441, -0.8454, -0.7920, -0.5721,  0.6071, -0.5953,\n",
              "         -0.7991, -0.3393, -0.7978,  0.5046,  0.3794, -0.2314, -0.7868,  0.0014,\n",
              "         -0.8356, -0.8384, -0.7036, -0.5505, -0.7909,  0.9473,  0.8773, -0.1917,\n",
              "         -0.0049, -0.7933,  0.7018,  0.4010, -0.7691,  0.1025,  0.6972,  0.6408,\n",
              "         -0.6744, -0.8493, -0.8182, -0.6554,  0.9566,  0.8411,  0.0832, -0.4423,\n",
              "          0.7878, -0.8463,  0.6638, -0.8578, -0.0559, -0.6541, -0.8636, -0.1079,\n",
              "          0.9116,  0.7886,  0.4305,  0.8799,  0.0355, -0.5950,  0.9005, -0.8918,\n",
              "         -0.4402,  0.6304, -0.8198, -0.6095,  0.8164,  0.1840, -0.1494,  0.7269,\n",
              "          0.6714, -0.5110, -0.2801, -0.1324,  0.7328,  0.9080,  0.8220,  0.9644,\n",
              "         -0.1819, -0.4992, -0.5363,  0.7608,  0.6661, -0.8135, -0.4280,  0.8403,\n",
              "         -0.7166,  0.1943,  0.3381,  0.8551,  0.5554, -0.6930, -0.6517,  0.7809,\n",
              "          0.8756,  0.9738,  0.5612, -0.6334, -0.8155, -0.4135, -0.6208, -0.7527,\n",
              "         -0.7422,  0.6870, -0.6045, -0.8250,  0.4673,  0.6323,  0.5518, -0.5770,\n",
              "         -0.7437,  0.0449,  0.6787,  0.5120, -0.3935, -0.0498,  0.8571,  0.3436]],\n",
              "       grad_fn=<TanhBackward>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write your code here if needed, and then write your detailed answers below.\n",
        "tokenize = model.sentence_encoder.tokenize(item)\n",
        "model.sentence_encoder.forward(tokenize[0], tokenize[1], tokenize[2], tokenize[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkYWtUnrt0XO"
      },
      "source": [
        "### Réponses\n",
        "\n",
        "1. Les variables ent0 et ent1 sont les tokens qui représentent la tête et la queue. Les variables sent0, sent1, sent2 sont les tokens pour les autres parties du texte.\n",
        "2. re_tokens représente le texte complet en tokens. **Il y a aussi des tokens spéciaux qui ont étés introduit par le tokenizeur!\"**\n",
        "3. Dans la méthode tokenize, on encadre la tête et la queue de tokens particuliers \"unused\" pour pouvoir ensuite les repérer lors du forward.\n",
        "4. Le Padding permet de remplir le mask d'attention par des 0 lorsqu'on ne prend pas en compte les tokens concernés.**L'operation de padding enchaine à chaque phrase une série de jetons \"nuls\", jusqu'à avoir une longueur égale à max_length. Ainsi, toutes les phrases traitées auront la même longueur max_length. Cela sert à representer un lot de phrases avec une matrice, ce qui est nécessaire au réseau pour traiter les phrases en parallèle.**\n",
        "5. L'Attention mask permet de se focaliser sur des tokens, ceux qui sont à 1. On ne prend pas en compte tous les tokens. **Quels tokens ne sont pas pris en compte?**\n",
        "6. La différence entre la classe BERTEncoder et la classe BERTEntityEncoder est que les méthodes forward et tokenize utilisent les positions de départ de la tête et de la queue pour les repérer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99yJxX0-Z1h1"
      },
      "source": [
        "## NRE Pipeline\n",
        "\n",
        "Nous allons maintenant programmer une application qui prend en entrée une phrase et donne en sortie deux entités nommées dans la phrase ainsi que la relation entre eux.\n",
        "\n",
        "Pour ce faire, nous avons besoin :\n",
        "\n",
        "1. d’un système NER, qui reconnaît les entités nommées dans la phrase\n",
        "2. d’un système NRE, comme celui que nous avons utilisé jusqu’à là"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZYCITLLa0ii"
      },
      "source": [
        "#### Exercice 4\n",
        "\n",
        "En vous aident avec la documentation de HuggingFace, instanciez une [pipeline](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipeline#the-pipeline-abstraction) `ner_pipeline` pour la reconnaissance d'entités nommées, avec le modèle et le tokeniseur pré-entraînes [dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
        "\n",
        "Vous pouvez tester la bonne réussite de l’exercice avec le code ci-dessous.\n",
        "\n",
        "**Indice** : la solution consiste en deux lignes de code : l'une pour importer la classe pipeline, l'autre pour instancier la bonne pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "323UWMt1Z3y0"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"ner\")\n",
        "def ner_pipeline(text):\n",
        "  \n",
        "  return pipe(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhdvbovPBZmP",
        "outputId": "80e406db-a374-4acc-e702-f4d66e7208c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'word': 'M', 'score': 0.982085645198822, 'entity': 'I-PER', 'index': 6}, {'word': '##á', 'score': 0.8588606715202332, 'entity': 'I-PER', 'index': 7}, {'word': '##el', 'score': 0.876600980758667, 'entity': 'I-PER', 'index': 8}, {'word': 'D', 'score': 0.8375431299209595, 'entity': 'I-PER', 'index': 9}, {'word': '##ú', 'score': 0.4897184371948242, 'entity': 'I-PER', 'index': 10}, {'word': '##in', 'score': 0.9012535810470581, 'entity': 'I-PER', 'index': 11}, {'word': 'mac', 'score': 0.5821399688720703, 'entity': 'I-PER', 'index': 12}, {'word': 'M', 'score': 0.8884767889976501, 'entity': 'I-PER', 'index': 13}, {'word': '##á', 'score': 0.7004088759422302, 'entity': 'I-PER', 'index': 14}, {'word': '##ele', 'score': 0.8791091442108154, 'entity': 'I-PER', 'index': 15}, {'word': 'Fi', 'score': 0.9720048308372498, 'entity': 'I-PER', 'index': 16}, {'word': '##th', 'score': 0.6978078484535217, 'entity': 'I-PER', 'index': 17}, {'word': '##rich', 'score': 0.6970881223678589, 'entity': 'I-PER', 'index': 18}, {'word': 'Á', 'score': 0.9203091859817505, 'entity': 'I-PER', 'index': 26}, {'word': '##ed', 'score': 0.9416706562042236, 'entity': 'I-PER', 'index': 27}, {'word': 'U', 'score': 0.9702795147895813, 'entity': 'I-PER', 'index': 28}, {'word': '##ari', 'score': 0.8428964614868164, 'entity': 'I-PER', 'index': 29}, {'word': '##ach', 'score': 0.707791805267334, 'entity': 'I-PER', 'index': 31}]\n",
            "Something might be wrong with your pipeline.\n"
          ]
        }
      ],
      "source": [
        "# Apply your ner_pipeline to some sentences to see how it works,\n",
        "# Then you can test your code with this snippet\n",
        "text = 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).'\n",
        "print(ner_pipeline(text))\n",
        "try:\n",
        "  assert ner_pipeline(text) == [\n",
        "                                {'entity': 'I-PER', 'index': 6, 'score': 0.982085645198822, 'word': 'M'},\n",
        "                                {'entity': 'I-PER', 'index': 7, 'score': 0.8588601350784302, 'word': '##á'},\n",
        "                                {'entity': 'I-PER', 'index': 8, 'score': 0.8766007423400879, 'word': '##el'},\n",
        "                                {'entity': 'I-PER', 'index': 9, 'score': 0.8375428915023804, 'word': 'D'},\n",
        "                                {'entity': 'I-PER', 'index': 10, 'score': 0.4897182583808899, 'word': '##ú'},\n",
        "                                {'entity': 'I-PER', 'index': 11, 'score': 0.901253342628479, 'word': '##in'},\n",
        "                                {'entity': 'I-PER', 'index': 12, 'score': 0.5821399688720703, 'word': 'mac'},\n",
        "                                {'entity': 'I-PER', 'index': 13, 'score': 0.8884768486022949, 'word': 'M'},\n",
        "                                {'entity': 'I-PER', 'index': 14, 'score': 0.7004077434539795, 'word': '##á'},\n",
        "                                {'entity': 'I-PER', 'index': 15, 'score': 0.8791088461875916, 'word': '##ele'},\n",
        "                                {'entity': 'I-PER', 'index': 16, 'score': 0.9720047116279602, 'word': 'Fi'},\n",
        "                                {'entity': 'I-PER', 'index': 17, 'score': 0.697806715965271, 'word': '##th'},\n",
        "                                {'entity': 'I-PER', 'index': 18, 'score': 0.697088360786438, 'word': '##rich'},\n",
        "                                {'entity': 'I-PER', 'index': 26, 'score': 0.9203088283538818, 'word': 'Á'},\n",
        "                                {'entity': 'I-PER', 'index': 27, 'score': 0.9416706562042236, 'word': '##ed'},\n",
        "                                {'entity': 'I-PER', 'index': 28, 'score': 0.9702795147895813, 'word': 'U'},\n",
        "                                {'entity': 'I-PER', 'index': 29, 'score': 0.8428962230682373, 'word': '##ari'},\n",
        "                                {'entity': 'I-PER', 'index': 31, 'score': 0.7077912092208862, 'word': '##ach'}\n",
        "                              ]\n",
        "  print(\"Good job!\")\n",
        "except AssertionError:\n",
        "  print(\"Something might be wrong with your pipeline.\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zetie6ba8o1T"
      },
      "source": [
        "On obtient des résultats proches du tableau affiché."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOzSPqsJKkve"
      },
      "source": [
        "#### Exercice 5\n",
        "\n",
        "\n",
        "Nous pouvons finalement développer une pipeline NRE reposante sur notre modèle de NRE et notre pipeline NER. \n",
        "\n",
        "Écrivez ci-dessous une classe `NREPipeline` équipée (entre autres) d'une méthode `__call__(self, text)` qui prend un texte en entrée et effectue les opérations suivantes :\n",
        "\n",
        "- elle reconnaît les entités dans le texte\n",
        "    - retenir seulement les deux entités auxquelles la pipeline NER associe la probabilité la plus élevée, écarter les autres (si presentes)\n",
        "    - si la pipeline NER ne reconnaît qu’une seule entités dans le texte, `__call__(self, text)` retourne None (voir le test en bas) car il n’y a aucune relation à prédire\n",
        "- elle donne en sortie une liste en format `[e1, e2, rel, p]` où :\n",
        "    - `e1` est la première entité reconnue dans le texte (entre les deux plus probables, la première qui apparaître dans le texte)\n",
        "    - `e2` est la deuxième entité  reconnue dans le texte (entre les deux plus probables, la deuxième qui apparaître dans le texte)\n",
        "    - `rel` est la relation qu’il y a entre `e1` et `e2`\n",
        "    - `p` est la probabilité associée à la relation `rel` par le modèle de NRE\n",
        "\n",
        "Pour vérifier la bonne qualité de votre classe `NREPipeline`, utilisez l’extrait de code ci-dessous. Le résultat devrait ressabler à celui-ci :\n",
        "\n",
        "````\n",
        "Sentence 0: He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).\n",
        "System out:  ['Máel Dúin mac Máele Fithrich', 'Áed Uari', 'father', 0.9923498034477234]\n",
        "------------------------------\n",
        "Sentence 1: He was the son of Máel Dúin\n",
        "System out:  None\n",
        "------------------------------\n",
        "Sentence 2: Ōda is home to the Ōda Iwami Ginzan Silver Mine , a World Heritage Site .\n",
        "System out:  ['I', 'World Heritage Site', 'heritage designation', 0.9991846680641174]\n",
        "------------------------------\n",
        "Sentence 3: It has been shown to be equally effective as leuprorelin , which is a second - line medication against endometriosis .\n",
        "System out:  None\n",
        "------------------------------\n",
        "Sentence 4: Located at Earleville and listed on the National Register of Historic Places are : Bohemia Farm , Mount Harmon , Rose Hill , and St. Stephen 's Episcopal Church .\n",
        "System out:  ['Earleville', \"St . Stephen ' s Episcopal Church\", 'location', 0.9127373099327087]\n",
        "------------------------------\n",
        "````"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ToAxtF4nZ_Bq"
      },
      "outputs": [],
      "source": [
        "# Write your pipeline in this cell. The fun begins here, because we are coding a whole Python class from scratch !\n",
        "class NREPipeline(object):\n",
        "  def __init__(self, ner_pipeline, nre_model):\n",
        "    # ...\n",
        "    self.ner_pipeline = ner_pipeline\n",
        "    self.nre_model = nre_model\n",
        "  # ...\n",
        "\n",
        "  def __call__(self, text):\n",
        "   # ...\n",
        "   ner = ner_pipeline(text)\n",
        "   grouped_entities = pipe.group_entities(ner)\n",
        "   if len(grouped_entities) < 2:\n",
        "     return None\n",
        "   max1 = 0\n",
        "   max2 = 0\n",
        "   entity1 =\"\"\n",
        "   for entity in grouped_entities:\n",
        "     if entity['score'] > max1 and entity['score'] > max2:\n",
        "       max2 = max1\n",
        "       max1 = entity['score']\n",
        "       entity2 = entity1\n",
        "       entity1 = entity['word']\n",
        "     elif entity['score'] > max2:\n",
        "       max2 = entity['score']\n",
        "       entity2 = entity['word']\n",
        "\n",
        "   item = to_input_format(text, entity1, entity2)\n",
        "\n",
        "   infer = model.infer(item)\n",
        "\n",
        "    \n",
        "   return [entity1, entity2, infer[0], infer[1]]\n",
        "\n",
        "\n",
        "   #nre_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaWrZEJc4IDK",
        "outputId": "beb1c070-c47f-4646-d4ac-3b6c58f4aac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 0: He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).\n",
            "System out:  ['Áed Uari', 'Máel Dúin mac Máele Fithrich', 'father', 0.9711235761642456]\n",
            "------------------------------\n",
            "Sentence 1: He was the son of Máel Dúin\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 2: Ōda is home to the Ōda Iwami Ginzan Silver Mine , a World Heritage Site .\n",
            "System out:  ['World Heritage Site', '[UNK]', 'heritage designation', 0.9965190887451172]\n",
            "------------------------------\n",
            "Sentence 3: It has been shown to be equally effective as leuprorelin , which is a second - line medication against endometriosis .\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 4: Located at Earleville and listed on the National Register of Historic Places are : Bohemia Farm , Mount Harmon , Rose Hill , and St. Stephen 's Episcopal Church .\n",
            "System out:  ['Earleville', \"St . Stephen ' s Episcopal Church\", 'located in the administrative territorial entity', 0.5751824975013733]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test your code with this snippet\n",
        "\n",
        "sequences = [\n",
        "             'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', # Easy sentence\n",
        "             'He was the son of Máel Dúin', # There is only one entity in this sentence, therefore our pipeline should return None\n",
        "             'Ōda is home to the Ōda Iwami Ginzan Silver Mine , a World Heritage Site .', # Ōda is tokenized by the NER tokenizer as a \"[UNK]\" and it is detected as an entitity. For simplicity, you can discard [UNK] entities as if they were not detected. Here another difficulty arises from the fact that the different tokens in which Iwami Ginzan is decomposed are (wrongly) classified as entities belonging to different classes. In this case, we can consider only the first token \"I\" as standalone entity, for simplicity.  \n",
        "             'It has been shown to be equally effective as leuprorelin , which is a second - line medication against endometriosis .', # the NER system can not recognise any entity here, therefore the pipeline should return None\n",
        "             \"Located at Earleville and listed on the National Register of Historic Places are : Bohemia Farm , Mount Harmon , Rose Hill , and St. Stephen 's Episcopal Church .\" # This sentence is not trivial because the 's has to be managed properly\n",
        "\n",
        "]\n",
        "\n",
        "nre = NREPipeline(ner_pipeline, model)\n",
        "for n, sequence in enumerate(sequences):\n",
        "  out = nre(sequence)\n",
        "  print(\"Sentence {0}: {1}\".format(n, sequence))\n",
        "  print(\"System out: \", out)\n",
        "  print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_CvHuuUGrGI"
      },
      "source": [
        "Les exemple marchent bien.\n",
        "\n",
        "La relation prédite pour la dernière phrase est 'located in the administrative territorial entity' au lieu de 'location'. Cependant cela reste dans le thème de la localisation/position.\n",
        "\n",
        "On voit aussi que la reconstitution des entités nommées n'est pas parfaite car les tokens très peu présents sont retirés, il y a donc un trou entre les index des tokens et on ne peut donc pas rassembler toute l'entité nommée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl-hqEfaLvI-"
      },
      "source": [
        "## Application de le pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKfvPq_KLmD-"
      },
      "source": [
        "#### Exercice 6\n",
        "\n",
        "1. Appliquez la pipeline NRE aux phrases contenues dans le fichier _sentences.txt_, qui ont été extraites à partir de Wikipedia et Wikidata. Donnez un avis qualitatif sur sa performance, les problèmes rencontrés ainsi que des idées pour améliorations des résultats (e.g. entraînement sur des données différentes, modèle différent, etc.).\n",
        "\n",
        "2. Appliquez la pipeline NRE aux premières 50 phrases contenues dans le fichier _Sentences_AllAgree.txt_ que vous avez utilisé pour le TP BERT, qui parlent d’événements financiers. Remarquez-vous des différences en terme de performances par rapport à la question 2 ? Pourquoi ? Commentez…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSVoDcnFIXmy",
        "outputId": "ef7bbb5e-ce37-4749-a253-77e2ce55a516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenNRE  Sentences_AllAgree.txt  sentences.txt\n"
          ]
        }
      ],
      "source": [
        "!ls .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T2moXc3IA3f",
        "outputId": "3e2650a6-5916-4fdb-e0c1-f040abba7c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 0: The Willard and Josephine Hubbard House was individually listed on the National Register of Historic Places in 2016 .\n",
            "\n",
            "System out:  ['Josephine Hubbard', 'Willard', 'spouse', 0.8584063649177551]\n",
            "------------------------------\n",
            "Sentence 1: His station commander , Group Captain Claude Hilton Keith , found a letter among the missing airman 's personal possessions .\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 2: One was Quintus Caecilius Metellus Creticus , who was praetor in 74 BC and consul in 69 BC .\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 3: April 2009 In addition to musical acts , the label recorded beat poets Lawrence Ferlinghetti and Allen Ginsberg and comic Lenny Bruce .\n",
            "\n",
            "System out:  ['Lenny Bruce', 'Allen Ginsberg', 'sibling', 0.6458709239959717]\n",
            "------------------------------\n",
            "Sentence 4: Walter Neusel ( November 25 , 1907 – October 3 , 1964 ) was a German heavyweight boxer .\n",
            "\n",
            "System out:  ['Walter Neusel', 'German', 'country of citizenship', 0.9982675313949585]\n",
            "------------------------------\n",
            "Sentence 5: Hércules debut defending the national team was a 1938 FIFA World Cup game , played on June 5 , 1938 , against Poland .\n",
            "\n",
            "System out:  ['Poland', 'FIFA World Cup', 'participating team', 0.9908945560455322]\n",
            "------------------------------\n",
            "Sentence 6: Along the I-5 stretch , Walter Wirth Lake and McNary Field ( Salem Municipal Airport ) are on the left ; near Route 22 , the unincorporated neighborhood of Four Corners is on the right .\n",
            "\n",
            "System out:  ['Salem Municipal Airport', 'Four Corners', 'place served by transport hub', 0.9981524348258972]\n",
            "------------------------------\n",
            "Sentence 7: Michael Bradley and Landon Donovan scored the United States ' two goals in the match .\n",
            "\n",
            "System out:  ['Michael Bradley', 'Landon Donovan', 'spouse', 0.9853053689002991]\n",
            "------------------------------\n",
            "Sentence 8: The film was remade in Hindi as \" Love Ke Liye Kuch Bhi Karega \" and is loosely based on the American film \" Ruthless People \" .\n",
            "\n",
            "System out:  ['American', 'Hindi', 'original language of film or TV show', 0.8740090727806091]\n",
            "------------------------------\n",
            "Sentence 9: Incorporated in 1988 , Vertex Venture Holdings is based in Singapore , with offices Southeast Asia , Silicon Valley , China , India , Israel , and Taiwan .\n",
            "\n",
            "System out:  ['Israel', 'Taiwan', 'has part', 0.9584376811981201]\n",
            "------------------------------\n",
            "Sentence 10: Anna Caterina Antonacci , ( born 5 April 1961 ) is an Italian soprano known for roles in the bel canto and Baroque repertories .\n",
            "\n",
            "System out:  ['Italian', 'Anna Caterina Antonacci', 'voice type', 0.9965165853500366]\n",
            "------------------------------\n",
            "Sentence 11: On 2 April 2017 the former municipalities of Cresciano , Iragna and Osogna merged into the new municipality of Riviera .\n",
            "\n",
            "System out:  ['Osogna', 'Cresciano', 'located in the administrative territorial entity', 0.6573231220245361]\n",
            "------------------------------\n",
            "Sentence 12: Dimitar Ivanov Makriev (; born 7 January 1984 ) is a Bulgarian footballer who currently plays as a forward for Cypriot First Division side Nea Salamina Famagusta .\n",
            "\n",
            "System out:  ['Bulgarian', 'Dimitar Ivanov Makriev', 'country of citizenship', 0.9966973066329956]\n",
            "------------------------------\n",
            "Sentence 13: The 23d Fighter Group was assigned to the 347th Wing of Air Combat Command at Moody Air Force Base , Georgia but the group remained at Pope as a Geographically Separated Unit ( GSU ) .\n",
            "\n",
            "System out:  ['Georgia', 'Air Combat Command', 'military branch', 0.9295222163200378]\n",
            "------------------------------\n",
            "Sentence 14: \" А \" група , 2012/13 , 16 кръг , 01.03.2013 15:30 Stachowiak participated in all games of the club in Europa League during seasons 2013 - 14 and 2014 - 15 .\n",
            "\n",
            "System out:  ['Europa League', 'Stachowiak', 'league', 0.8327862620353699]\n",
            "------------------------------\n",
            "Sentence 15: Dartnell offered his services to the British Army on the outbreak of the First World War , and was commissioned into the 25th ( Frontiersmen ) Battalion , Royal Fusiliers in February 1915 .\n",
            "\n",
            "System out:  ['First World War', 'Royal Fusiliers', 'participant', 0.9957962036132812]\n",
            "------------------------------\n",
            "Sentence 16: In September 1260 , the two sides met in the plains south of Nazareth in a major confrontation known as the Battle of Ain Jalut . Cummins 2011 , p. 80 .\n",
            "\n",
            "System out:  ['Nazareth', 'Ain Jalut', 'location', 0.9964893460273743]\n",
            "------------------------------\n",
            "Sentence 17: Sir Hugh de Morville(d.1202 ) fled to the house after taking part as 1 of 4 knights in the murder on 29 December 1170 of Thomas Becket , Archbishop of Canterbury .\n",
            "\n",
            "System out:  ['Thomas Becket', 'Hugh de Morville', 'participant', 0.33052584528923035]\n",
            "------------------------------\n",
            "Sentence 18: The claim was made in 1264 as senior descendant and rightful heir of Alice of Champagne , second daughter of Queen Isabella I , Hugh being the son of their eldest daughter .\n",
            "\n",
            "System out:  ['Hugh', 'Alice', 'child', 0.9464401006698608]\n",
            "------------------------------\n",
            "Sentence 19: The Elected Member is a Booker Prize - winning novel by Welsh writer Bernice Rubens .\n",
            "\n",
            "System out:  ['Bernice Rubens', 'Booker Prize', 'winner', 0.9968593120574951]\n",
            "------------------------------\n",
            "Sentence 20: He currently serves as an Offensive Skills coach at the Brisbane Lions , and also as an assistant coach of the Lions ' NEAFL reserves team .\n",
            "\n",
            "System out:  ['Brisbane Lions', 'Lions', 'subsidiary', 0.696424126625061]\n",
            "------------------------------\n",
            "Sentence 21: In 2008 he produced , alongside Christian Colson , the critically acclaimed feature film \" Eden Lake \" ( 2008 ) , written and directed by James Watkins .\n",
            "\n",
            "System out:  ['James Watkins', 'Christian Colson', 'screenwriter', 0.7484315037727356]\n",
            "------------------------------\n",
            "Sentence 22: The West Middlesex Waterworks Company was founded in 1806 to supply water to the Marylebone and Paddington areas of London .\n",
            "\n",
            "System out:  ['West Middlesex Waterworks Company', 'London', 'headquarters location', 0.9655431509017944]\n",
            "------------------------------\n",
            "Sentence 23: Neutraface is a geometric sans - serif typeface designed by Christian Schwartz for House Industries , an American digital type foundry .\n",
            "\n",
            "System out:  ['Christian Schwartz', 'House Industries', 'owned by', 0.8855238556861877]\n",
            "------------------------------\n",
            "Sentence 24: The APG II states that Alangiaceae is a synonym of Cornaceae ( the Dogwood family ) , but still recognizes it as a \" nom .\n",
            "\n",
            "System out:  ['APG II', 'Cornaceae', 'taxon rank', 0.880980372428894]\n",
            "------------------------------\n",
            "Sentence 25: But because CKGM Montreal is the dominant class A station on 690 , CBU must use a directional signal to avoid causing interference .\n",
            "\n",
            "System out:  ['CBU', 'CKGM Montreal', 'licensed to broadcast to', 0.9963551759719849]\n",
            "------------------------------\n",
            "Sentence 26: It takes place when a quark of one hadron and an antiquark of another hadron annihilate , creating a virtual photon or Z boson which then decays into a pair of oppositely - charged leptons .\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 27: The first \" Uncle Slam \" album was \" Say Uncle \" , released in 1988 through Caroline Records .\n",
            "\n",
            "System out:  ['Caroline Records', 'Say Uncle', 'record label', 0.9988273978233337]\n",
            "------------------------------\n",
            "Sentence 28: The KC-135R - CRAG Stratotanker aerial refueling tanker aircraft of the Turkish Air Force are operated by the 101st Squadron , stationed at the Incirlik Air Base .\n",
            "\n",
            "System out:  ['Incirlik Air Base', 'Turkish Air Force', 'operator', 0.9957613348960876]\n",
            "------------------------------\n",
            "Sentence 29: The Church of The Epiphany was designed by noted Philadelphia architect Thomas Ustick Walter in the Greek Revival style .\n",
            "\n",
            "System out:  ['Thomas Ustick Walter', 'Greek Revival', 'movement', 0.997984766960144]\n",
            "------------------------------\n",
            "Sentence 30: Meaker Food Company Warehouse , Harriet May Mills House , St. Paul 's Armenian Apostolic Church , Alton Simmons House , and West Brothers Knitting Company are listed on the National Register of Historic Places .\n",
            "\n",
            "System out:  ['Meaker Food Company', 'West Brothers Knitting Company', 'occupant', 0.6864274740219116]\n",
            "------------------------------\n",
            "Sentence 31: This appointment was harshly criticized by Fiji Labour Party leader Mahendra Chaudhry .\n",
            "\n",
            "System out:  ['Fiji Labour Party', 'Mahendra Chaudhry', 'member of political party', 0.9914173483848572]\n",
            "------------------------------\n",
            "Sentence 32: In the summer of 2007 , Varea joined Deportivo Español in the third division , where he played 11 matches and scored two goals until December 2007 .\n",
            "\n",
            "System out:  ['Varea', 'Deportivo Español', 'position played on team / speciality', 0.8130488395690918]\n",
            "------------------------------\n",
            "Sentence 33: \" Aparajito \" was preceded by \" Pather Panchali \" ( 1955 ) and followed by \" Apur Sansar \" ( \" The World of Apu \" ) in 1959 .\n",
            "\n",
            "System out:  ['The World of Apu', 'Apur Sansar', 'said to be the same as', 0.5881137251853943]\n",
            "------------------------------\n",
            "Sentence 34: Neće rijeka zrakom teći is the second studio album by Croatian rock band Silente .\n",
            "\n",
            "System out:  ['Croatian', 'Silente', 'country of origin', 0.9018872976303101]\n",
            "------------------------------\n",
            "Sentence 35: This project was heavily opposed by the Apulian governor Michele Emiliano . Gasdotto Tap , il governo tira dritto dopo il no del Mibac .\n",
            "\n",
            "System out:  ['Michele Emiliano', 'Apulian', 'head of government', 0.7664771676063538]\n",
            "------------------------------\n",
            "Sentence 36: Among Microsoft software development tools , DTE80 is the Microsoft Development Environment 8.0 Extensibility namespace .\n",
            "\n",
            "System out:  ['Microsoft', 'Microsoft Development Environment 8 . 0 Ex', 'developer', 0.8645375370979309]\n",
            "------------------------------\n",
            "Sentence 37: Sprinkling Tarn is a body of water at the foot of Great End , in the Southern Fells in Lake District , 3   km from Seathwaite , Cumbria , England .\n",
            "\n",
            "System out:  ['Cumbria', 'Lake District', 'contains administrative territorial entity', 0.9956204295158386]\n",
            "------------------------------\n",
            "Sentence 38: Michel made her WTA tour debut at the 2013 Internationaux de Strasbourg , partnering Claire Feuerstein in doubles .\n",
            "\n",
            "System out:  ['Michel', 'Claire Feuerstein', 'spouse', 0.8597860336303711]\n",
            "------------------------------\n",
            "Sentence 39: He took part in the Council of Agde in 506 , where 34 Catholic bishops of the Visigothic kingdom met under the chairmanship of Saint Caesarius of Arles .\n",
            "\n",
            "System out:  ['Caesarius', 'Catholic', 'religion', 0.9990239143371582]\n",
            "------------------------------\n",
            "Sentence 40: A license obtained from the British Motor Corporation led to the Siam di Tella 1500 ; based on the British BMC Farina series of the late 1950s .\n",
            "\n",
            "System out:  ['British Motor Corporation', 'British', 'followed by', 0.6191114187240601]\n",
            "------------------------------\n",
            "Sentence 41: Euphemia was the youngest child of Władysław Odonic and his wife Jadwiga , disputed daughter of Mestwin I , Duke of Pomerania and Swinisław , daughter of Mieszko III the Old .\n",
            "\n",
            "System out:  ['Władysław Odonic', 'Swinisław', 'child', 0.3340180516242981]\n",
            "------------------------------\n",
            "Sentence 42: Hot 2Nite was produced by Ryan Leslie initially for New Edition 's album , One Love , released in 2004 under the label Bad Boy Records .\n",
            "\n",
            "System out:  ['Ryan Leslie', 'Bad Boy Records', 'record label', 0.9988422989845276]\n",
            "------------------------------\n",
            "Sentence 43: SN 1996ah was a supernova located in the spiral galaxy NGC 5640 in the constellation of Camelopardalis .\n",
            "\n",
            "System out:  ['S', 'Camelopardalis', 'constellation', 0.9993067979812622]\n",
            "------------------------------\n",
            "Sentence 44: She made her debut at The Proms in 2000 , singing Mozart 's Mass in C minor and Alban Berg 's \" Altenberg Lieder \" with the Bochumer Symphoniker , conducted by Simon Rattle .\n",
            "\n",
            "System out:  ['Alban Berg', 'Simon Rattle', 'composer', 0.992742657661438]\n",
            "------------------------------\n",
            "Sentence 45: The Albanian Wikipedia ( ) is the Albanian language edition of Wikipedia started on October 12 , 2003 .\n",
            "\n",
            "System out:  ['Albanian', 'Albanian Wikipedia', 'language of work or name', 0.9711563587188721]\n",
            "------------------------------\n",
            "Sentence 46: For her performance in this episode , Elaine Stritch received a Primetime Emmy Award nomination in the category for Outstanding Guest Actress in a Comedy Series .\n",
            "\n",
            "System out:  ['Primetime Emmy Award', 'Elaine Stritch', 'winner', 0.9866238832473755]\n",
            "------------------------------\n",
            "Sentence 47: He later worked as a journalist for the French language paper \" Le Matin \" .\n",
            "\n",
            "System out:  ['Le Matin', 'French', 'language of work or name', 0.9968658089637756]\n",
            "------------------------------\n",
            "Sentence 48: Loughton to Epping became part of the London Underground Central line on 25 September 1949 , leaving the single track line from Epping to Ongar as the last steam - worked section .\n",
            "\n",
            "System out:  ['Ongar', 'Epping', 'has part', 0.4434170424938202]\n",
            "------------------------------\n",
            "Sentence 49: Though Vivekananda gave up the idea of becoming Pavhari Baba 's disciple after seeing Ramakrishna 's melancholy face in his dream the night before his religious initiation , Baba deeply influenced him .\n",
            "\n",
            "System out:  ['Baba', 'Ramakrishna', 'notable work', 0.47660383582115173]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Write your code here and then write your detailed answers below. Do not hesitate to use more code cells if needed\n",
        "sentences = open('../sentences.txt', 'r')\n",
        "for n, sequence in enumerate(sentences):\n",
        "  out = nre(sequence)\n",
        "  print(\"Sentence {0}: {1}\".format(n, sequence))\n",
        "  print(\"System out: \", out)\n",
        "  print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rBerp4LJtXz"
      },
      "source": [
        "1. Les prédictions sont souvent correctes et il y a souvent des score élevés autour de 90%. Certaines relations sont mal prédites comme par exemple dans la phrase 3 où les 2 entités nommées sont juste 2 poètes mais la relation prédite est \"frère et soeur\".\n",
        "\n",
        "\n",
        "Des idées d'amélioration seraient d'entraîner le modèle sur d'autres données où on a déjà rencontré ces entités nommées ou ces relations, mais cela semble compliqué.\n",
        "\n",
        "Seulement 80 relations sont possibles avec ce modèle, on pourrait utiliser un modèles avec plus de relations. \n",
        "\n",
        "Nous avons vu que les entités nommées sont parfois incomplètes. On pourrait travailler sur une meilleure reconstitution des entités nommées.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg-RwdHKNHJJ",
        "outputId": "128be0a9-79fa-4984-c1ed-26b03fbc5d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 0: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\n",
            "\n",
            "System out:  ['Russia', 'Gran', 'owned by', 0.9595130681991577]\n",
            "------------------------------\n",
            "Sentence 1: For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .@positive\n",
            "\n",
            "System out:  ['Componenta', 'EU', 'applies to jurisdiction', 0.4356622099876404]\n",
            "------------------------------\n",
            "Sentence 2: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'member of', 0.6252670884132385]\n",
            "------------------------------\n",
            "Sentence 3: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'applies to jurisdiction', 0.3409985601902008]\n",
            "------------------------------\n",
            "Sentence 4: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'member of', 0.3967771530151367]\n",
            "------------------------------\n",
            "Sentence 5: Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'has part', 0.4633950889110565]\n",
            "------------------------------\n",
            "Sentence 6: Clothing retail chain Sepp+ñl+ñ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'applies to jurisdiction', 0.9572644829750061]\n",
            "------------------------------\n",
            "Sentence 7: Consolidated net sales increased 16 % to reach EUR74 .8 m , while operating profit amounted to EUR0 .9 m compared to a loss of EUR0 .7 m in the prior year period .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'has part', 0.4724876582622528]\n",
            "------------------------------\n",
            "Sentence 8: Foundries division reports its sales increased by 9.7 % to EUR 63.1 mn from EUR 57.5 mn in the corresponding period in 2006 , and sales of the Machine Shop division increased by 16.4 % to EUR 41.2 mn from EUR 35.4 mn in the corresponding period in 2006 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'applies to jurisdiction', 0.6209354400634766]\n",
            "------------------------------\n",
            "Sentence 9: HELSINKI ( AFX ) - Shares closed higher , led by Nokia after it announced plans to team up with Sanyo to manufacture 3G handsets , and by Nokian Tyres after its fourth-quarter earnings report beat analysts ' expectations , dealers said .@positive\n",
            "\n",
            "System out:  ['Nokia', 'Sanyo', 'subsidiary', 0.7974568605422974]\n",
            "------------------------------\n",
            "Sentence 10: Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'has part', 0.9053260087966919]\n",
            "------------------------------\n",
            "Sentence 11: MegaFon 's subscriber base increased 16.1 % in 2009 to 50.5 million users as of December 31 , while its market share by the number of customers amounted to 24 % as of late 2009 , up from 23 % as of late 2008 , according to TeliaSonera estimates .@positive\n",
            "\n",
            "System out:  ['MegaFon', 'TeliaSonera', 'owned by', 0.9980899691581726]\n",
            "------------------------------\n",
            "Sentence 12: Net income from life insurance doubled to EUR 6.8 mn from EUR 3.2 mn , and net income from non-life insurance rose to EUR 5.2 mn from EUR 1.5 mn in the corresponding period in 2009 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'applies to jurisdiction', 0.657645583152771]\n",
            "------------------------------\n",
            "Sentence 13: Net sales increased to EUR193 .3 m from EUR179 .9 m and pretax profit rose by 34.2 % to EUR43 .1 m. ( EUR1 = USD1 .4 )@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'has part', 0.6302099227905273]\n",
            "------------------------------\n",
            "Sentence 14: Net sales surged by 18.5 % to EUR167 .8 m. Teleste said that EUR20 .4 m , or 12.2 % , of the sales came from the acquisitions made in 2009 .@positive\n",
            "\n",
            "System out:  ['Teleste', 'EU', 'owned by', 0.9722409844398499]\n",
            "------------------------------\n",
            "Sentence 15: Nordea Group 's operating profit increased in 2010 by 18 percent year-on-year to 3.64 billion euros and total revenue by 3 percent to 9.33 billion euros .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 16: Operating profit for the nine-month period increased from EUR13 .6 m , while net sales increased from EUR394 .7 m , as compared to the corresponding period in 2005 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'has part', 0.6604453921318054]\n",
            "------------------------------\n",
            "Sentence 17: Operating profit for the three-month period increased from EUR1 .2 m , while revenue increased from EUR20 .2 m , as compared to the corresponding period in 2005 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'member of', 0.4905969798564911]\n",
            "------------------------------\n",
            "Sentence 18: The company 's net profit rose 11.4 % on the year to 82.2 million euros in 2005 on sales of 686.5 million euros , 13.8 % up on the year , the company said earlier .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 19: The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the results from its members .@positive\n",
            "\n",
            "System out:  ['Lithuanian', \"Lithuanian Brewers ' Association\", 'headquarters location', 0.37318751215934753]\n",
            "------------------------------\n",
            "Sentence 20: Viking Line 's cargo revenue increased by 5.4 % to EUR 21.46 mn , and cargo volume increased by 2.4 % to 70,116 cargo units .@positive\n",
            "\n",
            "System out:  ['Viking Line', 'EU', 'subsidiary', 0.28862264752388]\n",
            "------------------------------\n",
            "Sentence 21: The fair value of the property portfolio doubled as a result of the Kapiteeli acquisition and totalled EUR 2,686.2 1,259.7 million .@positive\n",
            "\n",
            "System out:  ['EU', 'Kapiteeli', 'subsidiary', 0.8734971284866333]\n",
            "------------------------------\n",
            "Sentence 22: 10 February 2011 - Finnish media company Sanoma Oyj HEL : SAA1V said yesterday its 2010 net profit almost tripled to EUR297 .3 m from EUR107 .1 m for 2009 and announced a proposal for a raised payout .@positive\n",
            "\n",
            "System out:  ['Finnish', 'EU', 'country', 0.9579282999038696]\n",
            "------------------------------\n",
            "Sentence 23: A Helsinki : ELIiV today reported EPS of EUR1 .13 for 2009 , an increase over EPS of EUR1 .12 in 2008 .@positive\n",
            "\n",
            "System out:  ['ELIiV', 'EU', 'member of', 0.33720988035202026]\n",
            "------------------------------\n",
            "Sentence 24: Commission income increased by 22 % to EUR 4.4 mn , and lending volume rose by 13.5 % .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 25: In January , traffic , measured in revenue passenger kilometres RPK , went up by 3.2 % and capacity , measured in available seat kilometres ASK , rose by 12.2 % .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 26: In January-September 2010 , Fiskars ' net profit went up by 14 % year-on-year to EUR 65.4 million and net sales to EUR 525.3 million from EUR 487.7 million .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'member of', 0.6408349871635437]\n",
            "------------------------------\n",
            "Sentence 27: Net income from life insurance rose to EUR 16.5 mn from EUR 14.0 mn , and net income from non-life insurance to EUR 22.6 mn from EUR 15.2 mn in 2009 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'applies to jurisdiction', 0.7869071960449219]\n",
            "------------------------------\n",
            "Sentence 28: Sales have risen in other export markets .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 29: Sales increased due to growing market rates and increased operations .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 30: The agreement strengthens our long-term partnership with Nokia Siemens Networks .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 31: The company 's order book stood at 1.5 bln euro $ 2.2 bln on September 30 , 2007 , up by 24.2 pct on the year , with international orders amounting to 365 mln euro $ 534.3 mln .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 32: The company said that paper demand increased in all of its main markets , including of publication papers , and that it increased average paper prices by 4 percent compared with last year .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 33: The world 's second largest stainless steel maker said net profit in the three-month period until Dec. 31 surged to euro603 million US$ 781 million , or euro3 .33 US$ 4.31 per share , from euro172 million , or euro0 .94 per share , the previous year .@positive\n",
            "\n",
            "System out:  ['US', 'US', 'instance of', 0.47332534193992615]\n",
            "------------------------------\n",
            "Sentence 34: Shares of Standard Chartered ( STAN ) rose 1.2 % in the FTSE 100 , while Royal Bank of Scotland ( RBS ) shares rose 2 % and Barclays shares ( BARC ) ( BCS ) were up 1.7 % .@positive\n",
            "\n",
            "System out:  ['Standard Chartered', 'Royal Bank of Scotland', 'subsidiary', 0.8384095430374146]\n",
            "------------------------------\n",
            "Sentence 35: Shares of Nokia Corp. rose Thursday after the cell phone maker said its third-quarter earnings almost doubled and its share of the global handset market increased .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 36: In its financial report , published on Friday , SEB said its net profit soared to SEK6 .745 bn in 2010 from a year-earlier SEK1 .114 bn and proposed a 50 % dividend increase to SEK1 .50 per share .@positive\n",
            "\n",
            "System out:  ['SEB', 'SE', 'subsidiary', 0.9945185780525208]\n",
            "------------------------------\n",
            "Sentence 37: At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .@neutral\n",
            "\n",
            "System out:  ['Finland', 'Finnish', 'country', 0.9643352031707764]\n",
            "------------------------------\n",
            "Sentence 38: STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .@neutral\n",
            "\n",
            "System out:  ['Bas', '##are', 'has part', 0.27988606691360474]\n",
            "------------------------------\n",
            "Sentence 39: A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .@neutral\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 40: Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .@neutral\n",
            "\n",
            "System out:  ['Finnish', 'Tiimari', 'owned by', 0.8387601375579834]\n",
            "------------------------------\n",
            "Sentence 41: The acquisition will considerably increase Kemira 's sales and market position in the Russian metal industry coatings market .@positive\n",
            "\n",
            "System out:  ['Russian', 'Kemira', 'owned by', 0.5403813719749451]\n",
            "------------------------------\n",
            "Sentence 42: In January-September 2007 , Finnlines ' net sales rose to EUR 505.4 mn from EUR 473.5 mn in the corresponding period in 2006 .@positive\n",
            "\n",
            "System out:  ['EU', 'EU', 'applies to jurisdiction', 0.9113050699234009]\n",
            "------------------------------\n",
            "Sentence 43: Adjusted for changes in the Group structure , the Division 's net sales increased by 1.7 % .@positive\n",
            "\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 44: 4 February 2011 - Finnish broadband data communication systems provider Teleste Oyj HEL : TLT1V saw its net profit jump to EUR2 .1 m for the last quarter of 2010 from EUR995 ,000 for the same period of 2009 .@positive\n",
            "\n",
            "System out:  ['Finnish', 'EU', 'has part', 0.5220372676849365]\n",
            "------------------------------\n",
            "Sentence 45: Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .@positive\n",
            "\n",
            "System out:  ['Finnish Aktia Group', 'EU', 'owned by', 0.6085987687110901]\n",
            "------------------------------\n",
            "Sentence 46: Finnish Bank of +àland 's consolidated net operating profit increased from EUR 4.8 mn in the first quarter of 2005 to EUR 6.4 mn in the first quarter of 2006 .@positive\n",
            "\n",
            "System out:  ['Finnish Bank of + àland', 'EU', 'instance of', 0.256258100271225]\n",
            "------------------------------\n",
            "Sentence 47: Finnish financial group Aktia reports operating profit of EUR 44.4 mn in January-September 2009 , up from EUR 37.3 mn in the corresponding period in 2008 .@positive\n",
            "\n",
            "System out:  ['Finnish', 'Aktia', 'owned by', 0.9477018713951111]\n",
            "------------------------------\n",
            "Sentence 48: Finnish high technology provider Vaahto Group reports net sales of EUR 41.8 mn in the accounting period September 2007 - February 2008 , an increase of 11.2 % from a year earlier .@positive\n",
            "\n",
            "System out:  ['Finnish', 'Vaahto Group', 'owned by', 0.8182757496833801]\n",
            "------------------------------\n",
            "Sentence 49: Net sales of Finnish food industry company L+ñnnen Tehtaat 's continuing operations increased by 13 % in 2008 to EUR 349.1 mn from EUR 309.6 mn in 2007 .@positive\n",
            "\n",
            "System out:  ['Finnish', 'EU', 'has part', 0.6022451519966125]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "sentences = open('../Sentences_AllAgree.txt', 'r', encoding='iso-8859-1')\n",
        "for n, sequence in enumerate(sentences.readlines()[0:50]):\n",
        "  out = nre(sequence)\n",
        "  print(\"Sentence {0}: {1}\".format(n, sequence))\n",
        "  print(\"System out: \", out)\n",
        "  print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4GY9z0SOQrL"
      },
      "source": [
        "2. Les scores des relations sont souvent plus faibles.\n",
        "Cela s'explique car il s'agit de phrases d'un thème spécifique : la finance. Il faudrait donc un dataset d'entrainement plus adapté, avec un ensemble de ressources de finance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tp_re.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
