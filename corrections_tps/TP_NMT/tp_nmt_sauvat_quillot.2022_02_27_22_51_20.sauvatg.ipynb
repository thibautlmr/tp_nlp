{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9497,"status":"ok","timestamp":1645989855630,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"gJwGumgfbLpJ","outputId":"62a34228-f30c-4333-ecdd-8e470cc67424"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting OpenNMT-py\n","  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n","\u001b[K     |████████████████████████████████| 216 kB 5.5 MB/s \n","\u001b[?25hCollecting pyonmttok<2,>=1.23\n","  Downloading pyonmttok-1.30.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n","\u001b[K     |████████████████████████████████| 16.3 MB 78 kB/s \n","\u001b[?25hCollecting configargparse\n","  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n","Collecting torchtext==0.5.0\n","  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (3.13)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.4)\n","Collecting waitress\n","  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.21.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 34.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (2.23.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.43.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (57.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (4.11.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.10.0.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.2.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py) (2.0.1)\n","Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.11.0\n","    Uninstalling torchtext-0.11.0:\n","      Successfully uninstalled torchtext-0.11.0\n","Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 pyonmttok-1.30.1 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.0.0\n"]}],"source":["!pip install OpenNMT-py"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1645989930947,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"Dayn60aThJYt","outputId":"2e2fd199-df38-481d-adc9-9491daf0db1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/TAL/tp_nmt\n"]}],"source":["%cd drive/MyDrive/TAL/tp_nmt/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18854,"status":"ok","timestamp":1645989925959,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"nDh9O1R0pHRD","outputId":"4ab6eb9a-a3d5-48e7-e23a-d432165a0643"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"uQjkjoNhiM9E"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"id":"6HRcj2Y8iZLZ"},"source":["Ces fichiers permettent d'entrainer le réseau de neuronnes. Il y a ainsi un fichier contenant les phrases en anglais et un autre contenant les traduction en français. Ainsi un premier fichier servira d'input et le deuxième d'output. <span style=\"color:red\">ca veut dire quoi que un fichier \"serve d'output\"?</span>"]},{"cell_type":"markdown","metadata":{"id":"CPB14fn7i_Ag"},"source":["## Question 2"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7898,"status":"ok","timestamp":1645989965735,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"gEWEajCeh-6e","outputId":"3ee85d05-0a72-4689-e74d-be25ee8daa23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","WARNING: No known abbreviations for language 'en', attempting fall-back to English version...\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","WARNING: No known abbreviations for language 'en', attempting fall-back to English version...\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","WARNING: No known abbreviations for language 'en', attempting fall-back to English version...\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","WARNING: No known abbreviations for language 'en', attempting fall-back to English version...\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","WARNING: No known abbreviations for language 'en', attempting fall-back to English version...\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","WARNING: No known abbreviations for language 'en', attempting fall-back to English version...\n"]},{"data":{"text/plain":[]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd BTEC-en-fr/dev/\n","awk -F '\\' '{print $NF}' IWSLT10.devset1_CSTAR03.en.txt > IWSLT10.devset1_CSTAR03.en.clean.txt\n","perl ../../tokenizer.perl -l en -lc < ./IWSLT10.devset1_CSTAR03.en.clean.txt > IWSLT10.devset1_CSTAR03.en.tok.txt\n","awk -F '\\' '{print $NF}' IWSLT10.devset1_CSTAR03.fr.txt > IWSLT10.devset1_CSTAR03.fr.clean.txt\n","perl ../../tokenizer.perl -l en -lc < ./IWSLT10.devset1_CSTAR03.fr.clean.txt > IWSLT10.devset1_CSTAR03.fr.tok.txt\n","cd ../test/\n","awk -F '\\' '{print $NF}' IWSLT09_BTEC.testset.en.txt > IWSLT09_BTEC.testset.en.clean.txt\n","perl ../../tokenizer.perl -l en -lc < ./IWSLT09_BTEC.testset.en.clean.txt > IWSLT09_BTEC.testset.en.tok.txt\n","awk -F '\\' '{print $NF}' IWSLT09_BTEC.testset.fr.txt > IWSLT09_BTEC.testset.fr.clean.txt\n","perl ../../tokenizer.perl -l en -lc < ./IWSLT09_BTEC.testset.fr.clean.txt > IWSLT09_BTEC.testset.fr.tok.txt\n","cd ../train/\n","awk -F '\\' '{print $NF}' IWSLT10_BTEC.train.en.txt > IWSLT10_BTEC.train.en.clean.txt\n","perl ../../tokenizer.perl -l en -lc < ./IWSLT10_BTEC.train.en.clean.txt >IWSLT10_BTEC.train.en.tok.txt\n","awk -F '\\' '{print $NF}' IWSLT10_BTEC.train.fr.txt > IWSLT10_BTEC.train.fr.clean.txt\n","perl ../../tokenizer.perl -l en -lc < ./IWSLT10_BTEC.train.fr.clean.txt >IWSLT10_BTEC.train.fr.tok.txt"]},{"cell_type":"markdown","metadata":{"id":"NGV2GdkUyq6n"},"source":["Les fichiers clean permettent d'enlever les identifiant au début de chaque phrase et le fichiers tok permettent de gérer la ponctuation.\n","L'intérêt est de pouvoir entrainer le réseau en reconnaissant seulement le contenu de chaque phrase et en reconnaissant les ponctuations. Chaque token est alors isolé et on pourra construire notre vocabulaire."]},{"cell_type":"markdown","metadata":{"id":"5SH6nmjz0mfG"},"source":["## Question 3"]},{"cell_type":"markdown","metadata":{"id":"PPFaNExB0shh"},"source":["La partie train est utilisé pendant le processus d'apprentissage afin d'ajuster les paramètres...\n","La partie dev est utilisé pendant le processus de validation afin de régler les hyperparamètres. \n","La partie test est utilisé pour évaluer les prédictions du modèle final mais ne participer pas à son entrainement"]},{"cell_type":"markdown","metadata":{"id":"Rz9d1cxO4_oK"},"source":["## Question 4"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2460,"status":"ok","timestamp":1645990525711,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"iK0WTWFG5KOK","outputId":"100aa746-8288-4319-e627-6035c91271c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 19:35:24,970 INFO] Counter vocab from -1 samples.\n","[2022-02-27 19:35:24,970 INFO] n_sample=-1: Build vocab on full datasets.\n","[2022-02-27 19:35:24,983 INFO] train's transforms: TransformPipe()\n","[2022-02-27 19:35:25,289 INFO] Counters src:10413\n","[2022-02-27 19:35:25,289 INFO] Counters tgt:8194\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/build_vocab.py\", line 71, in main\n","    build_vocab_main(opts)\n","  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/build_vocab.py\", line 55, in build_vocab_main\n","    save_counter(src_counter, opts.src_vocab)\n","  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/build_vocab.py\", line 44, in save_counter\n","    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n","  File \"/usr/local/lib/python3.7/dist-packages/onmt/utils/misc.py\", line 17, in check_path\n","    raise IOError(f\"path {path} exists, stop.\")\n","OSError: path ./BTEC-en-fr/BTEC-en-fr/vocab.src exists, stop.\n"]}],"source":["!onmt_build_vocab -config config-base.yaml -n_sample -1"]},{"cell_type":"markdown","metadata":{"id":"fZgsTwEE6ODN"},"source":["Le modèle de TA a besoin d'un vocabulaire afin de pouvoir calculer les probabilités de traduction des mots et ajuster ces dernières.\n","Un vocabulaire basé sur les données d'apprentissage sera plus précis et nous permettra de développer un meilleur modèle.\n","\n","<span style=\"color:red\">Ca veut dire quoi \"plus précis\"? Les tokens du vocabulaire basé sur l'ensemble d'apprentissage sont suffisants pour l’entraînement du modèle parce que pendent l’entraînement le modèle ne voit que des phrases issues du training set. En autre, utiliser un vocabulaire plus vaste serait inefficace et gourmand en calcul et en mémoire, car des tokens de ce vocabulaire ne seront jamais utilisés pour l’entraînement.</span> "]},{"cell_type":"markdown","metadata":{"id":"O7tA0CvR6yZk"},"source":["## Question 5"]},{"cell_type":"markdown","metadata":{"id":"0zqIx4Ms7jvQ"},"source":["Counters src indique le nombre de token du langage source et Counters tgt le nombre de tokens du langage cible <span style=\"color:red\">plus précisement: le nombre de tokens uniques présents dans le vocabulaire source et cible.</span> "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46259,"status":"ok","timestamp":1645962406690,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"rDmLVxVM86Et","outputId":"62b506cb-27b9-40ef-8645-a7a5649e006b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 11:46:02,098 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 11:46:02,433 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 11:46:02,434 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 11:46:02,434 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 11:46:02,435 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 11:46:02,435 INFO] Loading vocab from text file...\n","[2022-02-27 11:46:02,435 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 11:46:02,733 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 11:46:02,738 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 11:46:03,042 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 11:46:03,046 INFO] Building fields with vocab in counters...\n","[2022-02-27 11:46:03,055 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 11:46:03,068 INFO]  * src vocab size: 10415.\n","[2022-02-27 11:46:03,069 INFO]  * src vocab size = 10415\n","[2022-02-27 11:46:03,069 INFO]  * tgt vocab size = 8198\n","[2022-02-27 11:46:03,074 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 11:46:12,430 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 11:46:12,431 INFO] encoder: 5852620\n","[2022-02-27 11:46:12,431 INFO] decoder: 7244222\n","[2022-02-27 11:46:12,431 INFO] * number of parameters: 13096842\n","[2022-02-27 11:46:12,433 INFO] Starting training on GPU: [0]\n","[2022-02-27 11:46:12,433 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 11:46:12,433 INFO] train's transforms: TransformPipe()\n","[2022-02-27 11:46:12,433 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 11:46:12,866 INFO] Step 20/ 2000; acc:  10.13; ppl: 1900.46; xent: 7.55; lr: 1.00000; 13331/13865 tok/s;      0 sec\n","[2022-02-27 11:46:13,131 INFO] Step 40/ 2000; acc:  16.53; ppl: 421.39; xent: 6.04; lr: 1.00000; 23535/25237 tok/s;      1 sec\n","[2022-02-27 11:46:13,472 INFO] Step 60/ 2000; acc:  14.58; ppl: 376.95; xent: 5.93; lr: 1.00000; 22243/22828 tok/s;      1 sec\n","[2022-02-27 11:46:13,782 INFO] Step 80/ 2000; acc:  20.65; ppl: 196.50; xent: 5.28; lr: 1.00000; 19029/19888 tok/s;      1 sec\n","[2022-02-27 11:46:14,048 INFO] Step 100/ 2000; acc:  22.20; ppl: 187.31; xent: 5.23; lr: 1.00000; 24699/25887 tok/s;      2 sec\n","[2022-02-27 11:46:14,344 INFO] Step 120/ 2000; acc:  20.16; ppl: 188.84; xent: 5.24; lr: 1.00000; 25350/25249 tok/s;      2 sec\n","[2022-02-27 11:46:14,621 INFO] Step 140/ 2000; acc:  25.12; ppl: 124.24; xent: 4.82; lr: 1.00000; 21146/22530 tok/s;      2 sec\n","[2022-02-27 11:46:14,882 INFO] Step 160/ 2000; acc:  29.91; ppl: 106.52; xent: 4.67; lr: 1.00000; 24140/25343 tok/s;      2 sec\n","[2022-02-27 11:46:15,141 INFO] Step 180/ 2000; acc:  28.18; ppl: 106.38; xent: 4.67; lr: 1.00000; 25472/26116 tok/s;      3 sec\n","[2022-02-27 11:46:15,455 INFO] Step 200/ 2000; acc:  27.84; ppl: 116.08; xent: 4.75; lr: 1.00000; 21609/22740 tok/s;      3 sec\n","[2022-02-27 11:46:15,699 INFO] Step 220/ 2000; acc:  32.78; ppl: 78.54; xent: 4.36; lr: 1.00000; 25023/27145 tok/s;      3 sec\n","[2022-02-27 11:46:15,952 INFO] Step 240/ 2000; acc:  33.86; ppl: 74.27; xent: 4.31; lr: 1.00000; 23279/24456 tok/s;      4 sec\n","[2022-02-27 11:46:16,272 INFO] Step 260/ 2000; acc:  31.06; ppl: 85.35; xent: 4.45; lr: 1.00000; 20719/22117 tok/s;      4 sec\n","[2022-02-27 11:46:16,508 INFO] Step 280/ 2000; acc:  37.29; ppl: 57.28; xent: 4.05; lr: 1.00000; 24001/25556 tok/s;      4 sec\n","[2022-02-27 11:46:16,780 INFO] Step 300/ 2000; acc:  34.78; ppl: 65.31; xent: 4.18; lr: 1.00000; 22878/24569 tok/s;      4 sec\n","[2022-02-27 11:46:17,077 INFO] Step 320/ 2000; acc:  30.34; ppl: 86.38; xent: 4.46; lr: 1.00000; 24318/24831 tok/s;      5 sec\n","[2022-02-27 11:46:17,403 INFO] Step 340/ 2000; acc:  37.00; ppl: 53.22; xent: 3.97; lr: 1.00000; 18091/18539 tok/s;      5 sec\n","[2022-02-27 11:46:17,648 INFO] Step 360/ 2000; acc:  35.48; ppl: 58.41; xent: 4.07; lr: 1.00000; 26007/26609 tok/s;      5 sec\n","[2022-02-27 11:46:17,959 INFO] Step 380/ 2000; acc:  31.51; ppl: 73.23; xent: 4.29; lr: 1.00000; 24747/24737 tok/s;      6 sec\n","[2022-02-27 11:46:18,251 INFO] Step 400/ 2000; acc:  38.88; ppl: 45.73; xent: 3.82; lr: 1.00000; 20257/20645 tok/s;      6 sec\n","[2022-02-27 11:46:18,504 INFO] Step 420/ 2000; acc:  36.62; ppl: 50.76; xent: 3.93; lr: 1.00000; 26150/26340 tok/s;      6 sec\n","[2022-02-27 11:46:18,813 INFO] Step 440/ 2000; acc:  32.75; ppl: 69.51; xent: 4.24; lr: 1.00000; 24524/24255 tok/s;      6 sec\n","[2022-02-27 11:46:19,102 INFO] Step 460/ 2000; acc:  41.15; ppl: 42.12; xent: 3.74; lr: 1.00000; 20147/21133 tok/s;      7 sec\n","[2022-02-27 11:46:19,349 INFO] Step 480/ 2000; acc:  37.42; ppl: 50.51; xent: 3.92; lr: 1.00000; 25269/26790 tok/s;      7 sec\n","[2022-02-27 11:46:19,627 INFO] Step 500/ 2000; acc:  36.60; ppl: 48.73; xent: 3.89; lr: 1.00000; 23633/24688 tok/s;      7 sec\n","[2022-02-27 11:46:19,953 INFO] Step 520/ 2000; acc:  35.77; ppl: 56.44; xent: 4.03; lr: 1.00000; 20541/21605 tok/s;      8 sec\n","[2022-02-27 11:46:20,203 INFO] Step 540/ 2000; acc:  40.48; ppl: 40.79; xent: 3.71; lr: 1.00000; 24745/26482 tok/s;      8 sec\n","[2022-02-27 11:46:20,457 INFO] Step 560/ 2000; acc:  40.08; ppl: 42.92; xent: 3.76; lr: 1.00000; 23593/25228 tok/s;      8 sec\n","[2022-02-27 11:46:20,692 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 11:46:20,775 INFO] Step 580/ 2000; acc:  37.38; ppl: 48.69; xent: 3.89; lr: 1.00000; 21244/22261 tok/s;      8 sec\n","[2022-02-27 11:46:21,026 INFO] Step 600/ 2000; acc:  43.70; ppl: 33.54; xent: 3.51; lr: 1.00000; 22467/23579 tok/s;      9 sec\n","[2022-02-27 11:46:21,287 INFO] Step 620/ 2000; acc:  39.01; ppl: 42.19; xent: 3.74; lr: 1.00000; 23996/25895 tok/s;      9 sec\n","[2022-02-27 11:46:21,367 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 11:46:21,492 INFO] Validation perplexity: 23.4902\n","[2022-02-27 11:46:21,493 INFO] Validation accuracy: 48.2383\n","[2022-02-27 11:46:21,545 INFO] Saving checkpoint ./models/base/model_step_625.pt\n","[2022-02-27 11:46:23,036 INFO] Step 640/ 2000; acc:  36.48; ppl: 52.78; xent: 3.97; lr: 1.00000; 4073/4167 tok/s;     11 sec\n","[2022-02-27 11:46:23,374 INFO] Step 660/ 2000; acc:  44.20; ppl: 30.35; xent: 3.41; lr: 1.00000; 17007/17924 tok/s;     11 sec\n","[2022-02-27 11:46:23,659 INFO] Step 680/ 2000; acc:  41.38; ppl: 37.28; xent: 3.62; lr: 1.00000; 21837/23125 tok/s;     11 sec\n","[2022-02-27 11:46:23,983 INFO] Step 700/ 2000; acc:  36.89; ppl: 47.21; xent: 3.85; lr: 1.00000; 23144/23518 tok/s;     12 sec\n","[2022-02-27 11:46:24,277 INFO] Step 720/ 2000; acc:  43.33; ppl: 33.04; xent: 3.50; lr: 1.00000; 20140/21213 tok/s;     12 sec\n","[2022-02-27 11:46:24,533 INFO] Step 740/ 2000; acc:  42.31; ppl: 35.37; xent: 3.57; lr: 1.00000; 26051/26498 tok/s;     12 sec\n","[2022-02-27 11:46:24,827 INFO] Step 760/ 2000; acc:  37.62; ppl: 46.33; xent: 3.84; lr: 1.00000; 25927/26182 tok/s;     12 sec\n","[2022-02-27 11:46:25,121 INFO] Step 780/ 2000; acc:  45.02; ppl: 29.71; xent: 3.39; lr: 1.00000; 19814/20913 tok/s;     13 sec\n","[2022-02-27 11:46:25,359 INFO] Step 800/ 2000; acc:  44.11; ppl: 29.00; xent: 3.37; lr: 1.00000; 25734/27503 tok/s;     13 sec\n","[2022-02-27 11:46:25,626 INFO] Step 820/ 2000; acc:  42.59; ppl: 32.97; xent: 3.50; lr: 1.00000; 24158/25333 tok/s;     13 sec\n","[2022-02-27 11:46:25,964 INFO] Step 840/ 2000; acc:  41.99; ppl: 37.27; xent: 3.62; lr: 1.00000; 19828/20867 tok/s;     14 sec\n","[2022-02-27 11:46:26,212 INFO] Step 860/ 2000; acc:  45.68; ppl: 26.94; xent: 3.29; lr: 1.00000; 24749/26514 tok/s;     14 sec\n","[2022-02-27 11:46:26,454 INFO] Step 880/ 2000; acc:  45.03; ppl: 29.19; xent: 3.37; lr: 1.00000; 24656/25981 tok/s;     14 sec\n","[2022-02-27 11:46:26,773 INFO] Step 900/ 2000; acc:  41.75; ppl: 32.56; xent: 3.48; lr: 1.00000; 21026/22547 tok/s;     14 sec\n","[2022-02-27 11:46:27,012 INFO] Step 920/ 2000; acc:  48.20; ppl: 23.02; xent: 3.14; lr: 1.00000; 23949/24745 tok/s;     15 sec\n","[2022-02-27 11:46:27,274 INFO] Step 940/ 2000; acc:  44.26; ppl: 29.00; xent: 3.37; lr: 1.00000; 24144/25547 tok/s;     15 sec\n","[2022-02-27 11:46:27,558 INFO] Step 960/ 2000; acc:  40.23; ppl: 39.48; xent: 3.68; lr: 1.00000; 25874/26410 tok/s;     15 sec\n","[2022-02-27 11:46:27,877 INFO] Step 980/ 2000; acc:  47.61; ppl: 24.05; xent: 3.18; lr: 1.00000; 18248/19126 tok/s;     15 sec\n","[2022-02-27 11:46:28,191 INFO] Step 1000/ 2000; acc:  45.85; ppl: 25.72; xent: 3.25; lr: 1.00000; 20208/20176 tok/s;     16 sec\n","[2022-02-27 11:46:28,635 INFO] Step 1020/ 2000; acc:  41.72; ppl: 32.30; xent: 3.47; lr: 1.00000; 16887/16820 tok/s;     16 sec\n","[2022-02-27 11:46:28,912 INFO] Step 1040/ 2000; acc:  46.74; ppl: 23.06; xent: 3.14; lr: 1.00000; 21325/22717 tok/s;     16 sec\n","[2022-02-27 11:46:29,176 INFO] Step 1060/ 2000; acc:  45.29; ppl: 26.15; xent: 3.26; lr: 1.00000; 25450/25351 tok/s;     17 sec\n","[2022-02-27 11:46:29,479 INFO] Step 1080/ 2000; acc:  39.70; ppl: 37.64; xent: 3.63; lr: 1.00000; 25640/25412 tok/s;     17 sec\n","[2022-02-27 11:46:29,760 INFO] Step 1100/ 2000; acc:  49.10; ppl: 22.35; xent: 3.11; lr: 1.00000; 20724/21717 tok/s;     17 sec\n","[2022-02-27 11:46:30,016 INFO] Step 1120/ 2000; acc:  45.47; ppl: 26.04; xent: 3.26; lr: 1.00000; 24332/25426 tok/s;     18 sec\n","[2022-02-27 11:46:30,294 INFO] Step 1140/ 2000; acc:  45.11; ppl: 25.42; xent: 3.24; lr: 1.00000; 23525/25062 tok/s;     18 sec\n","[2022-02-27 11:46:30,593 INFO] Step 1160/ 2000; acc:  44.35; ppl: 28.92; xent: 3.36; lr: 1.00000; 22093/23322 tok/s;     18 sec\n","[2022-02-27 11:46:30,850 INFO] Step 1180/ 2000; acc:  47.14; ppl: 22.73; xent: 3.12; lr: 1.00000; 23846/25883 tok/s;     18 sec\n","[2022-02-27 11:46:31,104 INFO] Step 1200/ 2000; acc:  47.83; ppl: 22.87; xent: 3.13; lr: 1.00000; 23189/25293 tok/s;     19 sec\n","[2022-02-27 11:46:31,334 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 11:46:31,428 INFO] Step 1220/ 2000; acc:  44.76; ppl: 27.44; xent: 3.31; lr: 1.00000; 20868/21199 tok/s;     19 sec\n","[2022-02-27 11:46:31,663 INFO] Step 1240/ 2000; acc:  50.48; ppl: 19.53; xent: 2.97; lr: 1.00000; 24356/25269 tok/s;     19 sec\n","[2022-02-27 11:46:31,917 INFO] Validation perplexity: 13.1062\n","[2022-02-27 11:46:31,917 INFO] Validation accuracy: 55.8414\n","[2022-02-27 11:46:31,972 INFO] Saving checkpoint ./models/base/model_step_1250.pt\n","[2022-02-27 11:46:33,037 INFO] Step 1260/ 2000; acc:  46.76; ppl: 23.86; xent: 3.17; lr: 1.00000; 4581/4902 tok/s;     21 sec\n","[2022-02-27 11:46:33,363 INFO] Step 1280/ 2000; acc:  43.24; ppl: 30.00; xent: 3.40; lr: 1.00000; 21740/22608 tok/s;     21 sec\n","[2022-02-27 11:46:33,681 INFO] Step 1300/ 2000; acc:  50.44; ppl: 18.15; xent: 2.90; lr: 1.00000; 18325/19094 tok/s;     21 sec\n","[2022-02-27 11:46:33,938 INFO] Step 1320/ 2000; acc:  48.18; ppl: 21.09; xent: 3.05; lr: 1.00000; 24475/25269 tok/s;     22 sec\n","[2022-02-27 11:46:34,273 INFO] Step 1340/ 2000; acc:  43.47; ppl: 27.12; xent: 3.30; lr: 1.00000; 22679/22996 tok/s;     22 sec\n","[2022-02-27 11:46:34,552 INFO] Step 1360/ 2000; acc:  49.29; ppl: 19.97; xent: 2.99; lr: 1.00000; 21153/22659 tok/s;     22 sec\n","[2022-02-27 11:46:34,818 INFO] Step 1380/ 2000; acc:  47.17; ppl: 22.24; xent: 3.10; lr: 1.00000; 24805/25760 tok/s;     22 sec\n","[2022-02-27 11:46:35,124 INFO] Step 1400/ 2000; acc:  44.43; ppl: 28.27; xent: 3.34; lr: 1.00000; 25014/24837 tok/s;     23 sec\n","[2022-02-27 11:46:35,407 INFO] Step 1420/ 2000; acc:  50.61; ppl: 17.81; xent: 2.88; lr: 1.00000; 20218/21745 tok/s;     23 sec\n","[2022-02-27 11:46:35,648 INFO] Step 1440/ 2000; acc:  49.80; ppl: 19.10; xent: 2.95; lr: 1.00000; 24982/27456 tok/s;     23 sec\n","[2022-02-27 11:46:35,915 INFO] Step 1460/ 2000; acc:  47.98; ppl: 20.98; xent: 3.04; lr: 1.00000; 23638/24788 tok/s;     23 sec\n","[2022-02-27 11:46:36,237 INFO] Step 1480/ 2000; acc:  47.81; ppl: 22.48; xent: 3.11; lr: 1.00000; 20544/21876 tok/s;     24 sec\n","[2022-02-27 11:46:36,504 INFO] Step 1500/ 2000; acc:  50.70; ppl: 17.24; xent: 2.85; lr: 1.00000; 23286/24603 tok/s;     24 sec\n","[2022-02-27 11:46:36,754 INFO] Step 1520/ 2000; acc:  50.28; ppl: 19.88; xent: 2.99; lr: 1.00000; 23976/25213 tok/s;     24 sec\n","[2022-02-27 11:46:37,087 INFO] Step 1540/ 2000; acc:  45.47; ppl: 23.62; xent: 3.16; lr: 1.00000; 20324/21500 tok/s;     25 sec\n","[2022-02-27 11:46:37,347 INFO] Step 1560/ 2000; acc:  52.47; ppl: 15.17; xent: 2.72; lr: 1.00000; 22268/23494 tok/s;     25 sec\n","[2022-02-27 11:46:37,602 INFO] Step 1580/ 2000; acc:  49.35; ppl: 18.38; xent: 2.91; lr: 1.00000; 25126/25942 tok/s;     25 sec\n","[2022-02-27 11:46:37,884 INFO] Step 1600/ 2000; acc:  44.54; ppl: 26.22; xent: 3.27; lr: 1.00000; 26355/26245 tok/s;     25 sec\n","[2022-02-27 11:46:38,272 INFO] Step 1620/ 2000; acc:  52.81; ppl: 15.47; xent: 2.74; lr: 1.00000; 15048/15631 tok/s;     26 sec\n","[2022-02-27 11:46:38,619 INFO] Step 1640/ 2000; acc:  49.96; ppl: 18.07; xent: 2.89; lr: 1.00000; 18283/18519 tok/s;     26 sec\n","[2022-02-27 11:46:38,947 INFO] Step 1660/ 2000; acc:  44.72; ppl: 24.15; xent: 3.18; lr: 1.00000; 23291/23212 tok/s;     27 sec\n","[2022-02-27 11:46:39,231 INFO] Step 1680/ 2000; acc:  51.89; ppl: 15.73; xent: 2.76; lr: 1.00000; 20680/21311 tok/s;     27 sec\n","[2022-02-27 11:46:39,482 INFO] Step 1700/ 2000; acc:  49.32; ppl: 18.28; xent: 2.91; lr: 1.00000; 26361/26963 tok/s;     27 sec\n","[2022-02-27 11:46:39,791 INFO] Step 1720/ 2000; acc:  44.77; ppl: 24.95; xent: 3.22; lr: 1.00000; 24608/23992 tok/s;     27 sec\n","[2022-02-27 11:46:40,073 INFO] Step 1740/ 2000; acc:  52.24; ppl: 15.55; xent: 2.74; lr: 1.00000; 20329/22236 tok/s;     28 sec\n","[2022-02-27 11:46:40,338 INFO] Step 1760/ 2000; acc:  49.32; ppl: 17.54; xent: 2.86; lr: 1.00000; 23424/24589 tok/s;     28 sec\n","[2022-02-27 11:46:40,595 INFO] Step 1780/ 2000; acc:  49.48; ppl: 17.38; xent: 2.86; lr: 1.00000; 25373/27216 tok/s;     28 sec\n","[2022-02-27 11:46:40,906 INFO] Step 1800/ 2000; acc:  47.29; ppl: 21.27; xent: 3.06; lr: 1.00000; 21408/22497 tok/s;     28 sec\n","[2022-02-27 11:46:41,150 INFO] Step 1820/ 2000; acc:  50.88; ppl: 16.42; xent: 2.80; lr: 1.00000; 24966/27393 tok/s;     29 sec\n","[2022-02-27 11:46:41,404 INFO] Step 1840/ 2000; acc:  52.18; ppl: 15.69; xent: 2.75; lr: 1.00000; 23166/24798 tok/s;     29 sec\n","[2022-02-27 11:46:41,637 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 11:46:41,735 INFO] Step 1860/ 2000; acc:  49.80; ppl: 17.81; xent: 2.88; lr: 1.00000; 20246/20679 tok/s;     29 sec\n","[2022-02-27 11:46:42,032 INFO] Validation perplexity: 10.2442\n","[2022-02-27 11:46:42,032 INFO] Validation accuracy: 58.9244\n","[2022-02-27 11:46:42,085 INFO] Saving checkpoint ./models/base/model_step_1875.pt\n","[2022-02-27 11:46:43,159 INFO] Step 1880/ 2000; acc:  55.40; ppl: 13.07; xent: 2.57; lr: 1.00000; 4033/4118 tok/s;     31 sec\n","[2022-02-27 11:46:43,481 INFO] Step 1900/ 2000; acc:  50.18; ppl: 17.29; xent: 2.85; lr: 1.00000; 23475/25315 tok/s;     31 sec\n","[2022-02-27 11:46:43,783 INFO] Step 1920/ 2000; acc:  47.04; ppl: 21.00; xent: 3.04; lr: 1.00000; 23857/24879 tok/s;     31 sec\n","[2022-02-27 11:46:44,106 INFO] Step 1940/ 2000; acc:  54.52; ppl: 13.18; xent: 2.58; lr: 1.00000; 18091/19095 tok/s;     32 sec\n","[2022-02-27 11:46:44,375 INFO] Step 1960/ 2000; acc:  52.17; ppl: 15.13; xent: 2.72; lr: 1.00000; 23611/24338 tok/s;     32 sec\n","[2022-02-27 11:46:44,690 INFO] Step 1980/ 2000; acc:  46.64; ppl: 21.24; xent: 3.06; lr: 1.00000; 24250/24247 tok/s;     32 sec\n","[2022-02-27 11:46:44,976 INFO] Step 2000/ 2000; acc:  52.20; ppl: 14.24; xent: 2.66; lr: 1.00000; 20652/22387 tok/s;     33 sec\n","[2022-02-27 11:46:45,030 INFO] Saving checkpoint ./models/base/model_step_2000.pt\n"]}],"source":["!onmt_train -config config-base.yaml"]},{"cell_type":"markdown","metadata":{"id":"DnVjbgf6-meX"},"source":["## Question 6"]},{"cell_type":"markdown","metadata":{"id":"AukqB3oY_U8y"},"source":["Acc permet de calculer l'accuracy, c'est à dire le pourcentage de token correctement traduit. Il est donc préférable d'avoir la précision la plus élevée possible\n","\n","La perplexité permet de mesurer si la distribution de probabilité prédit bien un échantillon. Il est préférable d'avoir une faible ppl, ce qui indique que la distribution est bonne"]},{"cell_type":"markdown","metadata":{"id":"KuHcv0dfAZJk"},"source":["## Question 7"]},{"cell_type":"markdown","metadata":{"id":"7vt2A3KgWusf"},"source":["train_step : nombre d'étapes de descente de gradient sur les données d'entrainement <span style=\"color:red\">Nombre total d'étapes d'optimisation des poids du modèle, où chaque étape est constitué d'un passage en avant d'un lot (batch) de données d'apprentissage dans le modèle (forward pass) et d'un passage en arrière des gradients générés par la fonction de perte (backward pass). </span>\n","\n","valid_step : nombre d'étapes de descente de gradient sur les données de validation <span style=\"color:red\">Il n'y a pas de l'optimization pendent la validation. Elle sert seulement à évaluer le modèle et décider si changer des hyperparametres où arreter l'entrainement.</span> \n","\n","enc_layers : nombre de couches de l'encodeur\n","\n","dec_layers : nombre de couches du décodeur\n","\n","enc_rnn_size : nombre de couches cachées de l'encodeur <span style=\"color:red\">Taille des representations vectorielles dans l'encodeur.</span>\n","\n","dec_rnn_size : nombre de couche cachées du décodeur <span style=\"color:red\">Taille des représentations vectorielles dans le décodeur.</span>\n","\n","batch_size : Taille d'un groupe de données <span style=\"color:red\">C'est à dire? batch_size=nombre d'exemples d'entrainement (phrase source + phrase de réference) passé au modèle à chaque étape d'entrainement (avant le calcul du gradient)</span>"]},{"cell_type":"markdown","metadata":{"id":"KcqL9myYYVeG"},"source":["## Question 8"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25312,"status":"ok","timestamp":1645990861710,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"14HChkoAYnwE","outputId":"566fcb00-6793-451f-ffc4-03d86919c191"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 19:40:38,279 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 19:40:42,607 INFO] PRED AVG SCORE: -1.1741, PRED PPL: 3.2352\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 19:40:44,574 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 19:40:49,318 INFO] PRED AVG SCORE: -1.0750, PRED PPL: 2.9300\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 19:40:51,300 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 19:40:55,846 INFO] PRED AVG SCORE: -1.0240, PRED PPL: 2.7843\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 19:40:57,820 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 19:41:01,112 INFO] PRED AVG SCORE: -1.1461, PRED PPL: 3.1460\n"]},{"data":{"text/plain":[]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model models/base/model_step_1250.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred1250.txt\n","onmt_translate -model models/base/model_step_1875.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred1875.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000.txt\n","onmt_translate -model models/base/model_step_625.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred625.txt"]},{"cell_type":"markdown","metadata":{"id":"UWQFapcpOZSZ"},"source":["L'algorithme BLEU utilisé dans l'évaluation de la performance de la traduction produit un score entre 0 et 1. Une valeur proche de 1 correspond à une très bonne qualité de traduction tandis qu'une valeur proche de 0 correspond à une qualité médiocre de traduction. Ainsi, l'idéal serait d'obtenir un score de 1, même si en pratique, on cherche simplement à maximiser le score BLEU et à être aux environs de la valeur 1. <span style=\"color:red\">Ca veut dire quoi \"bonne qualité de traduction\"? En fait, un meilleur BLEU score ne veut dire pas **toujours** une meilleure traduction</span>"]},{"cell_type":"markdown","metadata":{"id":"D2Dq1qcSPShY"},"source":["# Question 9"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1645991195044,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"mLWmTBdmPWoS","outputId":"d56913a6-369c-4d72-fe84-3609a1c9007a"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 13.47, 48.7/20.3/11.9/4.7 (BP=0.880, ratio=0.887, hyp_len=3170, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 16.82, 47.6/21.0/12.7/6.3 (BP=1.000, ratio=1.030, hyp_len=3681, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 18.32, 52.0/24.3/14.8/7.4 (BP=0.952, ratio=0.953, hyp_len=3405, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 6.30, 54.5/18.3/10.4/3.5 (BP=0.456, ratio=0.560, hyp_len=2003, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred1250.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred1875.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred625.txt"]},{"cell_type":"markdown","metadata":{"id":"1dOsgVyPPXgh"},"source":["Avec les résultats précédents, on retient le modèle qui a le score BLEU le plus proche de 1, il s'agit du modèle model_step_2000"]},{"cell_type":"markdown","metadata":{"id":"13tq9qCwPiPr"},"source":["# Question 10"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385245,"status":"ok","timestamp":1645993005337,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"9NF4ojpwv6I1","outputId":"24e3ff93-2e21-4c64-a084-99317db1b73a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:10:21,839 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 20:10:21,840 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 20:10:21,841 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 20:10:21,841 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 20:10:21,842 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 20:10:21,842 INFO] Loading vocab from text file...\n","[2022-02-27 20:10:21,843 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 20:10:21,871 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 20:10:21,877 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 20:10:21,899 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 20:10:21,903 INFO] Building fields with vocab in counters...\n","[2022-02-27 20:10:21,913 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 20:10:21,929 INFO]  * src vocab size: 10415.\n","[2022-02-27 20:10:21,930 INFO]  * src vocab size = 10415\n","[2022-02-27 20:10:21,930 INFO]  * tgt vocab size = 8198\n","[2022-02-27 20:10:21,934 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:10:24,567 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 20:10:24,568 INFO] encoder: 5852620\n","[2022-02-27 20:10:24,568 INFO] decoder: 7244222\n","[2022-02-27 20:10:24,568 INFO] * number of parameters: 13096842\n","[2022-02-27 20:10:24,571 INFO] Starting training on GPU: [0]\n","[2022-02-27 20:10:24,571 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 20:10:24,571 INFO] train's transforms: TransformPipe()\n","[2022-02-27 20:10:24,572 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 20:10:25,513 INFO] Step 20/10000; acc:   7.54; ppl: 7266.23; xent: 8.89; lr: 1.00000; 7555/7907 tok/s;      1 sec\n","[2022-02-27 20:10:26,259 INFO] Step 40/10000; acc:  14.39; ppl: 518.83; xent: 6.25; lr: 1.00000; 8676/9038 tok/s;      2 sec\n","[2022-02-27 20:10:26,930 INFO] Step 60/10000; acc:  18.59; ppl: 281.67; xent: 5.64; lr: 1.00000; 8806/9364 tok/s;      2 sec\n","[2022-02-27 20:10:27,751 INFO] Step 80/10000; acc:  19.61; ppl: 236.76; xent: 5.47; lr: 1.00000; 8009/8120 tok/s;      3 sec\n","[2022-02-27 20:10:28,542 INFO] Step 100/10000; acc:  19.63; ppl: 217.52; xent: 5.38; lr: 1.00000; 9291/9455 tok/s;      4 sec\n","[2022-02-27 20:10:29,198 INFO] Step 120/10000; acc:  26.24; ppl: 119.10; xent: 4.78; lr: 1.00000; 8752/9158 tok/s;      5 sec\n","[2022-02-27 20:10:30,026 INFO] Step 140/10000; acc:  24.12; ppl: 153.90; xent: 5.04; lr: 1.00000; 8252/8654 tok/s;      5 sec\n","[2022-02-27 20:10:30,785 INFO] Step 160/10000; acc:  24.64; ppl: 139.79; xent: 4.94; lr: 1.00000; 9605/9820 tok/s;      6 sec\n","[2022-02-27 20:10:31,435 INFO] Step 180/10000; acc:  32.09; ppl: 84.23; xent: 4.43; lr: 1.00000; 8638/9413 tok/s;      7 sec\n","[2022-02-27 20:10:32,228 INFO] Step 200/10000; acc:  29.08; ppl: 104.94; xent: 4.65; lr: 1.00000; 8232/8310 tok/s;      8 sec\n","[2022-02-27 20:10:32,980 INFO] Step 220/10000; acc:  30.49; ppl: 88.74; xent: 4.49; lr: 1.00000; 8920/9229 tok/s;      8 sec\n","[2022-02-27 20:10:33,633 INFO] Step 240/10000; acc:  35.13; ppl: 67.24; xent: 4.21; lr: 1.00000; 8678/9546 tok/s;      9 sec\n","[2022-02-27 20:10:34,343 INFO] Step 260/10000; acc:  35.24; ppl: 71.38; xent: 4.27; lr: 1.00000; 7909/8582 tok/s;     10 sec\n","[2022-02-27 20:10:35,173 INFO] Step 280/10000; acc:  29.33; ppl: 89.44; xent: 4.49; lr: 1.00000; 9159/9543 tok/s;     11 sec\n","[2022-02-27 20:10:35,845 INFO] Step 300/10000; acc:  35.55; ppl: 63.08; xent: 4.14; lr: 1.00000; 8736/9210 tok/s;     11 sec\n","[2022-02-27 20:10:36,530 INFO] Step 320/10000; acc:  36.78; ppl: 57.48; xent: 4.05; lr: 1.00000; 8576/9236 tok/s;     12 sec\n","[2022-02-27 20:10:37,411 INFO] Step 340/10000; acc:  32.96; ppl: 73.78; xent: 4.30; lr: 1.00000; 8282/8329 tok/s;     13 sec\n","[2022-02-27 20:10:38,167 INFO] Step 360/10000; acc:  35.34; ppl: 59.48; xent: 4.09; lr: 1.00000; 8652/8891 tok/s;     14 sec\n","[2022-02-27 20:10:38,857 INFO] Step 380/10000; acc:  37.57; ppl: 50.63; xent: 3.92; lr: 1.00000; 8723/8915 tok/s;     14 sec\n","[2022-02-27 20:10:39,672 INFO] Step 400/10000; acc:  36.02; ppl: 60.81; xent: 4.11; lr: 1.00000; 8134/7999 tok/s;     15 sec\n","[2022-02-27 20:10:40,461 INFO] Step 420/10000; acc:  33.54; ppl: 64.11; xent: 4.16; lr: 1.00000; 9386/9472 tok/s;     16 sec\n","[2022-02-27 20:10:41,125 INFO] Step 440/10000; acc:  41.20; ppl: 39.19; xent: 3.67; lr: 1.00000; 8608/8983 tok/s;     17 sec\n","[2022-02-27 20:10:41,946 INFO] Step 460/10000; acc:  34.17; ppl: 61.24; xent: 4.11; lr: 1.00000; 8243/8615 tok/s;     17 sec\n","[2022-02-27 20:10:42,756 INFO] Step 480/10000; acc:  35.02; ppl: 54.85; xent: 4.00; lr: 1.00000; 8998/9176 tok/s;     18 sec\n","[2022-02-27 20:10:43,425 INFO] Step 500/10000; acc:  41.22; ppl: 39.34; xent: 3.67; lr: 1.00000; 8335/9052 tok/s;     19 sec\n","[2022-02-27 20:10:44,191 INFO] Step 520/10000; acc:  37.82; ppl: 57.75; xent: 4.06; lr: 1.00000; 8578/8664 tok/s;     20 sec\n","[2022-02-27 20:10:44,948 INFO] Step 540/10000; acc:  38.28; ppl: 48.95; xent: 3.89; lr: 1.00000; 9044/9356 tok/s;     20 sec\n","[2022-02-27 20:10:45,589 INFO] Step 560/10000; acc:  42.03; ppl: 35.18; xent: 3.56; lr: 1.00000; 8844/9620 tok/s;     21 sec\n","[2022-02-27 20:10:46,189 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 20:10:46,347 INFO] Step 580/10000; acc:  40.56; ppl: 40.79; xent: 3.71; lr: 1.00000; 7486/8330 tok/s;     22 sec\n","[2022-02-27 20:10:47,198 INFO] Step 600/10000; acc:  37.07; ppl: 49.68; xent: 3.91; lr: 1.00000; 8907/9101 tok/s;     23 sec\n","[2022-02-27 20:10:47,865 INFO] Step 620/10000; acc:  42.68; ppl: 36.83; xent: 3.61; lr: 1.00000; 8802/9218 tok/s;     23 sec\n","[2022-02-27 20:10:48,021 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 20:10:48,279 INFO] Validation perplexity: 24.9674\n","[2022-02-27 20:10:48,280 INFO] Validation accuracy: 45.2248\n","[2022-02-27 20:10:48,789 INFO] Step 640/10000; acc:  40.27; ppl: 42.56; xent: 3.75; lr: 1.00000; 6333/6770 tok/s;     24 sec\n","[2022-02-27 20:10:49,701 INFO] Step 660/10000; acc:  38.85; ppl: 41.91; xent: 3.74; lr: 1.00000; 7716/8162 tok/s;     25 sec\n","[2022-02-27 20:10:50,416 INFO] Step 680/10000; acc:  41.11; ppl: 39.04; xent: 3.66; lr: 1.00000; 9039/9211 tok/s;     26 sec\n","[2022-02-27 20:10:51,100 INFO] Step 700/10000; acc:  42.37; ppl: 34.81; xent: 3.55; lr: 1.00000; 8589/9139 tok/s;     27 sec\n","[2022-02-27 20:10:51,897 INFO] Step 720/10000; acc:  40.84; ppl: 38.84; xent: 3.66; lr: 1.00000; 8370/8453 tok/s;     27 sec\n","[2022-02-27 20:10:52,721 INFO] Step 740/10000; acc:  39.01; ppl: 42.14; xent: 3.74; lr: 1.00000; 9037/9278 tok/s;     28 sec\n","[2022-02-27 20:10:53,373 INFO] Step 760/10000; acc:  46.57; ppl: 28.23; xent: 3.34; lr: 1.00000; 8889/9123 tok/s;     29 sec\n","[2022-02-27 20:10:54,169 INFO] Step 780/10000; acc:  39.21; ppl: 42.86; xent: 3.76; lr: 1.00000; 8467/9041 tok/s;     30 sec\n","[2022-02-27 20:10:54,929 INFO] Step 800/10000; acc:  41.71; ppl: 35.25; xent: 3.56; lr: 1.00000; 9408/9483 tok/s;     30 sec\n","[2022-02-27 20:10:55,586 INFO] Step 820/10000; acc:  46.49; ppl: 26.45; xent: 3.28; lr: 1.00000; 8363/9323 tok/s;     31 sec\n","[2022-02-27 20:10:56,401 INFO] Step 840/10000; acc:  41.18; ppl: 38.69; xent: 3.66; lr: 1.00000; 7927/8231 tok/s;     32 sec\n","[2022-02-27 20:10:57,143 INFO] Step 860/10000; acc:  43.25; ppl: 30.91; xent: 3.43; lr: 1.00000; 9091/9465 tok/s;     33 sec\n","[2022-02-27 20:10:57,787 INFO] Step 880/10000; acc:  46.87; ppl: 25.27; xent: 3.23; lr: 1.00000; 8888/9592 tok/s;     33 sec\n","[2022-02-27 20:10:58,497 INFO] Step 900/10000; acc:  46.47; ppl: 26.47; xent: 3.28; lr: 1.00000; 8015/8612 tok/s;     34 sec\n","[2022-02-27 20:10:59,359 INFO] Step 920/10000; acc:  40.21; ppl: 37.28; xent: 3.62; lr: 1.00000; 8950/9496 tok/s;     35 sec\n","[2022-02-27 20:11:00,037 INFO] Step 940/10000; acc:  46.59; ppl: 25.90; xent: 3.25; lr: 1.00000; 8788/8921 tok/s;     35 sec\n","[2022-02-27 20:11:00,715 INFO] Step 960/10000; acc:  46.28; ppl: 27.15; xent: 3.30; lr: 1.00000; 8797/9137 tok/s;     36 sec\n","[2022-02-27 20:11:01,591 INFO] Step 980/10000; acc:  42.11; ppl: 31.80; xent: 3.46; lr: 1.00000; 8131/8215 tok/s;     37 sec\n","[2022-02-27 20:11:02,307 INFO] Step 1000/10000; acc:  45.40; ppl: 25.50; xent: 3.24; lr: 1.00000; 9064/9258 tok/s;     38 sec\n","[2022-02-27 20:11:02,369 INFO] Saving checkpoint ./Question10/model_step_1000.pt\n","[2022-02-27 20:11:03,248 INFO] Step 1020/10000; acc:  47.21; ppl: 24.52; xent: 3.20; lr: 1.00000; 6328/6503 tok/s;     39 sec\n","[2022-02-27 20:11:04,045 INFO] Step 1040/10000; acc:  44.71; ppl: 30.95; xent: 3.43; lr: 1.00000; 8445/8367 tok/s;     39 sec\n","[2022-02-27 20:11:04,836 INFO] Step 1060/10000; acc:  42.36; ppl: 31.03; xent: 3.43; lr: 1.00000; 9521/9520 tok/s;     40 sec\n","[2022-02-27 20:11:05,499 INFO] Step 1080/10000; acc:  48.26; ppl: 20.58; xent: 3.02; lr: 1.00000; 8705/9084 tok/s;     41 sec\n","[2022-02-27 20:11:06,319 INFO] Step 1100/10000; acc:  42.64; ppl: 31.49; xent: 3.45; lr: 1.00000; 8207/8763 tok/s;     42 sec\n","[2022-02-27 20:11:07,124 INFO] Step 1120/10000; acc:  44.39; ppl: 27.25; xent: 3.31; lr: 1.00000; 8972/9207 tok/s;     43 sec\n","[2022-02-27 20:11:07,776 INFO] Step 1140/10000; acc:  48.69; ppl: 21.14; xent: 3.05; lr: 1.00000; 8503/9397 tok/s;     43 sec\n","[2022-02-27 20:11:08,560 INFO] Step 1160/10000; acc:  43.21; ppl: 31.33; xent: 3.44; lr: 1.00000; 8351/8449 tok/s;     44 sec\n","[2022-02-27 20:11:09,314 INFO] Step 1180/10000; acc:  45.26; ppl: 25.99; xent: 3.26; lr: 1.00000; 8989/9361 tok/s;     45 sec\n","[2022-02-27 20:11:09,939 INFO] Step 1200/10000; acc:  50.03; ppl: 20.39; xent: 3.01; lr: 1.00000; 9041/9364 tok/s;     45 sec\n","[2022-02-27 20:11:10,544 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 20:11:10,700 INFO] Step 1220/10000; acc:  48.57; ppl: 22.40; xent: 3.11; lr: 1.00000; 7390/8204 tok/s;     46 sec\n","[2022-02-27 20:11:11,532 INFO] Step 1240/10000; acc:  43.22; ppl: 29.28; xent: 3.38; lr: 1.00000; 9110/9408 tok/s;     47 sec\n","[2022-02-27 20:11:12,174 INFO] Validation perplexity: 13.0267\n","[2022-02-27 20:11:12,175 INFO] Validation accuracy: 56.0501\n","[2022-02-27 20:11:12,505 INFO] Step 1260/10000; acc:  48.56; ppl: 21.01; xent: 3.04; lr: 1.00000; 6056/6447 tok/s;     48 sec\n","[2022-02-27 20:11:13,220 INFO] Step 1280/10000; acc:  47.40; ppl: 23.52; xent: 3.16; lr: 1.00000; 8261/8799 tok/s;     49 sec\n","[2022-02-27 20:11:14,170 INFO] Step 1300/10000; acc:  45.43; ppl: 23.97; xent: 3.18; lr: 1.00000; 7487/7792 tok/s;     50 sec\n","[2022-02-27 20:11:14,876 INFO] Step 1320/10000; acc:  47.85; ppl: 22.14; xent: 3.10; lr: 1.00000; 9272/9427 tok/s;     50 sec\n","[2022-02-27 20:11:15,568 INFO] Step 1340/10000; acc:  49.15; ppl: 20.35; xent: 3.01; lr: 1.00000; 8596/9010 tok/s;     51 sec\n","[2022-02-27 20:11:16,377 INFO] Step 1360/10000; acc:  46.05; ppl: 25.08; xent: 3.22; lr: 1.00000; 8258/8415 tok/s;     52 sec\n","[2022-02-27 20:11:17,164 INFO] Step 1380/10000; acc:  44.61; ppl: 25.57; xent: 3.24; lr: 1.00000; 9404/9673 tok/s;     53 sec\n","[2022-02-27 20:11:17,849 INFO] Step 1400/10000; acc:  51.11; ppl: 18.51; xent: 2.92; lr: 1.00000; 8382/8802 tok/s;     53 sec\n","[2022-02-27 20:11:18,654 INFO] Step 1420/10000; acc:  46.37; ppl: 25.66; xent: 3.24; lr: 1.00000; 8282/8642 tok/s;     54 sec\n","[2022-02-27 20:11:19,425 INFO] Step 1440/10000; acc:  47.47; ppl: 21.40; xent: 3.06; lr: 1.00000; 9089/9343 tok/s;     55 sec\n","[2022-02-27 20:11:20,067 INFO] Step 1460/10000; acc:  51.43; ppl: 16.69; xent: 2.81; lr: 1.00000; 8440/9223 tok/s;     55 sec\n","[2022-02-27 20:11:20,875 INFO] Step 1480/10000; acc:  46.58; ppl: 23.66; xent: 3.16; lr: 1.00000; 7843/8616 tok/s;     56 sec\n","[2022-02-27 20:11:21,622 INFO] Step 1500/10000; acc:  48.93; ppl: 19.14; xent: 2.95; lr: 1.00000; 9095/9605 tok/s;     57 sec\n","[2022-02-27 20:11:22,288 INFO] Step 1520/10000; acc:  51.40; ppl: 17.87; xent: 2.88; lr: 1.00000; 8655/9303 tok/s;     58 sec\n","[2022-02-27 20:11:22,984 INFO] Step 1540/10000; acc:  52.49; ppl: 16.97; xent: 2.83; lr: 1.00000; 8339/8433 tok/s;     58 sec\n","[2022-02-27 20:11:23,853 INFO] Step 1560/10000; acc:  43.48; ppl: 26.64; xent: 3.28; lr: 1.00000; 9024/9216 tok/s;     59 sec\n","[2022-02-27 20:11:24,512 INFO] Step 1580/10000; acc:  51.17; ppl: 16.98; xent: 2.83; lr: 1.00000; 9087/9535 tok/s;     60 sec\n","[2022-02-27 20:11:25,199 INFO] Step 1600/10000; acc:  50.63; ppl: 17.56; xent: 2.87; lr: 1.00000; 8770/9086 tok/s;     61 sec\n","[2022-02-27 20:11:26,145 INFO] Step 1620/10000; acc:  45.75; ppl: 22.83; xent: 3.13; lr: 1.00000; 7625/7662 tok/s;     62 sec\n","[2022-02-27 20:11:26,849 INFO] Step 1640/10000; acc:  48.43; ppl: 18.98; xent: 2.94; lr: 1.00000; 9274/9525 tok/s;     62 sec\n","[2022-02-27 20:11:27,511 INFO] Step 1660/10000; acc:  50.67; ppl: 17.32; xent: 2.85; lr: 1.00000; 9025/9303 tok/s;     63 sec\n","[2022-02-27 20:11:28,333 INFO] Step 1680/10000; acc:  47.80; ppl: 21.67; xent: 3.08; lr: 1.00000; 8030/7985 tok/s;     64 sec\n","[2022-02-27 20:11:29,106 INFO] Step 1700/10000; acc:  47.36; ppl: 20.83; xent: 3.04; lr: 1.00000; 9610/9565 tok/s;     65 sec\n","[2022-02-27 20:11:29,762 INFO] Step 1720/10000; acc:  53.55; ppl: 14.71; xent: 2.69; lr: 1.00000; 8667/9147 tok/s;     65 sec\n","[2022-02-27 20:11:30,577 INFO] Step 1740/10000; acc:  46.22; ppl: 22.43; xent: 3.11; lr: 1.00000; 8255/8836 tok/s;     66 sec\n","[2022-02-27 20:11:31,371 INFO] Step 1760/10000; acc:  48.28; ppl: 19.73; xent: 2.98; lr: 1.00000; 9051/9435 tok/s;     67 sec\n","[2022-02-27 20:11:32,030 INFO] Step 1780/10000; acc:  52.68; ppl: 14.61; xent: 2.68; lr: 1.00000; 8401/9209 tok/s;     67 sec\n","[2022-02-27 20:11:32,862 INFO] Step 1800/10000; acc:  48.32; ppl: 20.11; xent: 3.00; lr: 1.00000; 7762/8029 tok/s;     68 sec\n","[2022-02-27 20:11:33,616 INFO] Step 1820/10000; acc:  49.53; ppl: 17.44; xent: 2.86; lr: 1.00000; 8934/9383 tok/s;     69 sec\n","[2022-02-27 20:11:34,231 INFO] Step 1840/10000; acc:  52.69; ppl: 15.01; xent: 2.71; lr: 1.00000; 9196/9425 tok/s;     70 sec\n","[2022-02-27 20:11:34,805 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 20:11:34,951 INFO] Step 1860/10000; acc:  50.79; ppl: 16.71; xent: 2.82; lr: 1.00000; 7837/8603 tok/s;     70 sec\n","[2022-02-27 20:11:35,892 INFO] Validation perplexity: 10.1371\n","[2022-02-27 20:11:35,893 INFO] Validation accuracy: 58.2522\n","[2022-02-27 20:11:36,094 INFO] Step 1880/10000; acc:  47.55; ppl: 20.84; xent: 3.04; lr: 1.00000; 6732/6950 tok/s;     72 sec\n","[2022-02-27 20:11:36,790 INFO] Step 1900/10000; acc:  51.65; ppl: 15.43; xent: 2.74; lr: 1.00000; 8491/9093 tok/s;     72 sec\n","[2022-02-27 20:11:37,480 INFO] Step 1920/10000; acc:  50.40; ppl: 17.24; xent: 2.85; lr: 1.00000; 8631/9041 tok/s;     73 sec\n","[2022-02-27 20:11:38,396 INFO] Step 1940/10000; acc:  48.81; ppl: 18.63; xent: 2.92; lr: 1.00000; 7811/8009 tok/s;     74 sec\n","[2022-02-27 20:11:39,116 INFO] Step 1960/10000; acc:  50.94; ppl: 16.87; xent: 2.83; lr: 1.00000; 9147/9340 tok/s;     75 sec\n","[2022-02-27 20:11:39,777 INFO] Step 1980/10000; acc:  52.22; ppl: 15.49; xent: 2.74; lr: 1.00000; 9053/9565 tok/s;     75 sec\n","[2022-02-27 20:11:40,571 INFO] Step 2000/10000; acc:  49.71; ppl: 19.36; xent: 2.96; lr: 1.00000; 8395/8669 tok/s;     76 sec\n","[2022-02-27 20:11:40,634 INFO] Saving checkpoint ./Question10/model_step_2000.pt\n","[2022-02-27 20:11:41,614 INFO] Step 2020/10000; acc:  48.10; ppl: 18.38; xent: 2.91; lr: 1.00000; 7099/7344 tok/s;     77 sec\n","[2022-02-27 20:11:42,300 INFO] Step 2040/10000; acc:  54.42; ppl: 13.52; xent: 2.60; lr: 1.00000; 8404/8789 tok/s;     78 sec\n","[2022-02-27 20:11:43,101 INFO] Step 2060/10000; acc:  49.90; ppl: 17.99; xent: 2.89; lr: 1.00000; 8248/8651 tok/s;     79 sec\n","[2022-02-27 20:11:43,863 INFO] Step 2080/10000; acc:  49.92; ppl: 15.93; xent: 2.77; lr: 1.00000; 9143/9415 tok/s;     79 sec\n","[2022-02-27 20:11:44,503 INFO] Step 2100/10000; acc:  55.27; ppl: 12.84; xent: 2.55; lr: 1.00000; 8503/9166 tok/s;     80 sec\n","[2022-02-27 20:11:45,308 INFO] Step 2120/10000; acc:  49.59; ppl: 17.76; xent: 2.88; lr: 1.00000; 7907/8563 tok/s;     81 sec\n","[2022-02-27 20:11:46,055 INFO] Step 2140/10000; acc:  51.43; ppl: 14.91; xent: 2.70; lr: 1.00000; 9023/9468 tok/s;     81 sec\n","[2022-02-27 20:11:46,700 INFO] Step 2160/10000; acc:  54.59; ppl: 13.18; xent: 2.58; lr: 1.00000; 8803/9344 tok/s;     82 sec\n","[2022-02-27 20:11:47,467 INFO] Step 2180/10000; acc:  54.68; ppl: 13.03; xent: 2.57; lr: 1.00000; 7420/7962 tok/s;     83 sec\n","[2022-02-27 20:11:48,331 INFO] Step 2200/10000; acc:  47.24; ppl: 20.71; xent: 3.03; lr: 1.00000; 9254/9166 tok/s;     84 sec\n","[2022-02-27 20:11:49,031 INFO] Step 2220/10000; acc:  53.82; ppl: 13.20; xent: 2.58; lr: 1.00000; 8677/9118 tok/s;     84 sec\n","[2022-02-27 20:11:49,712 INFO] Step 2240/10000; acc:  52.50; ppl: 14.33; xent: 2.66; lr: 1.00000; 8993/9174 tok/s;     85 sec\n","[2022-02-27 20:11:50,569 INFO] Step 2260/10000; acc:  48.94; ppl: 17.71; xent: 2.87; lr: 1.00000; 8376/8455 tok/s;     86 sec\n","[2022-02-27 20:11:51,272 INFO] Step 2280/10000; acc:  52.09; ppl: 14.25; xent: 2.66; lr: 1.00000; 9262/9432 tok/s;     87 sec\n","[2022-02-27 20:11:51,935 INFO] Step 2300/10000; acc:  54.92; ppl: 12.36; xent: 2.51; lr: 1.00000; 8973/9276 tok/s;     87 sec\n","[2022-02-27 20:11:52,763 INFO] Step 2320/10000; acc:  50.65; ppl: 16.09; xent: 2.78; lr: 1.00000; 7953/8064 tok/s;     88 sec\n","[2022-02-27 20:11:53,560 INFO] Step 2340/10000; acc:  49.66; ppl: 17.03; xent: 2.83; lr: 1.00000; 9379/9395 tok/s;     89 sec\n","[2022-02-27 20:11:54,208 INFO] Step 2360/10000; acc:  55.83; ppl: 11.68; xent: 2.46; lr: 1.00000; 8837/9442 tok/s;     90 sec\n","[2022-02-27 20:11:55,021 INFO] Step 2380/10000; acc:  49.51; ppl: 17.63; xent: 2.87; lr: 1.00000; 8247/8609 tok/s;     90 sec\n","[2022-02-27 20:11:55,782 INFO] Step 2400/10000; acc:  51.13; ppl: 14.83; xent: 2.70; lr: 1.00000; 9331/9555 tok/s;     91 sec\n","[2022-02-27 20:11:56,411 INFO] Step 2420/10000; acc:  56.82; ppl: 11.03; xent: 2.40; lr: 1.00000; 8684/9410 tok/s;     92 sec\n","[2022-02-27 20:11:57,241 INFO] Step 2440/10000; acc:  50.49; ppl: 15.67; xent: 2.75; lr: 1.00000; 7722/8270 tok/s;     93 sec\n","[2022-02-27 20:11:57,987 INFO] Step 2460/10000; acc:  53.47; ppl: 13.23; xent: 2.58; lr: 1.00000; 9067/9417 tok/s;     93 sec\n","[2022-02-27 20:11:58,635 INFO] Step 2480/10000; acc:  55.75; ppl: 11.51; xent: 2.44; lr: 1.00000; 8734/9271 tok/s;     94 sec\n","[2022-02-27 20:11:59,194 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 5\n","[2022-02-27 20:11:59,341 INFO] Step 2500/10000; acc:  53.71; ppl: 12.75; xent: 2.55; lr: 1.00000; 8018/8755 tok/s;     95 sec\n","[2022-02-27 20:11:59,606 INFO] Validation perplexity: 8.55388\n","[2022-02-27 20:11:59,606 INFO] Validation accuracy: 61.4047\n","[2022-02-27 20:12:00,499 INFO] Step 2520/10000; acc:  49.28; ppl: 17.12; xent: 2.84; lr: 1.00000; 6737/6991 tok/s;     96 sec\n","[2022-02-27 20:12:01,198 INFO] Step 2540/10000; acc:  54.12; ppl: 12.83; xent: 2.55; lr: 1.00000; 8499/8944 tok/s;     97 sec\n","[2022-02-27 20:12:01,897 INFO] Step 2560/10000; acc:  53.75; ppl: 12.71; xent: 2.54; lr: 1.00000; 8558/8965 tok/s;     97 sec\n","[2022-02-27 20:12:02,767 INFO] Step 2580/10000; acc:  50.57; ppl: 16.05; xent: 2.78; lr: 1.00000; 8248/8412 tok/s;     98 sec\n","[2022-02-27 20:12:03,489 INFO] Step 2600/10000; acc:  53.72; ppl: 12.87; xent: 2.56; lr: 1.00000; 9186/9376 tok/s;     99 sec\n","[2022-02-27 20:12:04,168 INFO] Step 2620/10000; acc:  55.48; ppl: 12.34; xent: 2.51; lr: 1.00000; 8874/9254 tok/s;    100 sec\n","[2022-02-27 20:12:04,980 INFO] Step 2640/10000; acc:  52.10; ppl: 14.64; xent: 2.68; lr: 1.00000; 8026/8551 tok/s;    100 sec\n","[2022-02-27 20:12:05,740 INFO] Step 2660/10000; acc:  52.48; ppl: 13.47; xent: 2.60; lr: 1.00000; 9618/9706 tok/s;    101 sec\n","[2022-02-27 20:12:06,397 INFO] Step 2680/10000; acc:  57.25; ppl: 10.64; xent: 2.36; lr: 1.00000; 8569/9396 tok/s;    102 sec\n","[2022-02-27 20:12:07,222 INFO] Step 2700/10000; acc:  51.81; ppl: 14.67; xent: 2.69; lr: 1.00000; 8087/8192 tok/s;    103 sec\n","[2022-02-27 20:12:07,981 INFO] Step 2720/10000; acc:  52.78; ppl: 12.45; xent: 2.52; lr: 1.00000; 9206/9563 tok/s;    103 sec\n","[2022-02-27 20:12:08,619 INFO] Step 2740/10000; acc:  57.36; ppl: 10.34; xent: 2.34; lr: 1.00000; 8574/9234 tok/s;    104 sec\n","[2022-02-27 20:12:09,423 INFO] Step 2760/10000; acc:  50.94; ppl: 15.05; xent: 2.71; lr: 1.00000; 7975/8505 tok/s;    105 sec\n","[2022-02-27 20:12:10,158 INFO] Step 2780/10000; acc:  53.96; ppl: 12.24; xent: 2.50; lr: 1.00000; 9248/9675 tok/s;    106 sec\n","[2022-02-27 20:12:10,817 INFO] Step 2800/10000; acc:  57.36; ppl: 10.11; xent: 2.31; lr: 1.00000; 8641/9352 tok/s;    106 sec\n","[2022-02-27 20:12:11,520 INFO] Step 2820/10000; acc:  56.53; ppl: 10.87; xent: 2.39; lr: 1.00000; 8183/8724 tok/s;    107 sec\n","[2022-02-27 20:12:12,382 INFO] Step 2840/10000; acc:  48.56; ppl: 17.14; xent: 2.84; lr: 1.00000; 9244/9198 tok/s;    108 sec\n","[2022-02-27 20:12:13,060 INFO] Step 2860/10000; acc:  57.06; ppl:  9.79; xent: 2.28; lr: 1.00000; 8902/9245 tok/s;    108 sec\n","[2022-02-27 20:12:13,751 INFO] Step 2880/10000; acc:  56.39; ppl: 10.42; xent: 2.34; lr: 1.00000; 8808/8921 tok/s;    109 sec\n","[2022-02-27 20:12:14,604 INFO] Step 2900/10000; acc:  51.43; ppl: 14.88; xent: 2.70; lr: 1.00000; 8542/8499 tok/s;    110 sec\n","[2022-02-27 20:12:15,325 INFO] Step 2920/10000; acc:  53.83; ppl: 11.98; xent: 2.48; lr: 1.00000; 9153/9440 tok/s;    111 sec\n","[2022-02-27 20:12:15,999 INFO] Step 2940/10000; acc:  56.82; ppl: 10.44; xent: 2.35; lr: 1.00000; 8990/9100 tok/s;    111 sec\n","[2022-02-27 20:12:16,812 INFO] Step 2960/10000; acc:  53.29; ppl: 12.07; xent: 2.49; lr: 1.00000; 7897/8441 tok/s;    112 sec\n","[2022-02-27 20:12:17,632 INFO] Step 2980/10000; acc:  52.02; ppl: 13.26; xent: 2.59; lr: 1.00000; 8869/8919 tok/s;    113 sec\n","[2022-02-27 20:12:18,288 INFO] Step 3000/10000; acc:  58.00; ppl:  9.35; xent: 2.24; lr: 1.00000; 8517/9285 tok/s;    114 sec\n","[2022-02-27 20:12:18,348 INFO] Saving checkpoint ./Question10/model_step_3000.pt\n","[2022-02-27 20:12:19,340 INFO] Step 3020/10000; acc:  51.65; ppl: 14.75; xent: 2.69; lr: 1.00000; 6374/6499 tok/s;    115 sec\n","[2022-02-27 20:12:20,101 INFO] Step 3040/10000; acc:  53.38; ppl: 12.14; xent: 2.50; lr: 1.00000; 9314/9663 tok/s;    116 sec\n","[2022-02-27 20:12:20,740 INFO] Step 3060/10000; acc:  59.34; ppl:  8.39; xent: 2.13; lr: 1.00000; 8552/9387 tok/s;    116 sec\n","[2022-02-27 20:12:21,187 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 6\n","[2022-02-27 20:12:21,593 INFO] Step 3080/10000; acc:  52.52; ppl: 13.39; xent: 2.59; lr: 1.00000; 7547/8081 tok/s;    117 sec\n","[2022-02-27 20:12:22,334 INFO] Step 3100/10000; acc:  54.48; ppl: 11.36; xent: 2.43; lr: 1.00000; 9250/9363 tok/s;    118 sec\n","[2022-02-27 20:12:22,978 INFO] Step 3120/10000; acc:  57.77; ppl:  9.41; xent: 2.24; lr: 1.00000; 8825/9481 tok/s;    118 sec\n","[2022-02-27 20:12:23,405 INFO] Validation perplexity: 7.50106\n","[2022-02-27 20:12:23,406 INFO] Validation accuracy: 63.3751\n","[2022-02-27 20:12:23,916 INFO] Step 3140/10000; acc:  56.44; ppl: 10.43; xent: 2.34; lr: 1.00000; 6078/6547 tok/s;    119 sec\n","[2022-02-27 20:12:24,826 INFO] Step 3160/10000; acc:  51.80; ppl: 13.31; xent: 2.59; lr: 1.00000; 8452/8809 tok/s;    120 sec\n","[2022-02-27 20:12:25,521 INFO] Step 3180/10000; acc:  57.19; ppl: 10.16; xent: 2.32; lr: 1.00000; 8576/8851 tok/s;    121 sec\n","[2022-02-27 20:12:26,209 INFO] Step 3200/10000; acc:  56.53; ppl: 10.06; xent: 2.31; lr: 1.00000; 8645/9046 tok/s;    122 sec\n","[2022-02-27 20:12:27,120 INFO] Step 3220/10000; acc:  51.85; ppl: 14.52; xent: 2.68; lr: 1.00000; 8038/8253 tok/s;    123 sec\n","[2022-02-27 20:12:27,862 INFO] Step 3240/10000; acc:  55.24; ppl: 10.55; xent: 2.36; lr: 1.00000; 8974/9260 tok/s;    123 sec\n","[2022-02-27 20:12:28,532 INFO] Step 3260/10000; acc:  57.11; ppl: 10.28; xent: 2.33; lr: 1.00000; 9099/9236 tok/s;    124 sec\n","[2022-02-27 20:12:29,296 INFO] Step 3280/10000; acc:  54.27; ppl: 12.31; xent: 2.51; lr: 1.00000; 8359/9042 tok/s;    125 sec\n","[2022-02-27 20:12:30,085 INFO] Step 3300/10000; acc:  54.54; ppl: 11.03; xent: 2.40; lr: 1.00000; 9086/9189 tok/s;    126 sec\n","[2022-02-27 20:12:30,730 INFO] Step 3320/10000; acc:  59.74; ppl:  8.40; xent: 2.13; lr: 1.00000; 8571/9466 tok/s;    126 sec\n","[2022-02-27 20:12:31,567 INFO] Step 3340/10000; acc:  52.59; ppl: 12.80; xent: 2.55; lr: 1.00000; 7918/8256 tok/s;    127 sec\n","[2022-02-27 20:12:32,316 INFO] Step 3360/10000; acc:  54.84; ppl: 10.29; xent: 2.33; lr: 1.00000; 9383/9779 tok/s;    128 sec\n","[2022-02-27 20:12:32,948 INFO] Step 3380/10000; acc:  60.75; ppl:  8.09; xent: 2.09; lr: 1.00000; 8744/9246 tok/s;    128 sec\n","[2022-02-27 20:12:33,760 INFO] Step 3400/10000; acc:  54.46; ppl: 12.10; xent: 2.49; lr: 1.00000; 8029/8535 tok/s;    129 sec\n","[2022-02-27 20:12:34,507 INFO] Step 3420/10000; acc:  55.49; ppl: 10.34; xent: 2.34; lr: 1.00000; 9192/9727 tok/s;    130 sec\n","[2022-02-27 20:12:35,159 INFO] Step 3440/10000; acc:  59.67; ppl:  8.09; xent: 2.09; lr: 1.00000; 8848/9350 tok/s;    131 sec\n","[2022-02-27 20:12:35,872 INFO] Step 3460/10000; acc:  58.52; ppl:  9.05; xent: 2.20; lr: 1.00000; 8180/8274 tok/s;    131 sec\n","[2022-02-27 20:12:36,730 INFO] Step 3480/10000; acc:  51.77; ppl: 13.06; xent: 2.57; lr: 1.00000; 9042/9194 tok/s;    132 sec\n","[2022-02-27 20:12:37,413 INFO] Step 3500/10000; acc:  59.40; ppl:  8.28; xent: 2.11; lr: 1.00000; 8784/8998 tok/s;    133 sec\n","[2022-02-27 20:12:38,105 INFO] Step 3520/10000; acc:  57.80; ppl:  8.96; xent: 2.19; lr: 1.00000; 8704/8831 tok/s;    134 sec\n","[2022-02-27 20:12:38,973 INFO] Step 3540/10000; acc:  52.23; ppl: 13.13; xent: 2.57; lr: 1.00000; 8580/8571 tok/s;    134 sec\n","[2022-02-27 20:12:39,697 INFO] Step 3560/10000; acc:  56.41; ppl:  9.77; xent: 2.28; lr: 1.00000; 9262/9376 tok/s;    135 sec\n","[2022-02-27 20:12:40,362 INFO] Step 3580/10000; acc:  57.87; ppl:  9.07; xent: 2.20; lr: 1.00000; 9243/9419 tok/s;    136 sec\n","[2022-02-27 20:12:41,193 INFO] Step 3600/10000; acc:  55.37; ppl: 10.48; xent: 2.35; lr: 1.00000; 7645/8340 tok/s;    137 sec\n","[2022-02-27 20:12:41,987 INFO] Step 3620/10000; acc:  54.45; ppl: 10.98; xent: 2.40; lr: 1.00000; 9095/9260 tok/s;    137 sec\n","[2022-02-27 20:12:42,630 INFO] Step 3640/10000; acc:  60.65; ppl:  7.65; xent: 2.03; lr: 1.00000; 8652/9479 tok/s;    138 sec\n","[2022-02-27 20:12:43,443 INFO] Step 3660/10000; acc:  53.53; ppl: 12.38; xent: 2.52; lr: 1.00000; 8213/8362 tok/s;    139 sec\n","[2022-02-27 20:12:44,203 INFO] Step 3680/10000; acc:  55.69; ppl:  9.99; xent: 2.30; lr: 1.00000; 9282/9596 tok/s;    140 sec\n","[2022-02-27 20:12:44,820 INFO] Step 3700/10000; acc:  61.28; ppl:  7.42; xent: 2.00; lr: 1.00000; 8803/9424 tok/s;    140 sec\n","[2022-02-27 20:12:45,258 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 7\n","[2022-02-27 20:12:45,638 INFO] Step 3720/10000; acc:  54.81; ppl: 10.94; xent: 2.39; lr: 1.00000; 7771/8281 tok/s;    141 sec\n","[2022-02-27 20:12:46,376 INFO] Step 3740/10000; acc:  56.17; ppl: 10.29; xent: 2.33; lr: 1.00000; 9277/9616 tok/s;    142 sec\n","[2022-02-27 20:12:46,988 INFO] Validation perplexity: 6.69338\n","[2022-02-27 20:12:46,988 INFO] Validation accuracy: 64.905\n","[2022-02-27 20:12:47,280 INFO] Step 3760/10000; acc:  58.92; ppl:  8.14; xent: 2.10; lr: 1.00000; 6347/6866 tok/s;    143 sec\n","[2022-02-27 20:12:47,994 INFO] Step 3780/10000; acc:  57.98; ppl:  9.07; xent: 2.21; lr: 1.00000; 8051/8511 tok/s;    143 sec\n","[2022-02-27 20:12:48,906 INFO] Step 3800/10000; acc:  53.49; ppl: 11.46; xent: 2.44; lr: 1.00000; 8529/8852 tok/s;    144 sec\n","[2022-02-27 20:12:49,575 INFO] Step 3820/10000; acc:  59.99; ppl:  8.20; xent: 2.10; lr: 1.00000; 9016/9292 tok/s;    145 sec\n","[2022-02-27 20:12:50,269 INFO] Step 3840/10000; acc:  59.16; ppl:  8.37; xent: 2.12; lr: 1.00000; 8664/8997 tok/s;    146 sec\n","[2022-02-27 20:12:51,156 INFO] Step 3860/10000; acc:  53.00; ppl: 12.68; xent: 2.54; lr: 1.00000; 8284/8470 tok/s;    147 sec\n","[2022-02-27 20:12:51,897 INFO] Step 3880/10000; acc:  57.42; ppl:  8.74; xent: 2.17; lr: 1.00000; 8928/9281 tok/s;    147 sec\n","[2022-02-27 20:12:52,578 INFO] Step 3900/10000; acc:  58.02; ppl:  8.77; xent: 2.17; lr: 1.00000; 8931/9209 tok/s;    148 sec\n","[2022-02-27 20:12:53,365 INFO] Step 3920/10000; acc:  57.56; ppl:  9.58; xent: 2.26; lr: 1.00000; 7981/8574 tok/s;    149 sec\n","[2022-02-27 20:12:54,140 INFO] Step 3940/10000; acc:  56.15; ppl:  9.25; xent: 2.23; lr: 1.00000; 9055/9205 tok/s;    150 sec\n","[2022-02-27 20:12:54,778 INFO] Step 3960/10000; acc:  62.20; ppl:  6.83; xent: 1.92; lr: 1.00000; 8534/9327 tok/s;    150 sec\n","[2022-02-27 20:12:55,606 INFO] Step 3980/10000; acc:  55.15; ppl: 10.65; xent: 2.37; lr: 1.00000; 7880/8631 tok/s;    151 sec\n","[2022-02-27 20:12:56,363 INFO] Step 4000/10000; acc:  56.59; ppl:  8.99; xent: 2.20; lr: 1.00000; 9368/9770 tok/s;    152 sec\n","[2022-02-27 20:12:56,423 INFO] Saving checkpoint ./Question10/model_step_4000.pt\n","[2022-02-27 20:12:57,255 INFO] Step 4020/10000; acc:  62.12; ppl:  6.94; xent: 1.94; lr: 1.00000; 6219/6706 tok/s;    153 sec\n","[2022-02-27 20:12:58,066 INFO] Step 4040/10000; acc:  56.48; ppl: 10.48; xent: 2.35; lr: 1.00000; 8210/8224 tok/s;    153 sec\n","[2022-02-27 20:12:58,830 INFO] Step 4060/10000; acc:  56.56; ppl:  9.41; xent: 2.24; lr: 1.00000; 9090/9334 tok/s;    154 sec\n","[2022-02-27 20:12:59,486 INFO] Step 4080/10000; acc:  61.06; ppl:  6.85; xent: 1.92; lr: 1.00000; 8857/9548 tok/s;    155 sec\n","[2022-02-27 20:13:00,186 INFO] Step 4100/10000; acc:  60.71; ppl:  7.52; xent: 2.02; lr: 1.00000; 8393/8500 tok/s;    156 sec\n","[2022-02-27 20:13:01,060 INFO] Step 4120/10000; acc:  53.13; ppl: 11.50; xent: 2.44; lr: 1.00000; 9009/9092 tok/s;    156 sec\n","[2022-02-27 20:13:01,729 INFO] Step 4140/10000; acc:  59.56; ppl:  7.86; xent: 2.06; lr: 1.00000; 8980/9243 tok/s;    157 sec\n","[2022-02-27 20:13:02,475 INFO] Step 4160/10000; acc:  61.04; ppl:  7.48; xent: 2.01; lr: 1.00000; 8083/8224 tok/s;    158 sec\n","[2022-02-27 20:13:03,391 INFO] Step 4180/10000; acc:  53.08; ppl: 11.66; xent: 2.46; lr: 1.00000; 7973/7916 tok/s;    159 sec\n","[2022-02-27 20:13:04,119 INFO] Step 4200/10000; acc:  59.54; ppl:  7.88; xent: 2.06; lr: 1.00000; 9072/9369 tok/s;    160 sec\n","[2022-02-27 20:13:04,778 INFO] Step 4220/10000; acc:  59.64; ppl:  7.95; xent: 2.07; lr: 1.00000; 9195/9409 tok/s;    160 sec\n","[2022-02-27 20:13:05,601 INFO] Step 4240/10000; acc:  57.70; ppl:  8.77; xent: 2.17; lr: 1.00000; 7725/8509 tok/s;    161 sec\n","[2022-02-27 20:13:06,384 INFO] Step 4260/10000; acc:  55.67; ppl:  9.73; xent: 2.28; lr: 1.00000; 9202/9391 tok/s;    162 sec\n","[2022-02-27 20:13:07,038 INFO] Step 4280/10000; acc:  62.34; ppl:  6.34; xent: 1.85; lr: 1.00000; 8503/9290 tok/s;    162 sec\n","[2022-02-27 20:13:07,865 INFO] Step 4300/10000; acc:  56.79; ppl:  9.75; xent: 2.28; lr: 1.00000; 7959/8225 tok/s;    163 sec\n","[2022-02-27 20:13:08,638 INFO] Step 4320/10000; acc:  57.64; ppl:  8.47; xent: 2.14; lr: 1.00000; 9059/9452 tok/s;    164 sec\n","[2022-02-27 20:13:09,234 INFO] Step 4340/10000; acc:  62.79; ppl:  6.48; xent: 1.87; lr: 1.00000; 9126/9528 tok/s;    165 sec\n","[2022-02-27 20:13:09,668 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 8\n","[2022-02-27 20:13:10,077 INFO] Step 4360/10000; acc:  55.06; ppl: 10.34; xent: 2.34; lr: 1.00000; 7665/8130 tok/s;    166 sec\n","[2022-02-27 20:13:10,919 INFO] Validation perplexity: 6.32998\n","[2022-02-27 20:13:10,919 INFO] Validation accuracy: 66.5276\n","[2022-02-27 20:13:11,088 INFO] Step 4380/10000; acc:  57.35; ppl:  8.89; xent: 2.19; lr: 1.00000; 6814/7095 tok/s;    167 sec\n","[2022-02-27 20:13:11,782 INFO] Step 4400/10000; acc:  61.12; ppl:  6.75; xent: 1.91; lr: 1.00000; 8310/9029 tok/s;    167 sec\n","[2022-02-27 20:13:12,495 INFO] Step 4420/10000; acc:  60.35; ppl:  7.76; xent: 2.05; lr: 1.00000; 8155/8412 tok/s;    168 sec\n","[2022-02-27 20:13:13,415 INFO] Step 4440/10000; acc:  53.53; ppl: 11.05; xent: 2.40; lr: 1.00000; 8511/8833 tok/s;    169 sec\n","[2022-02-27 20:13:14,091 INFO] Step 4460/10000; acc:  61.30; ppl:  6.88; xent: 1.93; lr: 1.00000; 8960/9281 tok/s;    170 sec\n","[2022-02-27 20:13:14,760 INFO] Step 4480/10000; acc:  59.58; ppl:  7.69; xent: 2.04; lr: 1.00000; 9058/9423 tok/s;    170 sec\n","[2022-02-27 20:13:15,621 INFO] Step 4500/10000; acc:  54.24; ppl: 11.22; xent: 2.42; lr: 1.00000; 8527/8827 tok/s;    171 sec\n","[2022-02-27 20:13:16,361 INFO] Step 4520/10000; acc:  58.89; ppl:  7.75; xent: 2.05; lr: 1.00000; 8931/9286 tok/s;    172 sec\n","[2022-02-27 20:13:17,047 INFO] Step 4540/10000; acc:  61.07; ppl:  7.30; xent: 1.99; lr: 1.00000; 8908/9113 tok/s;    172 sec\n","[2022-02-27 20:13:17,839 INFO] Step 4560/10000; acc:  59.91; ppl:  7.78; xent: 2.05; lr: 1.00000; 7830/8477 tok/s;    173 sec\n","[2022-02-27 20:13:18,612 INFO] Step 4580/10000; acc:  58.68; ppl:  7.89; xent: 2.07; lr: 1.00000; 8988/9208 tok/s;    174 sec\n","[2022-02-27 20:13:19,248 INFO] Step 4600/10000; acc:  64.33; ppl:  6.00; xent: 1.79; lr: 1.00000; 8566/9287 tok/s;    175 sec\n","[2022-02-27 20:13:20,058 INFO] Step 4620/10000; acc:  56.78; ppl:  9.21; xent: 2.22; lr: 1.00000; 8091/8642 tok/s;    175 sec\n","[2022-02-27 20:13:20,807 INFO] Step 4640/10000; acc:  58.91; ppl:  7.73; xent: 2.04; lr: 1.00000; 9377/9856 tok/s;    176 sec\n","[2022-02-27 20:13:21,455 INFO] Step 4660/10000; acc:  64.37; ppl:  5.96; xent: 1.79; lr: 1.00000; 8468/8950 tok/s;    177 sec\n","[2022-02-27 20:13:22,261 INFO] Step 4680/10000; acc:  57.67; ppl:  9.33; xent: 2.23; lr: 1.00000; 8248/8456 tok/s;    178 sec\n","[2022-02-27 20:13:23,038 INFO] Step 4700/10000; acc:  57.98; ppl:  8.36; xent: 2.12; lr: 1.00000; 9061/9266 tok/s;    178 sec\n","[2022-02-27 20:13:23,706 INFO] Step 4720/10000; acc:  62.94; ppl:  6.19; xent: 1.82; lr: 1.00000; 8906/9260 tok/s;    179 sec\n","[2022-02-27 20:13:24,415 INFO] Step 4740/10000; acc:  60.77; ppl:  7.23; xent: 1.98; lr: 1.00000; 8384/8541 tok/s;    180 sec\n","[2022-02-27 20:13:25,271 INFO] Step 4760/10000; acc:  54.40; ppl: 10.27; xent: 2.33; lr: 1.00000; 9157/9277 tok/s;    181 sec\n","[2022-02-27 20:13:25,926 INFO] Step 4780/10000; acc:  61.81; ppl:  6.50; xent: 1.87; lr: 1.00000; 9169/9355 tok/s;    181 sec\n","[2022-02-27 20:13:26,617 INFO] Step 4800/10000; acc:  62.37; ppl:  6.44; xent: 1.86; lr: 1.00000; 8686/8936 tok/s;    182 sec\n","[2022-02-27 20:13:27,496 INFO] Step 4820/10000; acc:  54.25; ppl: 10.10; xent: 2.31; lr: 1.00000; 8300/8387 tok/s;    183 sec\n","[2022-02-27 20:13:28,237 INFO] Step 4840/10000; acc:  60.13; ppl:  7.33; xent: 1.99; lr: 1.00000; 8966/9252 tok/s;    184 sec\n","[2022-02-27 20:13:28,897 INFO] Step 4860/10000; acc:  60.60; ppl:  7.11; xent: 1.96; lr: 1.00000; 9214/9572 tok/s;    184 sec\n","[2022-02-27 20:13:29,694 INFO] Step 4880/10000; acc:  60.23; ppl:  7.75; xent: 2.05; lr: 1.00000; 7902/8515 tok/s;    185 sec\n","[2022-02-27 20:13:30,448 INFO] Step 4900/10000; acc:  57.94; ppl:  7.84; xent: 2.06; lr: 1.00000; 9475/9503 tok/s;    186 sec\n","[2022-02-27 20:13:31,078 INFO] Step 4920/10000; acc:  63.93; ppl:  5.86; xent: 1.77; lr: 1.00000; 8667/9449 tok/s;    187 sec\n","[2022-02-27 20:13:31,924 INFO] Step 4940/10000; acc:  57.75; ppl:  8.22; xent: 2.11; lr: 1.00000; 7780/8270 tok/s;    187 sec\n","[2022-02-27 20:13:32,686 INFO] Step 4960/10000; acc:  59.36; ppl:  7.50; xent: 2.02; lr: 1.00000; 9211/9560 tok/s;    188 sec\n","[2022-02-27 20:13:33,313 INFO] Step 4980/10000; acc:  63.97; ppl:  5.77; xent: 1.75; lr: 1.00000; 8683/9325 tok/s;    189 sec\n","[2022-02-27 20:13:33,746 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 9\n","[2022-02-27 20:13:34,178 INFO] Step 5000/10000; acc:  56.90; ppl:  9.11; xent: 2.21; lr: 1.00000; 7568/8007 tok/s;    190 sec\n","[2022-02-27 20:13:34,430 INFO] Validation perplexity: 6.02782\n","[2022-02-27 20:13:34,431 INFO] Validation accuracy: 67.3157\n","[2022-02-27 20:13:34,488 INFO] Saving checkpoint ./Question10/model_step_5000.pt\n","[2022-02-27 20:13:35,442 INFO] Step 5020/10000; acc:  58.36; ppl:  7.85; xent: 2.06; lr: 1.00000; 5473/5686 tok/s;    191 sec\n","[2022-02-27 20:13:36,129 INFO] Step 5040/10000; acc:  62.35; ppl:  6.41; xent: 1.86; lr: 1.00000; 8413/9090 tok/s;    192 sec\n","[2022-02-27 20:13:36,826 INFO] Step 5060/10000; acc:  61.86; ppl:  6.55; xent: 1.88; lr: 1.00000; 8383/8684 tok/s;    192 sec\n","[2022-02-27 20:13:37,697 INFO] Step 5080/10000; acc:  55.01; ppl: 10.08; xent: 2.31; lr: 1.00000; 8996/9268 tok/s;    193 sec\n","[2022-02-27 20:13:38,384 INFO] Step 5100/10000; acc:  62.91; ppl:  6.24; xent: 1.83; lr: 1.00000; 8895/9157 tok/s;    194 sec\n","[2022-02-27 20:13:39,089 INFO] Step 5120/10000; acc:  61.20; ppl:  6.89; xent: 1.93; lr: 1.00000; 8674/8925 tok/s;    195 sec\n","[2022-02-27 20:13:39,966 INFO] Step 5140/10000; acc:  56.74; ppl:  9.25; xent: 2.22; lr: 1.00000; 8139/8584 tok/s;    195 sec\n","[2022-02-27 20:13:40,683 INFO] Step 5160/10000; acc:  62.01; ppl:  6.52; xent: 1.88; lr: 1.00000; 9118/9396 tok/s;    196 sec\n","[2022-02-27 20:13:41,356 INFO] Step 5180/10000; acc:  63.24; ppl:  6.08; xent: 1.81; lr: 1.00000; 8879/9376 tok/s;    197 sec\n","[2022-02-27 20:13:42,178 INFO] Step 5200/10000; acc:  59.32; ppl:  7.61; xent: 2.03; lr: 1.00000; 7641/8107 tok/s;    198 sec\n","[2022-02-27 20:13:42,974 INFO] Step 5220/10000; acc:  59.97; ppl:  7.12; xent: 1.96; lr: 1.00000; 8807/8943 tok/s;    198 sec\n","[2022-02-27 20:13:43,591 INFO] Step 5240/10000; acc:  65.01; ppl:  5.40; xent: 1.69; lr: 1.00000; 8867/9576 tok/s;    199 sec\n","[2022-02-27 20:13:44,412 INFO] Step 5260/10000; acc:  58.28; ppl:  8.37; xent: 2.12; lr: 1.00000; 8034/8532 tok/s;    200 sec\n","[2022-02-27 20:13:45,169 INFO] Step 5280/10000; acc:  60.14; ppl:  7.12; xent: 1.96; lr: 1.00000; 9341/9783 tok/s;    201 sec\n","[2022-02-27 20:13:45,826 INFO] Step 5300/10000; acc:  66.08; ppl:  5.26; xent: 1.66; lr: 1.00000; 8396/8948 tok/s;    201 sec\n","[2022-02-27 20:13:46,635 INFO] Step 5320/10000; acc:  58.43; ppl:  8.05; xent: 2.09; lr: 1.00000; 8235/8500 tok/s;    202 sec\n","[2022-02-27 20:13:47,415 INFO] Step 5340/10000; acc:  59.62; ppl:  7.41; xent: 2.00; lr: 1.00000; 8994/9148 tok/s;    203 sec\n","[2022-02-27 20:13:48,082 INFO] Step 5360/10000; acc:  64.47; ppl:  5.34; xent: 1.68; lr: 1.00000; 8839/9280 tok/s;    204 sec\n","[2022-02-27 20:13:48,767 INFO] Step 5380/10000; acc:  62.18; ppl:  6.25; xent: 1.83; lr: 1.00000; 8640/8705 tok/s;    204 sec\n","[2022-02-27 20:13:49,613 INFO] Step 5400/10000; acc:  55.14; ppl:  9.80; xent: 2.28; lr: 1.00000; 9426/9362 tok/s;    205 sec\n","[2022-02-27 20:13:50,279 INFO] Step 5420/10000; acc:  63.22; ppl:  5.69; xent: 1.74; lr: 1.00000; 9113/9384 tok/s;    206 sec\n","[2022-02-27 20:13:50,977 INFO] Step 5440/10000; acc:  62.53; ppl:  6.00; xent: 1.79; lr: 1.00000; 8743/8932 tok/s;    206 sec\n","[2022-02-27 20:13:51,843 INFO] Step 5460/10000; acc:  57.60; ppl:  7.99; xent: 2.08; lr: 1.00000; 8139/8536 tok/s;    207 sec\n","[2022-02-27 20:13:52,606 INFO] Step 5480/10000; acc:  61.11; ppl:  6.47; xent: 1.87; lr: 1.00000; 8528/8762 tok/s;    208 sec\n","[2022-02-27 20:13:53,274 INFO] Step 5500/10000; acc:  61.74; ppl:  6.18; xent: 1.82; lr: 1.00000; 8871/9481 tok/s;    209 sec\n","[2022-02-27 20:13:54,040 INFO] Step 5520/10000; acc:  60.29; ppl:  7.16; xent: 1.97; lr: 1.00000; 8255/8766 tok/s;    209 sec\n","[2022-02-27 20:13:54,807 INFO] Step 5540/10000; acc:  60.46; ppl:  6.82; xent: 1.92; lr: 1.00000; 9256/9363 tok/s;    210 sec\n","[2022-02-27 20:13:55,448 INFO] Step 5560/10000; acc:  65.47; ppl:  4.99; xent: 1.61; lr: 1.00000; 8562/9353 tok/s;    211 sec\n","[2022-02-27 20:13:55,774 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 10\n","[2022-02-27 20:13:56,315 INFO] Step 5580/10000; acc:  58.12; ppl:  8.33; xent: 2.12; lr: 1.00000; 7629/8147 tok/s;    212 sec\n","[2022-02-27 20:13:57,082 INFO] Step 5600/10000; acc:  60.88; ppl:  6.65; xent: 1.89; lr: 1.00000; 9307/9517 tok/s;    213 sec\n","[2022-02-27 20:13:57,723 INFO] Step 5620/10000; acc:  64.50; ppl:  5.38; xent: 1.68; lr: 1.00000; 8565/9213 tok/s;    213 sec\n","[2022-02-27 20:13:58,118 INFO] Validation perplexity: 5.88223\n","[2022-02-27 20:13:58,118 INFO] Validation accuracy: 67.5707\n","[2022-02-27 20:13:58,802 INFO] Step 5640/10000; acc:  58.43; ppl:  8.13; xent: 2.10; lr: 1.00000; 6054/6404 tok/s;    214 sec\n","[2022-02-27 20:13:59,570 INFO] Step 5660/10000; acc:  59.72; ppl:  6.93; xent: 1.94; lr: 1.00000; 8930/9225 tok/s;    215 sec\n","[2022-02-27 20:14:00,236 INFO] Step 5680/10000; acc:  63.78; ppl:  5.48; xent: 1.70; lr: 1.00000; 8698/9338 tok/s;    216 sec\n","[2022-02-27 20:14:00,923 INFO] Step 5700/10000; acc:  63.52; ppl:  5.85; xent: 1.77; lr: 1.00000; 8470/8675 tok/s;    216 sec\n","[2022-02-27 20:14:01,794 INFO] Step 5720/10000; acc:  55.23; ppl:  9.50; xent: 2.25; lr: 1.00000; 9194/9509 tok/s;    217 sec\n","[2022-02-27 20:14:02,491 INFO] Step 5740/10000; acc:  63.57; ppl:  5.61; xent: 1.72; lr: 1.00000; 8812/8990 tok/s;    218 sec\n","[2022-02-27 20:14:03,183 INFO] Step 5760/10000; acc:  62.22; ppl:  6.30; xent: 1.84; lr: 1.00000; 8883/9040 tok/s;    219 sec\n","[2022-02-27 20:14:04,006 INFO] Step 5780/10000; acc:  57.87; ppl:  8.06; xent: 2.09; lr: 1.00000; 8503/9064 tok/s;    219 sec\n","[2022-02-27 20:14:04,740 INFO] Step 5800/10000; acc:  63.17; ppl:  5.88; xent: 1.77; lr: 1.00000; 8742/9010 tok/s;    220 sec\n","[2022-02-27 20:14:05,422 INFO] Step 5820/10000; acc:  63.28; ppl:  5.69; xent: 1.74; lr: 1.00000; 8588/9284 tok/s;    221 sec\n","[2022-02-27 20:14:06,244 INFO] Step 5840/10000; acc:  60.94; ppl:  6.60; xent: 1.89; lr: 1.00000; 7632/8242 tok/s;    222 sec\n","[2022-02-27 20:14:06,992 INFO] Step 5860/10000; acc:  60.36; ppl:  6.72; xent: 1.91; lr: 1.00000; 9443/9480 tok/s;    222 sec\n","[2022-02-27 20:14:07,616 INFO] Step 5880/10000; acc:  66.28; ppl:  4.84; xent: 1.58; lr: 1.00000; 8801/9511 tok/s;    223 sec\n","[2022-02-27 20:14:08,453 INFO] Step 5900/10000; acc:  59.53; ppl:  7.52; xent: 2.02; lr: 1.00000; 8003/8539 tok/s;    224 sec\n","[2022-02-27 20:14:09,221 INFO] Step 5920/10000; acc:  60.55; ppl:  6.70; xent: 1.90; lr: 1.00000; 9330/9679 tok/s;    225 sec\n","[2022-02-27 20:14:09,867 INFO] Step 5940/10000; acc:  66.10; ppl:  4.79; xent: 1.57; lr: 1.00000; 8600/9309 tok/s;    225 sec\n","[2022-02-27 20:14:10,683 INFO] Step 5960/10000; acc:  60.13; ppl:  7.28; xent: 1.99; lr: 1.00000; 8130/7984 tok/s;    226 sec\n","[2022-02-27 20:14:11,452 INFO] Step 5980/10000; acc:  60.13; ppl:  6.62; xent: 1.89; lr: 1.00000; 9025/9322 tok/s;    227 sec\n","[2022-02-27 20:14:12,109 INFO] Step 6000/10000; acc:  66.39; ppl:  4.85; xent: 1.58; lr: 1.00000; 8930/9243 tok/s;    228 sec\n","[2022-02-27 20:14:12,167 INFO] Saving checkpoint ./Question10/model_step_6000.pt\n","[2022-02-27 20:14:13,048 INFO] Step 6020/10000; acc:  65.15; ppl:  5.19; xent: 1.65; lr: 1.00000; 6252/6308 tok/s;    228 sec\n","[2022-02-27 20:14:13,908 INFO] Step 6040/10000; acc:  55.81; ppl:  9.23; xent: 2.22; lr: 1.00000; 9452/9319 tok/s;    229 sec\n","[2022-02-27 20:14:14,592 INFO] Step 6060/10000; acc:  64.03; ppl:  5.33; xent: 1.67; lr: 1.00000; 8980/9266 tok/s;    230 sec\n","[2022-02-27 20:14:15,303 INFO] Step 6080/10000; acc:  62.89; ppl:  5.94; xent: 1.78; lr: 1.00000; 8706/8899 tok/s;    231 sec\n","[2022-02-27 20:14:16,168 INFO] Step 6100/10000; acc:  59.22; ppl:  7.11; xent: 1.96; lr: 1.00000; 8076/8620 tok/s;    232 sec\n","[2022-02-27 20:14:16,897 INFO] Step 6120/10000; acc:  62.91; ppl:  5.73; xent: 1.75; lr: 1.00000; 8866/9170 tok/s;    232 sec\n","[2022-02-27 20:14:17,554 INFO] Step 6140/10000; acc:  63.04; ppl:  5.72; xent: 1.74; lr: 1.00000; 8991/9593 tok/s;    233 sec\n","[2022-02-27 20:14:18,360 INFO] Step 6160/10000; acc:  62.48; ppl:  6.44; xent: 1.86; lr: 1.00000; 7824/8263 tok/s;    234 sec\n","[2022-02-27 20:14:19,141 INFO] Step 6180/10000; acc:  60.99; ppl:  6.44; xent: 1.86; lr: 1.00000; 9033/9238 tok/s;    235 sec\n","[2022-02-27 20:14:19,761 INFO] Step 6200/10000; acc:  67.48; ppl:  4.36; xent: 1.47; lr: 1.00000; 8763/9446 tok/s;    235 sec\n","[2022-02-27 20:14:20,078 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 11\n","[2022-02-27 20:14:20,604 INFO] Step 6220/10000; acc:  58.64; ppl:  7.55; xent: 2.02; lr: 1.00000; 7778/8226 tok/s;    236 sec\n","[2022-02-27 20:14:21,358 INFO] Step 6240/10000; acc:  61.78; ppl:  6.20; xent: 1.82; lr: 1.00000; 9489/9729 tok/s;    237 sec\n","[2022-02-27 20:14:21,919 INFO] Validation perplexity: 5.77573\n","[2022-02-27 20:14:21,919 INFO] Validation accuracy: 67.7561\n","[2022-02-27 20:14:22,310 INFO] Step 6260/10000; acc:  65.77; ppl:  4.82; xent: 1.57; lr: 1.00000; 5791/6352 tok/s;    238 sec\n","[2022-02-27 20:14:23,180 INFO] Step 6280/10000; acc:  59.40; ppl:  7.56; xent: 2.02; lr: 1.00000; 7572/7939 tok/s;    239 sec\n","[2022-02-27 20:14:23,949 INFO] Step 6300/10000; acc:  59.94; ppl:  6.59; xent: 1.88; lr: 1.00000; 9042/9263 tok/s;    239 sec\n","[2022-02-27 20:14:24,607 INFO] Step 6320/10000; acc:  66.57; ppl:  4.62; xent: 1.53; lr: 1.00000; 8908/9364 tok/s;    240 sec\n","[2022-02-27 20:14:25,305 INFO] Step 6340/10000; acc:  64.30; ppl:  5.36; xent: 1.68; lr: 1.00000; 8413/8704 tok/s;    241 sec\n","[2022-02-27 20:14:26,167 INFO] Step 6360/10000; acc:  55.62; ppl:  9.20; xent: 2.22; lr: 1.00000; 9299/9609 tok/s;    242 sec\n","[2022-02-27 20:14:26,858 INFO] Step 6380/10000; acc:  64.64; ppl:  5.15; xent: 1.64; lr: 1.00000; 8841/9237 tok/s;    242 sec\n","[2022-02-27 20:14:27,557 INFO] Step 6400/10000; acc:  63.18; ppl:  5.73; xent: 1.75; lr: 1.00000; 8810/8895 tok/s;    243 sec\n","[2022-02-27 20:14:28,406 INFO] Step 6420/10000; acc:  62.56; ppl:  6.26; xent: 1.83; lr: 1.00000; 8060/8458 tok/s;    244 sec\n","[2022-02-27 20:14:29,132 INFO] Step 6440/10000; acc:  62.51; ppl:  5.67; xent: 1.74; lr: 1.00000; 8669/9006 tok/s;    245 sec\n","[2022-02-27 20:14:29,788 INFO] Step 6460/10000; acc:  64.94; ppl:  5.06; xent: 1.62; lr: 1.00000; 8704/9615 tok/s;    245 sec\n","[2022-02-27 20:14:30,605 INFO] Step 6480/10000; acc:  62.49; ppl:  6.06; xent: 1.80; lr: 1.00000; 7640/8492 tok/s;    246 sec\n","[2022-02-27 20:14:31,366 INFO] Step 6500/10000; acc:  60.70; ppl:  6.39; xent: 1.85; lr: 1.00000; 9357/9470 tok/s;    247 sec\n","[2022-02-27 20:14:32,015 INFO] Step 6520/10000; acc:  67.36; ppl:  4.34; xent: 1.47; lr: 1.00000; 8540/9262 tok/s;    247 sec\n","[2022-02-27 20:14:32,855 INFO] Step 6540/10000; acc:  60.90; ppl:  6.94; xent: 1.94; lr: 1.00000; 8119/8215 tok/s;    248 sec\n","[2022-02-27 20:14:33,602 INFO] Step 6560/10000; acc:  59.90; ppl:  6.73; xent: 1.91; lr: 1.00000; 9729/9654 tok/s;    249 sec\n","[2022-02-27 20:14:34,270 INFO] Step 6580/10000; acc:  68.16; ppl:  4.17; xent: 1.43; lr: 1.00000; 8359/9349 tok/s;    250 sec\n","[2022-02-27 20:14:35,062 INFO] Step 6600/10000; acc:  59.66; ppl:  7.06; xent: 1.95; lr: 1.00000; 8474/8488 tok/s;    250 sec\n","[2022-02-27 20:14:35,822 INFO] Step 6620/10000; acc:  60.71; ppl:  6.27; xent: 1.84; lr: 1.00000; 9208/9374 tok/s;    251 sec\n","[2022-02-27 20:14:36,481 INFO] Step 6640/10000; acc:  67.27; ppl:  4.48; xent: 1.50; lr: 1.00000; 8892/9176 tok/s;    252 sec\n","[2022-02-27 20:14:37,182 INFO] Step 6660/10000; acc:  64.52; ppl:  5.14; xent: 1.64; lr: 1.00000; 8378/8448 tok/s;    253 sec\n","[2022-02-27 20:14:38,059 INFO] Step 6680/10000; acc:  56.58; ppl:  8.37; xent: 2.12; lr: 1.00000; 9077/9037 tok/s;    253 sec\n","[2022-02-27 20:14:38,742 INFO] Step 6700/10000; acc:  66.79; ppl:  4.53; xent: 1.51; lr: 1.00000; 8883/9154 tok/s;    254 sec\n","[2022-02-27 20:14:39,434 INFO] Step 6720/10000; acc:  63.34; ppl:  5.49; xent: 1.70; lr: 1.00000; 8818/9127 tok/s;    255 sec\n","[2022-02-27 20:14:40,302 INFO] Step 6740/10000; acc:  58.92; ppl:  7.15; xent: 1.97; lr: 1.00000; 8066/8769 tok/s;    256 sec\n","[2022-02-27 20:14:41,035 INFO] Step 6760/10000; acc:  64.32; ppl:  5.15; xent: 1.64; lr: 1.00000; 8826/9159 tok/s;    256 sec\n","[2022-02-27 20:14:41,704 INFO] Step 6780/10000; acc:  64.97; ppl:  5.04; xent: 1.62; lr: 1.00000; 8826/9374 tok/s;    257 sec\n","[2022-02-27 20:14:42,510 INFO] Step 6800/10000; acc:  63.56; ppl:  5.73; xent: 1.75; lr: 1.00000; 7727/8239 tok/s;    258 sec\n","[2022-02-27 20:14:43,282 INFO] Step 6820/10000; acc:  62.32; ppl:  5.63; xent: 1.73; lr: 1.00000; 9097/9318 tok/s;    259 sec\n","[2022-02-27 20:14:43,882 INFO] Step 6840/10000; acc:  67.67; ppl:  4.18; xent: 1.43; lr: 1.00000; 9046/9633 tok/s;    259 sec\n","[2022-02-27 20:14:44,192 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 12\n","[2022-02-27 20:14:44,750 INFO] Step 6860/10000; acc:  60.39; ppl:  7.13; xent: 1.96; lr: 1.00000; 7685/8094 tok/s;    260 sec\n","[2022-02-27 20:14:45,586 INFO] Validation perplexity: 5.80286\n","[2022-02-27 20:14:45,587 INFO] Validation accuracy: 67.5707\n","[2022-02-27 20:14:45,771 INFO] Step 6880/10000; acc:  61.31; ppl:  6.04; xent: 1.80; lr: 1.00000; 7047/7218 tok/s;    261 sec\n","[2022-02-27 20:14:46,447 INFO] Step 6900/10000; acc:  66.13; ppl:  4.68; xent: 1.54; lr: 1.00000; 8201/8973 tok/s;    262 sec\n","[2022-02-27 20:14:47,286 INFO] Step 6920/10000; acc:  60.52; ppl:  6.74; xent: 1.91; lr: 1.00000; 7863/8152 tok/s;    263 sec\n","[2022-02-27 20:14:48,064 INFO] Step 6940/10000; acc:  60.87; ppl:  6.40; xent: 1.86; lr: 1.00000; 8984/9236 tok/s;    263 sec\n","[2022-02-27 20:14:48,715 INFO] Step 6960/10000; acc:  67.61; ppl:  4.34; xent: 1.47; lr: 1.00000; 9058/9489 tok/s;    264 sec\n","[2022-02-27 20:14:49,405 INFO] Step 6980/10000; acc:  64.68; ppl:  5.03; xent: 1.62; lr: 1.00000; 8534/8971 tok/s;    265 sec\n","[2022-02-27 20:14:50,262 INFO] Step 7000/10000; acc:  56.87; ppl:  8.28; xent: 2.11; lr: 1.00000; 9378/9668 tok/s;    266 sec\n","[2022-02-27 20:14:50,322 INFO] Saving checkpoint ./Question10/model_step_7000.pt\n","[2022-02-27 20:14:51,210 INFO] Step 7020/10000; acc:  65.32; ppl:  4.79; xent: 1.57; lr: 1.00000; 6427/6739 tok/s;    267 sec\n","[2022-02-27 20:14:51,908 INFO] Step 7040/10000; acc:  64.70; ppl:  5.05; xent: 1.62; lr: 1.00000; 8865/8935 tok/s;    267 sec\n","[2022-02-27 20:14:52,748 INFO] Step 7060/10000; acc:  62.49; ppl:  5.89; xent: 1.77; lr: 1.00000; 7977/8515 tok/s;    268 sec\n","[2022-02-27 20:14:53,462 INFO] Step 7080/10000; acc:  65.23; ppl:  4.84; xent: 1.58; lr: 1.00000; 8778/9141 tok/s;    269 sec\n","[2022-02-27 20:14:54,109 INFO] Step 7100/10000; acc:  65.70; ppl:  4.72; xent: 1.55; lr: 1.00000; 8805/9526 tok/s;    270 sec\n","[2022-02-27 20:14:54,922 INFO] Step 7120/10000; acc:  63.59; ppl:  5.69; xent: 1.74; lr: 1.00000; 7714/8507 tok/s;    270 sec\n","[2022-02-27 20:14:55,689 INFO] Step 7140/10000; acc:  62.02; ppl:  5.82; xent: 1.76; lr: 1.00000; 9236/9415 tok/s;    271 sec\n","[2022-02-27 20:14:56,345 INFO] Step 7160/10000; acc:  68.85; ppl:  3.94; xent: 1.37; lr: 1.00000; 8370/9013 tok/s;    272 sec\n","[2022-02-27 20:14:57,168 INFO] Step 7180/10000; acc:  60.65; ppl:  6.64; xent: 1.89; lr: 1.00000; 8355/8398 tok/s;    273 sec\n","[2022-02-27 20:14:57,971 INFO] Step 7200/10000; acc:  60.52; ppl:  6.39; xent: 1.85; lr: 1.00000; 9181/9146 tok/s;    273 sec\n","[2022-02-27 20:14:58,644 INFO] Step 7220/10000; acc:  67.89; ppl:  4.08; xent: 1.41; lr: 1.00000; 8502/9047 tok/s;    274 sec\n","[2022-02-27 20:14:59,436 INFO] Step 7240/10000; acc:  60.61; ppl:  6.63; xent: 1.89; lr: 1.00000; 8452/8666 tok/s;    275 sec\n","[2022-02-27 20:15:00,192 INFO] Step 7260/10000; acc:  60.91; ppl:  5.81; xent: 1.76; lr: 1.00000; 9232/9396 tok/s;    276 sec\n","[2022-02-27 20:15:00,833 INFO] Step 7280/10000; acc:  67.71; ppl:  4.21; xent: 1.44; lr: 1.00000; 9138/9351 tok/s;    276 sec\n","[2022-02-27 20:15:01,532 INFO] Step 7300/10000; acc:  66.85; ppl:  4.45; xent: 1.49; lr: 1.00000; 8379/8518 tok/s;    277 sec\n","[2022-02-27 20:15:02,419 INFO] Step 7320/10000; acc:  57.14; ppl:  7.84; xent: 2.06; lr: 1.00000; 9004/9152 tok/s;    278 sec\n","[2022-02-27 20:15:03,113 INFO] Step 7340/10000; acc:  65.99; ppl:  4.55; xent: 1.52; lr: 1.00000; 8786/9109 tok/s;    279 sec\n","[2022-02-27 20:15:03,807 INFO] Step 7360/10000; acc:  64.87; ppl:  5.06; xent: 1.62; lr: 1.00000; 8835/9139 tok/s;    279 sec\n","[2022-02-27 20:15:04,648 INFO] Step 7380/10000; acc:  61.00; ppl:  6.39; xent: 1.85; lr: 1.00000; 8212/8667 tok/s;    280 sec\n","[2022-02-27 20:15:05,347 INFO] Step 7400/10000; acc:  64.76; ppl:  4.84; xent: 1.58; lr: 1.00000; 9124/9322 tok/s;    281 sec\n","[2022-02-27 20:15:06,022 INFO] Step 7420/10000; acc:  66.56; ppl:  4.53; xent: 1.51; lr: 1.00000; 8590/9334 tok/s;    281 sec\n","[2022-02-27 20:15:06,845 INFO] Step 7440/10000; acc:  64.96; ppl:  5.15; xent: 1.64; lr: 1.00000; 7572/8357 tok/s;    282 sec\n","[2022-02-27 20:15:07,608 INFO] Step 7460/10000; acc:  63.34; ppl:  5.47; xent: 1.70; lr: 1.00000; 9227/9335 tok/s;    283 sec\n","[2022-02-27 20:15:08,224 INFO] Step 7480/10000; acc:  68.65; ppl:  3.91; xent: 1.36; lr: 1.00000; 8866/9549 tok/s;    284 sec\n","[2022-02-27 20:15:08,533 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 13\n","[2022-02-27 20:15:09,154 INFO] Step 7500/10000; acc:  60.37; ppl:  6.63; xent: 1.89; lr: 1.00000; 7244/7668 tok/s;    285 sec\n","[2022-02-27 20:15:09,405 INFO] Validation perplexity: 5.60949\n","[2022-02-27 20:15:09,406 INFO] Validation accuracy: 68.1038\n","[2022-02-27 20:15:10,173 INFO] Step 7520/10000; acc:  61.99; ppl:  5.76; xent: 1.75; lr: 1.00000; 7065/7306 tok/s;    286 sec\n","[2022-02-27 20:15:10,854 INFO] Step 7540/10000; acc:  67.22; ppl:  4.30; xent: 1.46; lr: 1.00000; 8167/8729 tok/s;    286 sec\n","[2022-02-27 20:15:11,636 INFO] Step 7560/10000; acc:  61.24; ppl:  6.48; xent: 1.87; lr: 1.00000; 8495/8614 tok/s;    287 sec\n","[2022-02-27 20:15:12,414 INFO] Step 7580/10000; acc:  61.79; ppl:  6.02; xent: 1.80; lr: 1.00000; 9012/9483 tok/s;    288 sec\n","[2022-02-27 20:15:13,084 INFO] Step 7600/10000; acc:  68.66; ppl:  3.98; xent: 1.38; lr: 1.00000; 8923/9150 tok/s;    289 sec\n","[2022-02-27 20:15:13,789 INFO] Step 7620/10000; acc:  66.10; ppl:  4.63; xent: 1.53; lr: 1.00000; 8427/8681 tok/s;    289 sec\n","[2022-02-27 20:15:14,664 INFO] Step 7640/10000; acc:  58.94; ppl:  7.19; xent: 1.97; lr: 1.00000; 8917/9384 tok/s;    290 sec\n","[2022-02-27 20:15:15,322 INFO] Step 7660/10000; acc:  66.80; ppl:  4.32; xent: 1.46; lr: 1.00000; 9138/9597 tok/s;    291 sec\n","[2022-02-27 20:15:16,010 INFO] Step 7680/10000; acc:  66.37; ppl:  4.47; xent: 1.50; lr: 1.00000; 8811/9074 tok/s;    291 sec\n","[2022-02-27 20:15:16,867 INFO] Step 7700/10000; acc:  62.78; ppl:  5.79; xent: 1.76; lr: 1.00000; 7957/8292 tok/s;    292 sec\n","[2022-02-27 20:15:17,598 INFO] Step 7720/10000; acc:  65.39; ppl:  4.60; xent: 1.53; lr: 1.00000; 8638/8985 tok/s;    293 sec\n","[2022-02-27 20:15:18,240 INFO] Step 7740/10000; acc:  66.47; ppl:  4.48; xent: 1.50; lr: 1.00000; 8926/9715 tok/s;    294 sec\n","[2022-02-27 20:15:19,049 INFO] Step 7760/10000; acc:  63.94; ppl:  5.39; xent: 1.68; lr: 1.00000; 7781/8523 tok/s;    294 sec\n","[2022-02-27 20:15:19,819 INFO] Step 7780/10000; acc:  62.46; ppl:  5.51; xent: 1.71; lr: 1.00000; 9248/9426 tok/s;    295 sec\n","[2022-02-27 20:15:20,461 INFO] Step 7800/10000; acc:  70.06; ppl:  3.57; xent: 1.27; lr: 1.00000; 8589/9320 tok/s;    296 sec\n","[2022-02-27 20:15:21,280 INFO] Step 7820/10000; acc:  61.69; ppl:  6.23; xent: 1.83; lr: 1.00000; 8418/8426 tok/s;    297 sec\n","[2022-02-27 20:15:22,048 INFO] Step 7840/10000; acc:  61.77; ppl:  5.86; xent: 1.77; lr: 1.00000; 9557/9527 tok/s;    297 sec\n","[2022-02-27 20:15:22,724 INFO] Step 7860/10000; acc:  69.80; ppl:  3.69; xent: 1.31; lr: 1.00000; 8362/9028 tok/s;    298 sec\n","[2022-02-27 20:15:23,504 INFO] Step 7880/10000; acc:  62.08; ppl:  6.18; xent: 1.82; lr: 1.00000; 8606/8563 tok/s;    299 sec\n","[2022-02-27 20:15:24,266 INFO] Step 7900/10000; acc:  61.63; ppl:  5.61; xent: 1.73; lr: 1.00000; 9322/9405 tok/s;    300 sec\n","[2022-02-27 20:15:24,939 INFO] Step 7920/10000; acc:  68.52; ppl:  3.86; xent: 1.35; lr: 1.00000; 8765/9176 tok/s;    300 sec\n","[2022-02-27 20:15:25,638 INFO] Step 7940/10000; acc:  67.88; ppl:  4.35; xent: 1.47; lr: 1.00000; 8447/8587 tok/s;    301 sec\n","[2022-02-27 20:15:26,515 INFO] Step 7960/10000; acc:  59.10; ppl:  6.83; xent: 1.92; lr: 1.00000; 8843/9207 tok/s;    302 sec\n","[2022-02-27 20:15:27,228 INFO] Step 7980/10000; acc:  67.63; ppl:  4.14; xent: 1.42; lr: 1.00000; 8401/8684 tok/s;    303 sec\n","[2022-02-27 20:15:27,912 INFO] Step 8000/10000; acc:  65.78; ppl:  4.71; xent: 1.55; lr: 1.00000; 8787/9154 tok/s;    303 sec\n","[2022-02-27 20:15:27,972 INFO] Saving checkpoint ./Question10/model_step_8000.pt\n","[2022-02-27 20:15:29,035 INFO] Step 8020/10000; acc:  62.17; ppl:  5.94; xent: 1.78; lr: 1.00000; 6158/6510 tok/s;    304 sec\n","[2022-02-27 20:15:29,743 INFO] Step 8040/10000; acc:  66.76; ppl:  4.32; xent: 1.46; lr: 1.00000; 8980/9256 tok/s;    305 sec\n","[2022-02-27 20:15:30,407 INFO] Step 8060/10000; acc:  66.58; ppl:  4.34; xent: 1.47; lr: 1.00000; 8760/9535 tok/s;    306 sec\n","[2022-02-27 20:15:30,590 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 14\n","[2022-02-27 20:15:31,252 INFO] Step 8080/10000; acc:  64.47; ppl:  5.40; xent: 1.69; lr: 1.00000; 7427/8031 tok/s;    307 sec\n","[2022-02-27 20:15:32,035 INFO] Step 8100/10000; acc:  63.59; ppl:  5.19; xent: 1.65; lr: 1.00000; 9083/9366 tok/s;    307 sec\n","[2022-02-27 20:15:32,656 INFO] Step 8120/10000; acc:  69.85; ppl:  3.64; xent: 1.29; lr: 1.00000; 8838/9429 tok/s;    308 sec\n","[2022-02-27 20:15:33,109 INFO] Validation perplexity: 5.59637\n","[2022-02-27 20:15:33,110 INFO] Validation accuracy: 69.0079\n","[2022-02-27 20:15:33,766 INFO] Step 8140/10000; acc:  60.97; ppl:  6.31; xent: 1.84; lr: 1.00000; 6051/6400 tok/s;    309 sec\n","[2022-02-27 20:15:34,573 INFO] Step 8160/10000; acc:  63.39; ppl:  5.14; xent: 1.64; lr: 1.00000; 8906/8941 tok/s;    310 sec\n","[2022-02-27 20:15:35,212 INFO] Step 8180/10000; acc:  68.26; ppl:  3.91; xent: 1.36; lr: 1.00000; 8708/9492 tok/s;    311 sec\n","[2022-02-27 20:15:35,991 INFO] Step 8200/10000; acc:  61.80; ppl:  6.22; xent: 1.83; lr: 1.00000; 8638/8618 tok/s;    311 sec\n","[2022-02-27 20:15:36,799 INFO] Step 8220/10000; acc:  61.88; ppl:  5.84; xent: 1.76; lr: 1.00000; 8805/9331 tok/s;    312 sec\n","[2022-02-27 20:15:37,523 INFO] Step 8240/10000; acc:  68.13; ppl:  3.88; xent: 1.36; lr: 1.00000; 8279/8412 tok/s;    313 sec\n","[2022-02-27 20:15:38,225 INFO] Step 8260/10000; acc:  66.95; ppl:  4.36; xent: 1.47; lr: 1.00000; 8474/8794 tok/s;    314 sec\n","[2022-02-27 20:15:39,035 INFO] Step 8280/10000; acc:  60.77; ppl:  6.36; xent: 1.85; lr: 1.00000; 9438/9741 tok/s;    314 sec\n","[2022-02-27 20:15:39,730 INFO] Step 8300/10000; acc:  69.22; ppl:  3.92; xent: 1.37; lr: 1.00000; 8474/9019 tok/s;    315 sec\n","[2022-02-27 20:15:40,433 INFO] Step 8320/10000; acc:  66.43; ppl:  4.31; xent: 1.46; lr: 1.00000; 8392/8966 tok/s;    316 sec\n","[2022-02-27 20:15:41,282 INFO] Step 8340/10000; acc:  63.85; ppl:  5.37; xent: 1.68; lr: 1.00000; 8022/8503 tok/s;    317 sec\n","[2022-02-27 20:15:42,000 INFO] Step 8360/10000; acc:  66.71; ppl:  4.34; xent: 1.47; lr: 1.00000; 8886/9176 tok/s;    317 sec\n","[2022-02-27 20:15:42,653 INFO] Step 8380/10000; acc:  68.10; ppl:  4.04; xent: 1.40; lr: 1.00000; 8864/9527 tok/s;    318 sec\n","[2022-02-27 20:15:43,480 INFO] Step 8400/10000; acc:  64.66; ppl:  5.27; xent: 1.66; lr: 1.00000; 7759/8534 tok/s;    319 sec\n","[2022-02-27 20:15:44,232 INFO] Step 8420/10000; acc:  62.29; ppl:  5.27; xent: 1.66; lr: 1.00000; 9584/9530 tok/s;    320 sec\n","[2022-02-27 20:15:44,886 INFO] Step 8440/10000; acc:  69.95; ppl:  3.53; xent: 1.26; lr: 1.00000; 8479/9202 tok/s;    320 sec\n","[2022-02-27 20:15:45,686 INFO] Step 8460/10000; acc:  63.40; ppl:  5.53; xent: 1.71; lr: 1.00000; 8501/8456 tok/s;    321 sec\n","[2022-02-27 20:15:46,451 INFO] Step 8480/10000; acc:  62.90; ppl:  5.32; xent: 1.67; lr: 1.00000; 9538/9424 tok/s;    322 sec\n","[2022-02-27 20:15:47,103 INFO] Step 8500/10000; acc:  70.55; ppl:  3.49; xent: 1.25; lr: 1.00000; 8612/9252 tok/s;    323 sec\n","[2022-02-27 20:15:47,907 INFO] Step 8520/10000; acc:  62.23; ppl:  5.95; xent: 1.78; lr: 1.00000; 8457/8398 tok/s;    323 sec\n","[2022-02-27 20:15:48,679 INFO] Step 8540/10000; acc:  62.34; ppl:  5.70; xent: 1.74; lr: 1.00000; 9348/9474 tok/s;    324 sec\n","[2022-02-27 20:15:49,334 INFO] Step 8560/10000; acc:  69.76; ppl:  3.58; xent: 1.28; lr: 1.00000; 9154/9267 tok/s;    325 sec\n","[2022-02-27 20:15:50,058 INFO] Step 8580/10000; acc:  66.40; ppl:  4.46; xent: 1.50; lr: 1.00000; 8246/8700 tok/s;    325 sec\n","[2022-02-27 20:15:50,923 INFO] Step 8600/10000; acc:  60.22; ppl:  6.35; xent: 1.85; lr: 1.00000; 8830/9322 tok/s;    326 sec\n","[2022-02-27 20:15:51,606 INFO] Step 8620/10000; acc:  68.64; ppl:  3.85; xent: 1.35; lr: 1.00000; 8711/9076 tok/s;    327 sec\n","[2022-02-27 20:15:52,279 INFO] Step 8640/10000; acc:  66.32; ppl:  4.37; xent: 1.47; lr: 1.00000; 8915/9204 tok/s;    328 sec\n","[2022-02-27 20:15:53,146 INFO] Step 8660/10000; acc:  63.29; ppl:  5.55; xent: 1.71; lr: 1.00000; 7953/8341 tok/s;    329 sec\n","[2022-02-27 20:15:53,861 INFO] Step 8680/10000; acc:  66.95; ppl:  4.29; xent: 1.46; lr: 1.00000; 8836/9148 tok/s;    329 sec\n","[2022-02-27 20:15:54,514 INFO] Step 8700/10000; acc:  67.27; ppl:  4.22; xent: 1.44; lr: 1.00000; 8798/9660 tok/s;    330 sec\n","[2022-02-27 20:15:54,686 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 15\n","[2022-02-27 20:15:55,324 INFO] Step 8720/10000; acc:  65.50; ppl:  4.77; xent: 1.56; lr: 1.00000; 7706/8221 tok/s;    331 sec\n","[2022-02-27 20:15:56,114 INFO] Step 8740/10000; acc:  63.62; ppl:  5.06; xent: 1.62; lr: 1.00000; 9033/9319 tok/s;    332 sec\n","[2022-02-27 20:15:56,694 INFO] Validation perplexity: 5.61241\n","[2022-02-27 20:15:56,695 INFO] Validation accuracy: 68.9615\n","[2022-02-27 20:15:57,018 INFO] Step 8760/10000; acc:  69.62; ppl:  3.60; xent: 1.28; lr: 1.00000; 6107/6711 tok/s;    332 sec\n","[2022-02-27 20:15:57,885 INFO] Step 8780/10000; acc:  61.52; ppl:  6.32; xent: 1.84; lr: 1.00000; 7834/8101 tok/s;    333 sec\n","[2022-02-27 20:15:58,661 INFO] Step 8800/10000; acc:  64.11; ppl:  4.98; xent: 1.60; lr: 1.00000; 9399/9259 tok/s;    334 sec\n","[2022-02-27 20:15:59,319 INFO] Step 8820/10000; acc:  69.93; ppl:  3.56; xent: 1.27; lr: 1.00000; 8536/9175 tok/s;    335 sec\n","[2022-02-27 20:16:00,124 INFO] Step 8840/10000; acc:  62.27; ppl:  5.88; xent: 1.77; lr: 1.00000; 8395/8650 tok/s;    336 sec\n","[2022-02-27 20:16:00,898 INFO] Step 8860/10000; acc:  62.29; ppl:  5.64; xent: 1.73; lr: 1.00000; 9152/9568 tok/s;    336 sec\n","[2022-02-27 20:16:01,581 INFO] Step 8880/10000; acc:  69.47; ppl:  3.59; xent: 1.28; lr: 1.00000; 8703/9157 tok/s;    337 sec\n","[2022-02-27 20:16:02,304 INFO] Step 8900/10000; acc:  67.92; ppl:  4.07; xent: 1.40; lr: 1.00000; 8199/8434 tok/s;    338 sec\n","[2022-02-27 20:16:03,117 INFO] Step 8920/10000; acc:  62.34; ppl:  5.51; xent: 1.71; lr: 1.00000; 9236/9274 tok/s;    339 sec\n","[2022-02-27 20:16:03,800 INFO] Step 8940/10000; acc:  68.55; ppl:  3.81; xent: 1.34; lr: 1.00000; 8454/9205 tok/s;    339 sec\n","[2022-02-27 20:16:04,483 INFO] Step 8960/10000; acc:  67.15; ppl:  4.18; xent: 1.43; lr: 1.00000; 8409/9321 tok/s;    340 sec\n","[2022-02-27 20:16:05,352 INFO] Step 8980/10000; acc:  63.75; ppl:  5.34; xent: 1.68; lr: 1.00000; 7926/8555 tok/s;    341 sec\n","[2022-02-27 20:16:06,073 INFO] Step 9000/10000; acc:  66.90; ppl:  4.22; xent: 1.44; lr: 1.00000; 8911/9178 tok/s;    342 sec\n","[2022-02-27 20:16:06,136 INFO] Saving checkpoint ./Question10/model_step_9000.pt\n","[2022-02-27 20:16:06,980 INFO] Step 9020/10000; acc:  69.46; ppl:  3.76; xent: 1.32; lr: 1.00000; 6447/6859 tok/s;    342 sec\n","[2022-02-27 20:16:07,806 INFO] Step 9040/10000; acc:  65.75; ppl:  4.88; xent: 1.59; lr: 1.00000; 7984/7936 tok/s;    343 sec\n","[2022-02-27 20:16:08,545 INFO] Step 9060/10000; acc:  63.00; ppl:  5.22; xent: 1.65; lr: 1.00000; 9794/9989 tok/s;    344 sec\n","[2022-02-27 20:16:09,208 INFO] Step 9080/10000; acc:  70.83; ppl:  3.44; xent: 1.23; lr: 1.00000; 8468/9333 tok/s;    345 sec\n","[2022-02-27 20:16:10,034 INFO] Step 9100/10000; acc:  62.40; ppl:  5.74; xent: 1.75; lr: 1.00000; 8307/8361 tok/s;    345 sec\n","[2022-02-27 20:16:10,800 INFO] Step 9120/10000; acc:  63.50; ppl:  5.01; xent: 1.61; lr: 1.00000; 9566/9425 tok/s;    346 sec\n","[2022-02-27 20:16:11,473 INFO] Step 9140/10000; acc:  70.81; ppl:  3.45; xent: 1.24; lr: 1.00000; 8333/8943 tok/s;    347 sec\n","[2022-02-27 20:16:12,288 INFO] Step 9160/10000; acc:  63.57; ppl:  5.49; xent: 1.70; lr: 1.00000; 8254/8192 tok/s;    348 sec\n","[2022-02-27 20:16:13,105 INFO] Step 9180/10000; acc:  63.52; ppl:  5.19; xent: 1.65; lr: 1.00000; 8704/8717 tok/s;    349 sec\n","[2022-02-27 20:16:13,766 INFO] Step 9200/10000; acc:  70.88; ppl:  3.37; xent: 1.21; lr: 1.00000; 8926/9365 tok/s;    349 sec\n","[2022-02-27 20:16:14,477 INFO] Step 9220/10000; acc:  66.57; ppl:  4.36; xent: 1.47; lr: 1.00000; 8306/8694 tok/s;    350 sec\n","[2022-02-27 20:16:15,341 INFO] Step 9240/10000; acc:  60.74; ppl:  6.15; xent: 1.82; lr: 1.00000; 8894/9459 tok/s;    351 sec\n","[2022-02-27 20:16:16,019 INFO] Step 9260/10000; acc:  69.62; ppl:  3.65; xent: 1.29; lr: 1.00000; 8778/9198 tok/s;    351 sec\n","[2022-02-27 20:16:16,714 INFO] Step 9280/10000; acc:  68.80; ppl:  3.96; xent: 1.38; lr: 1.00000; 8640/9009 tok/s;    352 sec\n","[2022-02-27 20:16:17,569 INFO] Step 9300/10000; acc:  64.68; ppl:  5.06; xent: 1.62; lr: 1.00000; 7927/8434 tok/s;    353 sec\n","[2022-02-27 20:16:18,290 INFO] Step 9320/10000; acc:  67.99; ppl:  3.89; xent: 1.36; lr: 1.00000; 8772/8979 tok/s;    354 sec\n","[2022-02-27 20:16:18,920 INFO] Step 9340/10000; acc:  67.38; ppl:  4.09; xent: 1.41; lr: 1.00000; 9121/9888 tok/s;    354 sec\n","[2022-02-27 20:16:19,088 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 16\n","[2022-02-27 20:16:19,768 INFO] Step 9360/10000; acc:  65.63; ppl:  4.76; xent: 1.56; lr: 1.00000; 7497/7937 tok/s;    355 sec\n","[2022-02-27 20:16:20,656 INFO] Validation perplexity: 5.59103\n","[2022-02-27 20:16:20,657 INFO] Validation accuracy: 68.5211\n","[2022-02-27 20:16:20,820 INFO] Step 9380/10000; acc:  63.71; ppl:  4.95; xent: 1.60; lr: 1.00000; 6795/7136 tok/s;    356 sec\n","[2022-02-27 20:16:21,474 INFO] Step 9400/10000; acc:  70.73; ppl:  3.38; xent: 1.22; lr: 1.00000; 8492/9154 tok/s;    357 sec\n","[2022-02-27 20:16:22,353 INFO] Step 9420/10000; acc:  62.96; ppl:  5.55; xent: 1.71; lr: 1.00000; 7719/7882 tok/s;    358 sec\n","[2022-02-27 20:16:23,117 INFO] Step 9440/10000; acc:  63.97; ppl:  4.92; xent: 1.59; lr: 1.00000; 9585/9488 tok/s;    359 sec\n","[2022-02-27 20:16:23,763 INFO] Step 9460/10000; acc:  70.70; ppl:  3.40; xent: 1.22; lr: 1.00000; 8759/9407 tok/s;    359 sec\n","[2022-02-27 20:16:24,562 INFO] Step 9480/10000; acc:  62.40; ppl:  5.82; xent: 1.76; lr: 1.00000; 8492/8809 tok/s;    360 sec\n","[2022-02-27 20:16:25,340 INFO] Step 9500/10000; acc:  63.21; ppl:  5.26; xent: 1.66; lr: 1.00000; 9142/9569 tok/s;    361 sec\n","[2022-02-27 20:16:26,030 INFO] Step 9520/10000; acc:  70.40; ppl:  3.41; xent: 1.23; lr: 1.00000; 8629/9016 tok/s;    361 sec\n","[2022-02-27 20:16:26,743 INFO] Step 9540/10000; acc:  68.96; ppl:  3.77; xent: 1.33; lr: 1.00000; 8331/8729 tok/s;    362 sec\n","[2022-02-27 20:16:27,565 INFO] Step 9560/10000; acc:  64.49; ppl:  5.09; xent: 1.63; lr: 1.00000; 8960/9168 tok/s;    363 sec\n","[2022-02-27 20:16:28,244 INFO] Step 9580/10000; acc:  70.00; ppl:  3.56; xent: 1.27; lr: 1.00000; 8497/9155 tok/s;    364 sec\n","[2022-02-27 20:16:28,929 INFO] Step 9600/10000; acc:  69.01; ppl:  3.80; xent: 1.34; lr: 1.00000; 8354/9093 tok/s;    364 sec\n","[2022-02-27 20:16:29,818 INFO] Step 9620/10000; acc:  63.87; ppl:  5.27; xent: 1.66; lr: 1.00000; 7787/8286 tok/s;    365 sec\n","[2022-02-27 20:16:30,534 INFO] Step 9640/10000; acc:  68.12; ppl:  3.85; xent: 1.35; lr: 1.00000; 8889/9211 tok/s;    366 sec\n","[2022-02-27 20:16:31,204 INFO] Step 9660/10000; acc:  69.24; ppl:  3.70; xent: 1.31; lr: 1.00000; 8652/9266 tok/s;    367 sec\n","[2022-02-27 20:16:32,018 INFO] Step 9680/10000; acc:  65.98; ppl:  4.78; xent: 1.56; lr: 1.00000; 8180/8207 tok/s;    367 sec\n","[2022-02-27 20:16:32,802 INFO] Step 9700/10000; acc:  62.71; ppl:  5.21; xent: 1.65; lr: 1.00000; 9378/9501 tok/s;    368 sec\n","[2022-02-27 20:16:33,471 INFO] Step 9720/10000; acc:  71.30; ppl:  3.24; xent: 1.18; lr: 1.00000; 8623/8976 tok/s;    369 sec\n","[2022-02-27 20:16:34,285 INFO] Step 9740/10000; acc:  63.30; ppl:  5.41; xent: 1.69; lr: 1.00000; 8348/8686 tok/s;    370 sec\n","[2022-02-27 20:16:35,031 INFO] Step 9760/10000; acc:  65.11; ppl:  4.77; xent: 1.56; lr: 1.00000; 9776/9481 tok/s;    370 sec\n","[2022-02-27 20:16:35,677 INFO] Step 9780/10000; acc:  72.00; ppl:  3.15; xent: 1.15; lr: 1.00000; 8625/9332 tok/s;    371 sec\n","[2022-02-27 20:16:36,475 INFO] Step 9800/10000; acc:  64.73; ppl:  5.06; xent: 1.62; lr: 1.00000; 8384/8425 tok/s;    372 sec\n","[2022-02-27 20:16:37,270 INFO] Step 9820/10000; acc:  63.28; ppl:  5.18; xent: 1.65; lr: 1.00000; 8999/9179 tok/s;    373 sec\n","[2022-02-27 20:16:37,935 INFO] Step 9840/10000; acc:  71.47; ppl:  3.27; xent: 1.18; lr: 1.00000; 8895/9363 tok/s;    373 sec\n","[2022-02-27 20:16:38,655 INFO] Step 9860/10000; acc:  68.85; ppl:  3.86; xent: 1.35; lr: 1.00000; 8254/8539 tok/s;    374 sec\n","[2022-02-27 20:16:39,483 INFO] Step 9880/10000; acc:  62.11; ppl:  5.59; xent: 1.72; lr: 1.00000; 9164/9365 tok/s;    375 sec\n","[2022-02-27 20:16:40,145 INFO] Step 9900/10000; acc:  70.40; ppl:  3.41; xent: 1.23; lr: 1.00000; 8837/9384 tok/s;    376 sec\n","[2022-02-27 20:16:40,863 INFO] Step 9920/10000; acc:  67.87; ppl:  4.01; xent: 1.39; lr: 1.00000; 8147/8871 tok/s;    376 sec\n","[2022-02-27 20:16:41,716 INFO] Step 9940/10000; acc:  65.69; ppl:  4.85; xent: 1.58; lr: 1.00000; 8023/8531 tok/s;    377 sec\n","[2022-02-27 20:16:42,430 INFO] Step 9960/10000; acc:  68.92; ppl:  3.73; xent: 1.32; lr: 1.00000; 8856/9145 tok/s;    378 sec\n","[2022-02-27 20:16:43,068 INFO] Step 9980/10000; acc:  67.65; ppl:  4.04; xent: 1.40; lr: 1.00000; 9033/9706 tok/s;    378 sec\n","[2022-02-27 20:16:43,237 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 17\n","[2022-02-27 20:16:43,951 INFO] Step 10000/10000; acc:  65.74; ppl:  4.69; xent: 1.54; lr: 1.00000; 7283/7793 tok/s;    379 sec\n","[2022-02-27 20:16:44,205 INFO] Validation perplexity: 5.68047\n","[2022-02-27 20:16:44,206 INFO] Validation accuracy: 68.1734\n","[2022-02-27 20:16:44,268 INFO] Saving checkpoint ./Question10/model_step_10000.pt\n"]},{"data":{"text/plain":[]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_train -config Question10/config-base.yaml"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53532,"status":"ok","timestamp":1645993172179,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"Qb5VN2GzzZUx","outputId":"79b7307c-a5d0-4ead-c131-e2430ea92cbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:18:40,590 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:18:45,174 INFO] PRED AVG SCORE: -0.8394, PRED PPL: 2.3151\n","BLEU = 23.89, 56.7/29.5/19.7/11.9 (BP=0.955, ratio=0.956, hyp_len=3417, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:18:47,256 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:18:51,920 INFO] PRED AVG SCORE: -0.7606, PRED PPL: 2.1396\n","BLEU = 27.83, 58.9/33.3/23.3/14.6 (BP=0.973, ratio=0.973, hyp_len=3479, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:18:53,965 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:18:58,624 INFO] PRED AVG SCORE: -0.6755, PRED PPL: 1.9650\n","BLEU = 30.23, 62.2/36.7/25.4/16.6 (BP=0.964, ratio=0.965, hyp_len=3449, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:19:00,700 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:19:05,244 INFO] PRED AVG SCORE: -0.6102, PRED PPL: 1.8408\n","BLEU = 32.10, 63.4/38.6/27.3/18.5 (BP=0.962, ratio=0.963, hyp_len=3441, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:19:07,247 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:19:11,826 INFO] PRED AVG SCORE: -0.5809, PRED PPL: 1.7876\n","BLEU = 31.49, 61.7/37.2/26.3/17.6 (BP=0.981, ratio=0.981, hyp_len=3507, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:19:13,850 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:19:18,281 INFO] PRED AVG SCORE: -0.5294, PRED PPL: 1.6978\n","BLEU = 33.19, 64.8/40.5/29.0/20.1 (BP=0.944, ratio=0.946, hyp_len=3380, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:19:20,300 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:19:24,825 INFO] PRED AVG SCORE: -0.5241, PRED PPL: 1.6889\n","BLEU = 32.32, 63.9/39.0/27.1/18.0 (BP=0.973, ratio=0.973, hyp_len=3478, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:19:26,861 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:19:31,443 INFO] PRED AVG SCORE: -0.4878, PRED PPL: 1.6288\n","BLEU = 32.87, 64.5/39.5/28.3/19.4 (BP=0.955, ratio=0.956, hyp_len=3418, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model Question10/model_step_3000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred3000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred3000.txt\n","onmt_translate -model Question10/model_step_4000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred4000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred4000.txt\n","onmt_translate -model Question10/model_step_5000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred5000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred5000.txt\n","onmt_translate -model Question10/model_step_6000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred6000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred6000.txt\n","onmt_translate -model Question10/model_step_7000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred7000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred7000.txt\n","onmt_translate -model Question10/model_step_8000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred8000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred8000.txt\n","onmt_translate -model Question10/model_step_9000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred9000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred9000.txt\n","onmt_translate -model Question10/model_step_10000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question10/pred10000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question10/pred10000.txt"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382985,"status":"ok","timestamp":1645993923329,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"dmgUPBwA2YPP","outputId":"e34e1c94-4b3f-406e-de84-e65dc34acfe9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:25:41,908 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 20:25:41,911 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 20:25:41,911 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 20:25:41,916 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 20:25:41,917 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 20:25:41,917 INFO] Loading vocab from text file...\n","[2022-02-27 20:25:41,917 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 20:25:41,946 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 20:25:41,952 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 20:25:41,972 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 20:25:41,976 INFO] Building fields with vocab in counters...\n","[2022-02-27 20:25:41,986 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 20:25:42,000 INFO]  * src vocab size: 10415.\n","[2022-02-27 20:25:42,001 INFO]  * src vocab size = 10415\n","[2022-02-27 20:25:42,001 INFO]  * tgt vocab size = 8198\n","[2022-02-27 20:25:42,005 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:25:44,636 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 20:25:44,637 INFO] encoder: 5852620\n","[2022-02-27 20:25:44,637 INFO] decoder: 7244222\n","[2022-02-27 20:25:44,637 INFO] * number of parameters: 13096842\n","[2022-02-27 20:25:44,639 INFO] Starting training on GPU: [0]\n","[2022-02-27 20:25:44,640 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 20:25:44,640 INFO] train's transforms: TransformPipe()\n","[2022-02-27 20:25:44,640 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 20:25:45,452 INFO] Step 20/10000; acc:   9.37; ppl: 2345.08; xent: 7.76; lr: 1.00000; 7450/8009 tok/s;      1 sec\n","[2022-02-27 20:25:46,162 INFO] Step 40/10000; acc:  15.97; ppl: 450.17; xent: 6.11; lr: 1.00000; 8887/9159 tok/s;      2 sec\n","[2022-02-27 20:25:46,975 INFO] Step 60/10000; acc:  16.53; ppl: 296.60; xent: 5.69; lr: 1.00000; 8223/8751 tok/s;      2 sec\n","[2022-02-27 20:25:47,780 INFO] Step 80/10000; acc:  19.66; ppl: 214.05; xent: 5.37; lr: 1.00000; 8521/8351 tok/s;      3 sec\n","[2022-02-27 20:25:48,429 INFO] Step 100/10000; acc:  24.42; ppl: 148.66; xent: 5.00; lr: 1.00000; 8675/9233 tok/s;      4 sec\n","[2022-02-27 20:25:49,265 INFO] Step 120/10000; acc:  21.22; ppl: 193.49; xent: 5.27; lr: 1.00000; 8867/9113 tok/s;      5 sec\n","[2022-02-27 20:25:49,983 INFO] Step 140/10000; acc:  24.51; ppl: 142.55; xent: 4.96; lr: 1.00000; 9436/9397 tok/s;      5 sec\n","[2022-02-27 20:25:50,640 INFO] Step 160/10000; acc:  29.23; ppl: 94.45; xent: 4.55; lr: 1.00000; 8155/9137 tok/s;      6 sec\n","[2022-02-27 20:25:51,380 INFO] Step 180/10000; acc:  26.10; ppl: 116.09; xent: 4.75; lr: 1.00000; 9047/9642 tok/s;      7 sec\n","[2022-02-27 20:25:52,197 INFO] Step 200/10000; acc:  28.23; ppl: 112.55; xent: 4.72; lr: 1.00000; 8775/8966 tok/s;      8 sec\n","[2022-02-27 20:25:52,814 INFO] Step 220/10000; acc:  34.72; ppl: 70.12; xent: 4.25; lr: 1.00000; 8622/9354 tok/s;      8 sec\n","[2022-02-27 20:25:53,575 INFO] Step 240/10000; acc:  29.75; ppl: 88.60; xent: 4.48; lr: 1.00000; 8931/9315 tok/s;      9 sec\n","[2022-02-27 20:25:54,379 INFO] Step 260/10000; acc:  33.06; ppl: 77.21; xent: 4.35; lr: 1.00000; 7946/8476 tok/s;     10 sec\n","[2022-02-27 20:25:55,034 INFO] Step 280/10000; acc:  35.34; ppl: 64.83; xent: 4.17; lr: 1.00000; 9116/9816 tok/s;     10 sec\n","[2022-02-27 20:25:55,761 INFO] Step 300/10000; acc:  33.93; ppl: 69.55; xent: 4.24; lr: 1.00000; 8751/9352 tok/s;     11 sec\n","[2022-02-27 20:25:56,558 INFO] Step 320/10000; acc:  32.75; ppl: 73.98; xent: 4.30; lr: 1.00000; 8764/8897 tok/s;     12 sec\n","[2022-02-27 20:25:57,310 INFO] Step 340/10000; acc:  36.88; ppl: 55.51; xent: 4.02; lr: 1.00000; 8248/8317 tok/s;     13 sec\n","[2022-02-27 20:25:58,020 INFO] Step 360/10000; acc:  35.04; ppl: 60.58; xent: 4.10; lr: 1.00000; 9127/9254 tok/s;     13 sec\n","[2022-02-27 20:25:58,798 INFO] Step 380/10000; acc:  34.09; ppl: 59.91; xent: 4.09; lr: 1.00000; 8674/8975 tok/s;     14 sec\n","[2022-02-27 20:25:59,542 INFO] Step 400/10000; acc:  36.77; ppl: 54.13; xent: 3.99; lr: 1.00000; 9210/9023 tok/s;     15 sec\n","[2022-02-27 20:26:00,192 INFO] Step 420/10000; acc:  40.38; ppl: 45.75; xent: 3.82; lr: 1.00000; 8687/8986 tok/s;     16 sec\n","[2022-02-27 20:26:00,998 INFO] Step 440/10000; acc:  32.48; ppl: 70.71; xent: 4.26; lr: 1.00000; 9299/9497 tok/s;     16 sec\n","[2022-02-27 20:26:01,763 INFO] Step 460/10000; acc:  35.72; ppl: 52.54; xent: 3.96; lr: 1.00000; 8757/9120 tok/s;     17 sec\n","[2022-02-27 20:26:02,414 INFO] Step 480/10000; acc:  42.14; ppl: 36.66; xent: 3.60; lr: 1.00000; 8179/8765 tok/s;     18 sec\n","[2022-02-27 20:26:03,152 INFO] Step 500/10000; acc:  36.00; ppl: 51.52; xent: 3.94; lr: 1.00000; 9048/9528 tok/s;     19 sec\n","[2022-02-27 20:26:03,981 INFO] Step 520/10000; acc:  35.40; ppl: 59.37; xent: 4.08; lr: 1.00000; 8619/8721 tok/s;     19 sec\n","[2022-02-27 20:26:04,615 INFO] Step 540/10000; acc:  44.13; ppl: 33.87; xent: 3.52; lr: 1.00000; 8380/9317 tok/s;     20 sec\n","[2022-02-27 20:26:05,382 INFO] Step 560/10000; acc:  36.81; ppl: 50.35; xent: 3.92; lr: 1.00000; 9079/9470 tok/s;     21 sec\n","[2022-02-27 20:26:06,016 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 20:26:06,162 INFO] Step 580/10000; acc:  39.08; ppl: 46.74; xent: 3.84; lr: 1.00000; 8310/8643 tok/s;     22 sec\n","[2022-02-27 20:26:06,859 INFO] Step 600/10000; acc:  40.82; ppl: 36.50; xent: 3.60; lr: 1.00000; 8528/9376 tok/s;     22 sec\n","[2022-02-27 20:26:07,585 INFO] Step 620/10000; acc:  39.14; ppl: 44.81; xent: 3.80; lr: 1.00000; 8750/9289 tok/s;     23 sec\n","[2022-02-27 20:26:07,766 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 20:26:08,017 INFO] Validation perplexity: 22.9898\n","[2022-02-27 20:26:08,017 INFO] Validation accuracy: 49.0728\n","[2022-02-27 20:26:08,017 INFO] Model is improving ppl: inf --> 22.9898.\n","[2022-02-27 20:26:08,018 INFO] Model is improving acc: -inf --> 49.0728.\n","[2022-02-27 20:26:08,609 INFO] Step 640/10000; acc:  37.52; ppl: 50.83; xent: 3.93; lr: 1.00000; 6787/6739 tok/s;     24 sec\n","[2022-02-27 20:26:09,362 INFO] Step 660/10000; acc:  42.12; ppl: 34.70; xent: 3.55; lr: 1.00000; 7984/8637 tok/s;     25 sec\n","[2022-02-27 20:26:10,080 INFO] Step 680/10000; acc:  40.84; ppl: 38.49; xent: 3.65; lr: 1.00000; 8733/9094 tok/s;     25 sec\n","[2022-02-27 20:26:10,900 INFO] Step 700/10000; acc:  40.03; ppl: 38.48; xent: 3.65; lr: 1.00000; 8109/8455 tok/s;     26 sec\n","[2022-02-27 20:26:11,638 INFO] Step 720/10000; acc:  41.86; ppl: 36.34; xent: 3.59; lr: 1.00000; 9350/9048 tok/s;     27 sec\n","[2022-02-27 20:26:12,304 INFO] Step 740/10000; acc:  44.27; ppl: 31.58; xent: 3.45; lr: 1.00000; 8531/8986 tok/s;     28 sec\n","[2022-02-27 20:26:13,143 INFO] Step 760/10000; acc:  38.27; ppl: 45.24; xent: 3.81; lr: 1.00000; 9024/9282 tok/s;     29 sec\n","[2022-02-27 20:26:13,908 INFO] Step 780/10000; acc:  42.50; ppl: 32.55; xent: 3.48; lr: 1.00000; 8627/9407 tok/s;     29 sec\n","[2022-02-27 20:26:14,549 INFO] Step 800/10000; acc:  46.58; ppl: 27.14; xent: 3.30; lr: 1.00000; 8265/8794 tok/s;     30 sec\n","[2022-02-27 20:26:15,278 INFO] Step 820/10000; acc:  43.40; ppl: 32.46; xent: 3.48; lr: 1.00000; 9017/9447 tok/s;     31 sec\n","[2022-02-27 20:26:16,077 INFO] Step 840/10000; acc:  40.87; ppl: 39.42; xent: 3.67; lr: 1.00000; 8877/9145 tok/s;     31 sec\n","[2022-02-27 20:26:16,696 INFO] Step 860/10000; acc:  47.57; ppl: 25.11; xent: 3.22; lr: 1.00000; 8611/9505 tok/s;     32 sec\n","[2022-02-27 20:26:17,450 INFO] Step 880/10000; acc:  42.51; ppl: 31.62; xent: 3.45; lr: 1.00000; 9109/9325 tok/s;     33 sec\n","[2022-02-27 20:26:18,249 INFO] Step 900/10000; acc:  43.34; ppl: 32.42; xent: 3.48; lr: 1.00000; 8136/8450 tok/s;     34 sec\n","[2022-02-27 20:26:18,937 INFO] Step 920/10000; acc:  45.41; ppl: 26.72; xent: 3.29; lr: 1.00000; 8741/9608 tok/s;     34 sec\n","[2022-02-27 20:26:19,660 INFO] Step 940/10000; acc:  43.53; ppl: 31.33; xent: 3.44; lr: 1.00000; 8957/9305 tok/s;     35 sec\n","[2022-02-27 20:26:20,456 INFO] Step 960/10000; acc:  42.32; ppl: 34.18; xent: 3.53; lr: 1.00000; 8912/8977 tok/s;     36 sec\n","[2022-02-27 20:26:21,227 INFO] Step 980/10000; acc:  46.74; ppl: 24.72; xent: 3.21; lr: 1.00000; 7936/8129 tok/s;     37 sec\n","[2022-02-27 20:26:21,952 INFO] Step 1000/10000; acc:  44.99; ppl: 26.52; xent: 3.28; lr: 1.00000; 8812/9021 tok/s;     37 sec\n","[2022-02-27 20:26:22,015 INFO] Saving checkpoint ./Question10/model_step_1000.pt\n","[2022-02-27 20:26:22,989 INFO] Step 1020/10000; acc:  43.19; ppl: 30.27; xent: 3.41; lr: 1.00000; 6387/6557 tok/s;     38 sec\n","[2022-02-27 20:26:23,713 INFO] Step 1040/10000; acc:  44.36; ppl: 28.53; xent: 3.35; lr: 1.00000; 9552/9098 tok/s;     39 sec\n","[2022-02-27 20:26:24,373 INFO] Step 1060/10000; acc:  48.45; ppl: 24.09; xent: 3.18; lr: 1.00000; 8653/8977 tok/s;     40 sec\n","[2022-02-27 20:26:25,220 INFO] Step 1080/10000; acc:  41.37; ppl: 33.86; xent: 3.52; lr: 1.00000; 9031/9309 tok/s;     41 sec\n","[2022-02-27 20:26:25,983 INFO] Step 1100/10000; acc:  43.89; ppl: 27.90; xent: 3.33; lr: 1.00000; 8824/9277 tok/s;     41 sec\n","[2022-02-27 20:26:26,619 INFO] Step 1120/10000; acc:  48.51; ppl: 21.00; xent: 3.04; lr: 1.00000; 8353/9054 tok/s;     42 sec\n","[2022-02-27 20:26:27,370 INFO] Step 1140/10000; acc:  44.82; ppl: 27.89; xent: 3.33; lr: 1.00000; 8848/9323 tok/s;     43 sec\n","[2022-02-27 20:26:28,175 INFO] Step 1160/10000; acc:  44.31; ppl: 29.26; xent: 3.38; lr: 1.00000; 8739/8853 tok/s;     44 sec\n","[2022-02-27 20:26:28,814 INFO] Step 1180/10000; acc:  49.75; ppl: 20.54; xent: 3.02; lr: 1.00000; 8220/9292 tok/s;     44 sec\n","[2022-02-27 20:26:29,567 INFO] Step 1200/10000; acc:  44.54; ppl: 26.46; xent: 3.28; lr: 1.00000; 9141/9579 tok/s;     45 sec\n","[2022-02-27 20:26:30,211 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 20:26:30,378 INFO] Step 1220/10000; acc:  46.63; ppl: 26.44; xent: 3.27; lr: 1.00000; 7953/8343 tok/s;     46 sec\n","[2022-02-27 20:26:31,081 INFO] Step 1240/10000; acc:  48.17; ppl: 20.44; xent: 3.02; lr: 1.00000; 8577/9151 tok/s;     46 sec\n","[2022-02-27 20:26:31,708 INFO] Validation perplexity: 13.9161\n","[2022-02-27 20:26:31,708 INFO] Validation accuracy: 54.1956\n","[2022-02-27 20:26:31,709 INFO] Model is improving ppl: 22.9898 --> 13.9161.\n","[2022-02-27 20:26:31,709 INFO] Model is improving acc: 49.0728 --> 54.1956.\n","[2022-02-27 20:26:32,083 INFO] Step 1260/10000; acc:  45.42; ppl: 25.26; xent: 3.23; lr: 1.00000; 6372/6704 tok/s;     47 sec\n","[2022-02-27 20:26:32,931 INFO] Step 1280/10000; acc:  44.47; ppl: 29.11; xent: 3.37; lr: 1.00000; 8199/8297 tok/s;     48 sec\n","[2022-02-27 20:26:33,696 INFO] Step 1300/10000; acc:  48.69; ppl: 19.70; xent: 2.98; lr: 1.00000; 8005/8348 tok/s;     49 sec\n","[2022-02-27 20:26:34,397 INFO] Step 1320/10000; acc:  47.73; ppl: 21.63; xent: 3.07; lr: 1.00000; 9060/9325 tok/s;     50 sec\n","[2022-02-27 20:26:35,213 INFO] Step 1340/10000; acc:  45.59; ppl: 25.81; xent: 3.25; lr: 1.00000; 8217/8575 tok/s;     51 sec\n","[2022-02-27 20:26:35,983 INFO] Step 1360/10000; acc:  47.66; ppl: 22.76; xent: 3.12; lr: 1.00000; 8900/8893 tok/s;     51 sec\n","[2022-02-27 20:26:36,666 INFO] Step 1380/10000; acc:  49.71; ppl: 20.87; xent: 3.04; lr: 1.00000; 8286/8769 tok/s;     52 sec\n","[2022-02-27 20:26:37,503 INFO] Step 1400/10000; acc:  43.97; ppl: 28.25; xent: 3.34; lr: 1.00000; 9031/9353 tok/s;     53 sec\n","[2022-02-27 20:26:38,243 INFO] Step 1420/10000; acc:  47.64; ppl: 21.85; xent: 3.08; lr: 1.00000; 8856/9311 tok/s;     54 sec\n","[2022-02-27 20:26:38,889 INFO] Step 1440/10000; acc:  52.11; ppl: 16.62; xent: 2.81; lr: 1.00000; 7986/8974 tok/s;     54 sec\n","[2022-02-27 20:26:39,620 INFO] Step 1460/10000; acc:  48.08; ppl: 20.92; xent: 3.04; lr: 1.00000; 8835/9278 tok/s;     55 sec\n","[2022-02-27 20:26:40,427 INFO] Step 1480/10000; acc:  46.09; ppl: 24.14; xent: 3.18; lr: 1.00000; 8647/9124 tok/s;     56 sec\n","[2022-02-27 20:26:41,050 INFO] Step 1500/10000; acc:  53.06; ppl: 15.64; xent: 2.75; lr: 1.00000; 8606/9563 tok/s;     56 sec\n","[2022-02-27 20:26:41,819 INFO] Step 1520/10000; acc:  47.59; ppl: 21.15; xent: 3.05; lr: 1.00000; 9025/9099 tok/s;     57 sec\n","[2022-02-27 20:26:42,610 INFO] Step 1540/10000; acc:  47.37; ppl: 23.02; xent: 3.14; lr: 1.00000; 8317/8538 tok/s;     58 sec\n","[2022-02-27 20:26:43,287 INFO] Step 1560/10000; acc:  49.56; ppl: 18.24; xent: 2.90; lr: 1.00000; 9000/9733 tok/s;     59 sec\n","[2022-02-27 20:26:44,018 INFO] Step 1580/10000; acc:  48.64; ppl: 19.47; xent: 2.97; lr: 1.00000; 8939/9369 tok/s;     59 sec\n","[2022-02-27 20:26:44,788 INFO] Step 1600/10000; acc:  46.51; ppl: 23.62; xent: 3.16; lr: 1.00000; 9321/9098 tok/s;     60 sec\n","[2022-02-27 20:26:45,555 INFO] Step 1620/10000; acc:  50.57; ppl: 17.19; xent: 2.84; lr: 1.00000; 8002/8301 tok/s;     61 sec\n","[2022-02-27 20:26:46,285 INFO] Step 1640/10000; acc:  48.50; ppl: 18.84; xent: 2.94; lr: 1.00000; 8780/8968 tok/s;     62 sec\n","[2022-02-27 20:26:47,038 INFO] Step 1660/10000; acc:  46.97; ppl: 22.36; xent: 3.11; lr: 1.00000; 8920/9107 tok/s;     62 sec\n","[2022-02-27 20:26:47,785 INFO] Step 1680/10000; acc:  49.45; ppl: 18.40; xent: 2.91; lr: 1.00000; 9170/8868 tok/s;     63 sec\n","[2022-02-27 20:26:48,431 INFO] Step 1700/10000; acc:  51.05; ppl: 16.99; xent: 2.83; lr: 1.00000; 8709/9109 tok/s;     64 sec\n","[2022-02-27 20:26:49,282 INFO] Step 1720/10000; acc:  46.00; ppl: 22.88; xent: 3.13; lr: 1.00000; 8815/9004 tok/s;     65 sec\n","[2022-02-27 20:26:50,034 INFO] Step 1740/10000; acc:  47.85; ppl: 19.15; xent: 2.95; lr: 1.00000; 8865/9363 tok/s;     65 sec\n","[2022-02-27 20:26:50,681 INFO] Step 1760/10000; acc:  54.22; ppl: 14.10; xent: 2.65; lr: 1.00000; 8161/8995 tok/s;     66 sec\n","[2022-02-27 20:26:51,416 INFO] Step 1780/10000; acc:  49.10; ppl: 19.49; xent: 2.97; lr: 1.00000; 9049/9430 tok/s;     67 sec\n","[2022-02-27 20:26:52,231 INFO] Step 1800/10000; acc:  46.83; ppl: 21.29; xent: 3.06; lr: 1.00000; 8665/8869 tok/s;     68 sec\n","[2022-02-27 20:26:52,859 INFO] Step 1820/10000; acc:  54.22; ppl: 13.57; xent: 2.61; lr: 1.00000; 8389/9230 tok/s;     68 sec\n","[2022-02-27 20:26:53,607 INFO] Step 1840/10000; acc:  47.94; ppl: 19.17; xent: 2.95; lr: 1.00000; 9152/9653 tok/s;     69 sec\n","[2022-02-27 20:26:54,249 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 20:26:54,424 INFO] Step 1860/10000; acc:  50.34; ppl: 18.28; xent: 2.91; lr: 1.00000; 7806/8252 tok/s;     70 sec\n","[2022-02-27 20:26:55,228 INFO] Validation perplexity: 10.0947\n","[2022-02-27 20:26:55,229 INFO] Validation accuracy: 59.5967\n","[2022-02-27 20:26:55,229 INFO] Model is improving ppl: 13.9161 --> 10.0947.\n","[2022-02-27 20:26:55,229 INFO] Model is improving acc: 54.1956 --> 59.5967.\n","[2022-02-27 20:26:55,366 INFO] Step 1880/10000; acc:  52.93; ppl: 14.49; xent: 2.67; lr: 1.00000; 6416/6858 tok/s;     71 sec\n","[2022-02-27 20:26:56,090 INFO] Step 1900/10000; acc:  49.78; ppl: 17.98; xent: 2.89; lr: 1.00000; 8902/9253 tok/s;     71 sec\n","[2022-02-27 20:26:56,908 INFO] Step 1920/10000; acc:  48.26; ppl: 20.31; xent: 3.01; lr: 1.00000; 8613/8805 tok/s;     72 sec\n","[2022-02-27 20:26:57,670 INFO] Step 1940/10000; acc:  52.23; ppl: 15.25; xent: 2.72; lr: 1.00000; 8106/8382 tok/s;     73 sec\n","[2022-02-27 20:26:58,388 INFO] Step 1960/10000; acc:  52.51; ppl: 15.05; xent: 2.71; lr: 1.00000; 8890/9196 tok/s;     74 sec\n","[2022-02-27 20:26:59,206 INFO] Step 1980/10000; acc:  49.41; ppl: 18.53; xent: 2.92; lr: 1.00000; 8228/8567 tok/s;     75 sec\n","[2022-02-27 20:26:59,963 INFO] Step 2000/10000; acc:  51.04; ppl: 16.80; xent: 2.82; lr: 1.00000; 9062/9103 tok/s;     75 sec\n","[2022-02-27 20:27:00,024 INFO] Saving checkpoint ./Question10/model_step_2000.pt\n","[2022-02-27 20:27:00,895 INFO] Step 2020/10000; acc:  53.75; ppl: 14.45; xent: 2.67; lr: 1.00000; 6124/6404 tok/s;     76 sec\n","[2022-02-27 20:27:01,725 INFO] Step 2040/10000; acc:  47.53; ppl: 20.75; xent: 3.03; lr: 1.00000; 9087/9562 tok/s;     77 sec\n","[2022-02-27 20:27:02,459 INFO] Step 2060/10000; acc:  51.36; ppl: 16.34; xent: 2.79; lr: 1.00000; 8931/9402 tok/s;     78 sec\n","[2022-02-27 20:27:03,111 INFO] Step 2080/10000; acc:  55.51; ppl: 12.08; xent: 2.49; lr: 1.00000; 7930/8806 tok/s;     78 sec\n","[2022-02-27 20:27:03,824 INFO] Step 2100/10000; acc:  52.03; ppl: 15.05; xent: 2.71; lr: 1.00000; 8988/9411 tok/s;     79 sec\n","[2022-02-27 20:27:04,626 INFO] Step 2120/10000; acc:  49.69; ppl: 17.31; xent: 2.85; lr: 1.00000; 8558/8995 tok/s;     80 sec\n","[2022-02-27 20:27:05,256 INFO] Step 2140/10000; acc:  56.17; ppl: 12.09; xent: 2.49; lr: 1.00000; 8411/9244 tok/s;     81 sec\n","[2022-02-27 20:27:06,017 INFO] Step 2160/10000; acc:  51.85; ppl: 15.58; xent: 2.75; lr: 1.00000; 8994/9344 tok/s;     81 sec\n","[2022-02-27 20:27:06,793 INFO] Step 2180/10000; acc:  50.89; ppl: 16.85; xent: 2.82; lr: 1.00000; 8472/8652 tok/s;     82 sec\n","[2022-02-27 20:27:07,533 INFO] Step 2200/10000; acc:  53.25; ppl: 14.21; xent: 2.65; lr: 1.00000; 8455/8582 tok/s;     83 sec\n","[2022-02-27 20:27:08,276 INFO] Step 2220/10000; acc:  51.27; ppl: 15.79; xent: 2.76; lr: 1.00000; 8946/9274 tok/s;     84 sec\n","[2022-02-27 20:27:09,065 INFO] Step 2240/10000; acc:  48.76; ppl: 17.97; xent: 2.89; lr: 1.00000; 9164/9292 tok/s;     84 sec\n","[2022-02-27 20:27:09,835 INFO] Step 2260/10000; acc:  54.01; ppl: 12.72; xent: 2.54; lr: 1.00000; 7920/8311 tok/s;     85 sec\n","[2022-02-27 20:27:10,552 INFO] Step 2280/10000; acc:  52.45; ppl: 13.91; xent: 2.63; lr: 1.00000; 8954/9108 tok/s;     86 sec\n","[2022-02-27 20:27:11,297 INFO] Step 2300/10000; acc:  50.45; ppl: 16.33; xent: 2.79; lr: 1.00000; 8957/9156 tok/s;     87 sec\n","[2022-02-27 20:27:12,051 INFO] Step 2320/10000; acc:  52.47; ppl: 14.37; xent: 2.67; lr: 1.00000; 9118/8825 tok/s;     87 sec\n","[2022-02-27 20:27:12,699 INFO] Step 2340/10000; acc:  53.59; ppl: 13.49; xent: 2.60; lr: 1.00000; 8716/9285 tok/s;     88 sec\n","[2022-02-27 20:27:13,544 INFO] Step 2360/10000; acc:  49.00; ppl: 17.63; xent: 2.87; lr: 1.00000; 8900/9203 tok/s;     89 sec\n","[2022-02-27 20:27:14,298 INFO] Step 2380/10000; acc:  51.06; ppl: 15.02; xent: 2.71; lr: 1.00000; 8820/9082 tok/s;     90 sec\n","[2022-02-27 20:27:14,939 INFO] Step 2400/10000; acc:  57.86; ppl: 10.36; xent: 2.34; lr: 1.00000; 8066/9197 tok/s;     90 sec\n","[2022-02-27 20:27:15,676 INFO] Step 2420/10000; acc:  51.52; ppl: 14.29; xent: 2.66; lr: 1.00000; 8904/9315 tok/s;     91 sec\n","[2022-02-27 20:27:16,487 INFO] Step 2440/10000; acc:  50.66; ppl: 16.15; xent: 2.78; lr: 1.00000; 8606/8746 tok/s;     92 sec\n","[2022-02-27 20:27:17,109 INFO] Step 2460/10000; acc:  57.71; ppl:  9.97; xent: 2.30; lr: 1.00000; 8531/9414 tok/s;     92 sec\n","[2022-02-27 20:27:17,864 INFO] Step 2480/10000; acc:  51.50; ppl: 14.34; xent: 2.66; lr: 1.00000; 9067/9507 tok/s;     93 sec\n","[2022-02-27 20:27:18,510 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 5\n","[2022-02-27 20:27:18,675 INFO] Step 2500/10000; acc:  52.38; ppl: 15.21; xent: 2.72; lr: 1.00000; 8009/8213 tok/s;     94 sec\n","[2022-02-27 20:27:18,926 INFO] Validation perplexity: 8.50203\n","[2022-02-27 20:27:18,926 INFO] Validation accuracy: 61.6829\n","[2022-02-27 20:27:18,926 INFO] Model is improving ppl: 10.0947 --> 8.50203.\n","[2022-02-27 20:27:18,927 INFO] Model is improving acc: 59.5967 --> 61.6829.\n","[2022-02-27 20:27:19,615 INFO] Step 2520/10000; acc:  55.04; ppl: 12.11; xent: 2.49; lr: 1.00000; 6441/6963 tok/s;     95 sec\n","[2022-02-27 20:27:20,339 INFO] Step 2540/10000; acc:  52.45; ppl: 13.41; xent: 2.60; lr: 1.00000; 8944/9263 tok/s;     96 sec\n","[2022-02-27 20:27:21,189 INFO] Step 2560/10000; acc:  51.35; ppl: 15.37; xent: 2.73; lr: 1.00000; 8396/8643 tok/s;     97 sec\n","[2022-02-27 20:27:21,969 INFO] Step 2580/10000; acc:  55.91; ppl: 11.74; xent: 2.46; lr: 1.00000; 8020/8069 tok/s;     97 sec\n","[2022-02-27 20:27:22,686 INFO] Step 2600/10000; acc:  53.51; ppl: 13.16; xent: 2.58; lr: 1.00000; 8978/9285 tok/s;     98 sec\n","[2022-02-27 20:27:23,470 INFO] Step 2620/10000; acc:  52.63; ppl: 14.22; xent: 2.65; lr: 1.00000; 8558/8913 tok/s;     99 sec\n","[2022-02-27 20:27:24,221 INFO] Step 2640/10000; acc:  53.40; ppl: 13.18; xent: 2.58; lr: 1.00000; 9007/9243 tok/s;    100 sec\n","[2022-02-27 20:27:24,885 INFO] Step 2660/10000; acc:  56.77; ppl: 11.32; xent: 2.43; lr: 1.00000; 8394/9129 tok/s;    100 sec\n","[2022-02-27 20:27:25,698 INFO] Step 2680/10000; acc:  51.02; ppl: 14.50; xent: 2.67; lr: 1.00000; 9075/9509 tok/s;    101 sec\n","[2022-02-27 20:27:26,439 INFO] Step 2700/10000; acc:  54.09; ppl: 12.64; xent: 2.54; lr: 1.00000; 8863/9093 tok/s;    102 sec\n","[2022-02-27 20:27:27,067 INFO] Step 2720/10000; acc:  58.29; ppl:  9.73; xent: 2.28; lr: 1.00000; 8281/9133 tok/s;    102 sec\n","[2022-02-27 20:27:27,795 INFO] Step 2740/10000; acc:  55.29; ppl: 11.21; xent: 2.42; lr: 1.00000; 8838/9365 tok/s;    103 sec\n","[2022-02-27 20:27:28,626 INFO] Step 2760/10000; acc:  52.35; ppl: 14.51; xent: 2.67; lr: 1.00000; 8353/8796 tok/s;    104 sec\n","[2022-02-27 20:27:29,251 INFO] Step 2780/10000; acc:  58.63; ppl:  9.52; xent: 2.25; lr: 1.00000; 8537/9393 tok/s;    105 sec\n","[2022-02-27 20:27:30,032 INFO] Step 2800/10000; acc:  54.25; ppl: 12.85; xent: 2.55; lr: 1.00000; 8867/9123 tok/s;    105 sec\n","[2022-02-27 20:27:30,812 INFO] Step 2820/10000; acc:  53.42; ppl: 13.34; xent: 2.59; lr: 1.00000; 8481/8659 tok/s;    106 sec\n","[2022-02-27 20:27:31,544 INFO] Step 2840/10000; acc:  55.79; ppl: 10.68; xent: 2.37; lr: 1.00000; 8467/8738 tok/s;    107 sec\n","[2022-02-27 20:27:32,285 INFO] Step 2860/10000; acc:  54.01; ppl: 12.01; xent: 2.49; lr: 1.00000; 8950/9181 tok/s;    108 sec\n","[2022-02-27 20:27:33,058 INFO] Step 2880/10000; acc:  52.54; ppl: 13.56; xent: 2.61; lr: 1.00000; 9307/9253 tok/s;    108 sec\n","[2022-02-27 20:27:33,803 INFO] Step 2900/10000; acc:  56.65; ppl:  9.88; xent: 2.29; lr: 1.00000; 8376/8471 tok/s;    109 sec\n","[2022-02-27 20:27:34,513 INFO] Step 2920/10000; acc:  53.89; ppl: 12.69; xent: 2.54; lr: 1.00000; 9193/9127 tok/s;    110 sec\n","[2022-02-27 20:27:35,279 INFO] Step 2940/10000; acc:  52.67; ppl: 13.93; xent: 2.63; lr: 1.00000; 8808/9187 tok/s;    111 sec\n","[2022-02-27 20:27:36,063 INFO] Step 2960/10000; acc:  54.23; ppl: 11.35; xent: 2.43; lr: 1.00000; 8519/9017 tok/s;    111 sec\n","[2022-02-27 20:27:36,723 INFO] Step 2980/10000; acc:  57.44; ppl:  9.98; xent: 2.30; lr: 1.00000; 8398/8816 tok/s;    112 sec\n","[2022-02-27 20:27:37,569 INFO] Step 3000/10000; acc:  51.95; ppl: 13.59; xent: 2.61; lr: 1.00000; 8644/8996 tok/s;    113 sec\n","[2022-02-27 20:27:37,640 INFO] Saving checkpoint ./Question10/model_step_3000.pt\n","[2022-02-27 20:27:38,578 INFO] Step 3020/10000; acc:  53.88; ppl: 11.91; xent: 2.48; lr: 1.00000; 6539/6692 tok/s;    114 sec\n","[2022-02-27 20:27:39,207 INFO] Step 3040/10000; acc:  60.02; ppl:  8.61; xent: 2.15; lr: 1.00000; 8287/9279 tok/s;    115 sec\n","[2022-02-27 20:27:39,933 INFO] Step 3060/10000; acc:  54.92; ppl: 11.10; xent: 2.41; lr: 1.00000; 9044/9501 tok/s;    115 sec\n","[2022-02-27 20:27:40,452 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 6\n","[2022-02-27 20:27:40,752 INFO] Step 3080/10000; acc:  53.28; ppl: 12.67; xent: 2.54; lr: 1.00000; 8548/8934 tok/s;    116 sec\n","[2022-02-27 20:27:41,407 INFO] Step 3100/10000; acc:  59.85; ppl:  8.54; xent: 2.15; lr: 1.00000; 8148/8851 tok/s;    117 sec\n","[2022-02-27 20:27:42,169 INFO] Step 3120/10000; acc:  54.69; ppl: 11.79; xent: 2.47; lr: 1.00000; 9072/9396 tok/s;    118 sec\n","[2022-02-27 20:27:42,573 INFO] Validation perplexity: 7.47864\n","[2022-02-27 20:27:42,574 INFO] Validation accuracy: 64.1632\n","[2022-02-27 20:27:42,574 INFO] Model is improving ppl: 8.50203 --> 7.47864.\n","[2022-02-27 20:27:42,574 INFO] Model is improving acc: 61.6829 --> 64.1632.\n","[2022-02-27 20:27:43,186 INFO] Step 3140/10000; acc:  53.98; ppl: 12.84; xent: 2.55; lr: 1.00000; 6439/6512 tok/s;    119 sec\n","[2022-02-27 20:27:43,871 INFO] Step 3160/10000; acc:  57.02; ppl:  9.90; xent: 2.29; lr: 1.00000; 8788/9649 tok/s;    119 sec\n","[2022-02-27 20:27:44,599 INFO] Step 3180/10000; acc:  54.63; ppl: 10.90; xent: 2.39; lr: 1.00000; 8834/9153 tok/s;    120 sec\n","[2022-02-27 20:27:45,443 INFO] Step 3200/10000; acc:  54.24; ppl: 11.99; xent: 2.48; lr: 1.00000; 8399/8457 tok/s;    121 sec\n","[2022-02-27 20:27:46,183 INFO] Step 3220/10000; acc:  58.17; ppl:  9.75; xent: 2.28; lr: 1.00000; 8507/8497 tok/s;    122 sec\n","[2022-02-27 20:27:46,923 INFO] Step 3240/10000; acc:  55.57; ppl: 11.37; xent: 2.43; lr: 1.00000; 8832/9025 tok/s;    122 sec\n","[2022-02-27 20:27:47,749 INFO] Step 3260/10000; acc:  53.93; ppl: 12.42; xent: 2.52; lr: 1.00000; 8245/8724 tok/s;    123 sec\n","[2022-02-27 20:27:48,528 INFO] Step 3280/10000; acc:  55.72; ppl: 10.55; xent: 2.36; lr: 1.00000; 8454/9250 tok/s;    124 sec\n","[2022-02-27 20:27:49,171 INFO] Step 3300/10000; acc:  58.87; ppl:  9.03; xent: 2.20; lr: 1.00000; 8559/8973 tok/s;    125 sec\n","[2022-02-27 20:27:49,965 INFO] Step 3320/10000; acc:  53.28; ppl: 12.49; xent: 2.52; lr: 1.00000; 9112/9553 tok/s;    125 sec\n","[2022-02-27 20:27:50,692 INFO] Step 3340/10000; acc:  56.31; ppl: 10.25; xent: 2.33; lr: 1.00000; 9026/9298 tok/s;    126 sec\n","[2022-02-27 20:27:51,335 INFO] Step 3360/10000; acc:  61.17; ppl:  7.80; xent: 2.05; lr: 1.00000; 8149/9005 tok/s;    127 sec\n","[2022-02-27 20:27:52,059 INFO] Step 3380/10000; acc:  57.81; ppl:  9.46; xent: 2.25; lr: 1.00000; 8956/9350 tok/s;    127 sec\n","[2022-02-27 20:27:52,888 INFO] Step 3400/10000; acc:  53.60; ppl: 12.13; xent: 2.50; lr: 1.00000; 8492/8730 tok/s;    128 sec\n","[2022-02-27 20:27:53,519 INFO] Step 3420/10000; acc:  61.94; ppl:  7.21; xent: 1.98; lr: 1.00000; 8511/9502 tok/s;    129 sec\n","[2022-02-27 20:27:54,295 INFO] Step 3440/10000; acc:  54.29; ppl: 11.33; xent: 2.43; lr: 1.00000; 9053/9131 tok/s;    130 sec\n","[2022-02-27 20:27:55,108 INFO] Step 3460/10000; acc:  54.69; ppl: 11.62; xent: 2.45; lr: 1.00000; 8249/8334 tok/s;    130 sec\n","[2022-02-27 20:27:55,819 INFO] Step 3480/10000; acc:  58.95; ppl:  8.15; xent: 2.10; lr: 1.00000; 8590/9141 tok/s;    131 sec\n","[2022-02-27 20:27:56,554 INFO] Step 3500/10000; acc:  56.31; ppl: 10.03; xent: 2.31; lr: 1.00000; 8906/9072 tok/s;    132 sec\n","[2022-02-27 20:27:57,327 INFO] Step 3520/10000; acc:  54.90; ppl: 11.45; xent: 2.44; lr: 1.00000; 9140/8985 tok/s;    133 sec\n","[2022-02-27 20:27:58,069 INFO] Step 3540/10000; acc:  58.31; ppl:  8.84; xent: 2.18; lr: 1.00000; 8565/8278 tok/s;    133 sec\n","[2022-02-27 20:27:58,774 INFO] Step 3560/10000; acc:  55.88; ppl: 10.43; xent: 2.34; lr: 1.00000; 9356/9565 tok/s;    134 sec\n","[2022-02-27 20:27:59,581 INFO] Step 3580/10000; acc:  54.22; ppl: 11.43; xent: 2.44; lr: 1.00000; 8553/8940 tok/s;    135 sec\n","[2022-02-27 20:28:00,347 INFO] Step 3600/10000; acc:  57.35; ppl:  8.99; xent: 2.20; lr: 1.00000; 8732/9348 tok/s;    136 sec\n","[2022-02-27 20:28:00,989 INFO] Step 3620/10000; acc:  59.12; ppl:  8.57; xent: 2.15; lr: 1.00000; 8600/9110 tok/s;    136 sec\n","[2022-02-27 20:28:01,808 INFO] Step 3640/10000; acc:  53.05; ppl: 11.72; xent: 2.46; lr: 1.00000; 8790/9310 tok/s;    137 sec\n","[2022-02-27 20:28:02,562 INFO] Step 3660/10000; acc:  57.22; ppl:  9.57; xent: 2.26; lr: 1.00000; 8706/8890 tok/s;    138 sec\n","[2022-02-27 20:28:03,198 INFO] Step 3680/10000; acc:  62.30; ppl:  6.83; xent: 1.92; lr: 1.00000; 8066/9204 tok/s;    139 sec\n","[2022-02-27 20:28:03,897 INFO] Step 3700/10000; acc:  56.35; ppl:  9.64; xent: 2.27; lr: 1.00000; 9334/9706 tok/s;    139 sec\n","[2022-02-27 20:28:04,441 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 7\n","[2022-02-27 20:28:04,763 INFO] Step 3720/10000; acc:  54.61; ppl: 10.95; xent: 2.39; lr: 1.00000; 8103/8467 tok/s;    140 sec\n","[2022-02-27 20:28:05,397 INFO] Step 3740/10000; acc:  61.13; ppl:  7.66; xent: 2.04; lr: 1.00000; 8470/9037 tok/s;    141 sec\n","[2022-02-27 20:28:06,039 INFO] Validation perplexity: 6.78608\n","[2022-02-27 20:28:06,040 INFO] Validation accuracy: 65.2527\n","[2022-02-27 20:28:06,040 INFO] Model is improving ppl: 7.47864 --> 6.78608.\n","[2022-02-27 20:28:06,040 INFO] Model is improving acc: 64.1632 --> 65.2527.\n","[2022-02-27 20:28:06,400 INFO] Step 3760/10000; acc:  55.76; ppl: 10.30; xent: 2.33; lr: 1.00000; 6927/7159 tok/s;    142 sec\n","[2022-02-27 20:28:07,187 INFO] Step 3780/10000; acc:  54.58; ppl: 10.87; xent: 2.39; lr: 1.00000; 8310/8417 tok/s;    143 sec\n","[2022-02-27 20:28:07,887 INFO] Step 3800/10000; acc:  59.46; ppl:  7.85; xent: 2.06; lr: 1.00000; 8721/9394 tok/s;    143 sec\n","[2022-02-27 20:28:08,605 INFO] Step 3820/10000; acc:  57.36; ppl:  9.19; xent: 2.22; lr: 1.00000; 9071/9289 tok/s;    144 sec\n","[2022-02-27 20:28:09,456 INFO] Step 3840/10000; acc:  55.35; ppl: 10.77; xent: 2.38; lr: 1.00000; 8399/8560 tok/s;    145 sec\n","[2022-02-27 20:28:10,220 INFO] Step 3860/10000; acc:  60.49; ppl:  7.99; xent: 2.08; lr: 1.00000; 8190/8375 tok/s;    146 sec\n","[2022-02-27 20:28:10,968 INFO] Step 3880/10000; acc:  56.55; ppl: 10.09; xent: 2.31; lr: 1.00000; 8724/8970 tok/s;    146 sec\n","[2022-02-27 20:28:11,746 INFO] Step 3900/10000; acc:  55.11; ppl: 10.70; xent: 2.37; lr: 1.00000; 8781/9131 tok/s;    147 sec\n","[2022-02-27 20:28:12,495 INFO] Step 3920/10000; acc:  58.50; ppl:  8.61; xent: 2.15; lr: 1.00000; 8701/9285 tok/s;    148 sec\n","[2022-02-27 20:28:13,163 INFO] Step 3940/10000; acc:  60.47; ppl:  7.49; xent: 2.01; lr: 1.00000; 8009/8896 tok/s;    149 sec\n","[2022-02-27 20:28:13,937 INFO] Step 3960/10000; acc:  56.45; ppl: 10.05; xent: 2.31; lr: 1.00000; 9165/9468 tok/s;    149 sec\n","[2022-02-27 20:28:14,674 INFO] Step 3980/10000; acc:  58.42; ppl:  8.35; xent: 2.12; lr: 1.00000; 8798/9466 tok/s;    150 sec\n","[2022-02-27 20:28:15,319 INFO] Step 4000/10000; acc:  63.93; ppl:  6.21; xent: 1.83; lr: 1.00000; 8211/8966 tok/s;    151 sec\n","[2022-02-27 20:28:15,387 INFO] Saving checkpoint ./Question10/model_step_4000.pt\n","[2022-02-27 20:28:16,330 INFO] Step 4020/10000; acc:  59.73; ppl:  8.29; xent: 2.12; lr: 1.00000; 6472/6667 tok/s;    152 sec\n","[2022-02-27 20:28:17,177 INFO] Step 4040/10000; acc:  55.32; ppl: 10.63; xent: 2.36; lr: 1.00000; 8396/8558 tok/s;    153 sec\n","[2022-02-27 20:28:17,829 INFO] Step 4060/10000; acc:  62.13; ppl:  6.51; xent: 1.87; lr: 1.00000; 8353/9193 tok/s;    153 sec\n","[2022-02-27 20:28:18,596 INFO] Step 4080/10000; acc:  57.26; ppl:  9.13; xent: 2.21; lr: 1.00000; 9198/9425 tok/s;    154 sec\n","[2022-02-27 20:28:19,394 INFO] Step 4100/10000; acc:  56.31; ppl: 10.03; xent: 2.31; lr: 1.00000; 8514/8327 tok/s;    155 sec\n","[2022-02-27 20:28:20,111 INFO] Step 4120/10000; acc:  60.91; ppl:  7.21; xent: 1.98; lr: 1.00000; 8517/9142 tok/s;    155 sec\n","[2022-02-27 20:28:20,849 INFO] Step 4140/10000; acc:  56.74; ppl:  9.10; xent: 2.21; lr: 1.00000; 8921/9075 tok/s;    156 sec\n","[2022-02-27 20:28:21,625 INFO] Step 4160/10000; acc:  55.70; ppl: 10.19; xent: 2.32; lr: 1.00000; 9219/9007 tok/s;    157 sec\n","[2022-02-27 20:28:22,429 INFO] Step 4180/10000; acc:  60.74; ppl:  7.18; xent: 1.97; lr: 1.00000; 7770/7822 tok/s;    158 sec\n","[2022-02-27 20:28:23,153 INFO] Step 4200/10000; acc:  57.06; ppl:  9.23; xent: 2.22; lr: 1.00000; 9023/9025 tok/s;    159 sec\n","[2022-02-27 20:28:23,951 INFO] Step 4220/10000; acc:  56.08; ppl:  9.72; xent: 2.27; lr: 1.00000; 8463/8863 tok/s;    159 sec\n","[2022-02-27 20:28:24,715 INFO] Step 4240/10000; acc:  57.82; ppl:  8.06; xent: 2.09; lr: 1.00000; 8707/9224 tok/s;    160 sec\n","[2022-02-27 20:28:25,377 INFO] Step 4260/10000; acc:  62.11; ppl:  7.20; xent: 1.97; lr: 1.00000; 8289/9003 tok/s;    161 sec\n","[2022-02-27 20:28:26,185 INFO] Step 4280/10000; acc:  55.45; ppl: 10.09; xent: 2.31; lr: 1.00000; 8966/9400 tok/s;    162 sec\n","[2022-02-27 20:28:26,931 INFO] Step 4300/10000; acc:  58.89; ppl:  7.88; xent: 2.06; lr: 1.00000; 8777/8992 tok/s;    162 sec\n","[2022-02-27 20:28:27,556 INFO] Step 4320/10000; acc:  63.72; ppl:  5.94; xent: 1.78; lr: 1.00000; 8200/9258 tok/s;    163 sec\n","[2022-02-27 20:28:28,281 INFO] Step 4340/10000; acc:  57.76; ppl:  8.34; xent: 2.12; lr: 1.00000; 8957/9351 tok/s;    164 sec\n","[2022-02-27 20:28:28,808 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 8\n","[2022-02-27 20:28:29,145 INFO] Step 4360/10000; acc:  57.83; ppl:  8.84; xent: 2.18; lr: 1.00000; 8048/8415 tok/s;    165 sec\n","[2022-02-27 20:28:29,906 INFO] Validation perplexity: 6.43141\n","[2022-02-27 20:28:29,906 INFO] Validation accuracy: 65.9481\n","[2022-02-27 20:28:29,907 INFO] Model is improving ppl: 6.78608 --> 6.43141.\n","[2022-02-27 20:28:29,907 INFO] Model is improving acc: 65.2527 --> 65.9481.\n","[2022-02-27 20:28:30,019 INFO] Step 4380/10000; acc:  62.39; ppl:  6.51; xent: 1.87; lr: 1.00000; 6170/6597 tok/s;    165 sec\n","[2022-02-27 20:28:30,791 INFO] Step 4400/10000; acc:  57.07; ppl:  8.98; xent: 2.19; lr: 1.00000; 9087/9332 tok/s;    166 sec\n","[2022-02-27 20:28:31,604 INFO] Step 4420/10000; acc:  57.56; ppl:  9.17; xent: 2.22; lr: 1.00000; 8201/8344 tok/s;    167 sec\n","[2022-02-27 20:28:32,325 INFO] Step 4440/10000; acc:  60.75; ppl:  7.19; xent: 1.97; lr: 1.00000; 8536/9144 tok/s;    168 sec\n","[2022-02-27 20:28:33,047 INFO] Step 4460/10000; acc:  59.55; ppl:  7.90; xent: 2.07; lr: 1.00000; 9063/9388 tok/s;    168 sec\n","[2022-02-27 20:28:33,892 INFO] Step 4480/10000; acc:  55.64; ppl:  9.79; xent: 2.28; lr: 1.00000; 8506/8668 tok/s;    169 sec\n","[2022-02-27 20:28:34,647 INFO] Step 4500/10000; acc:  61.58; ppl:  6.96; xent: 1.94; lr: 1.00000; 8302/8412 tok/s;    170 sec\n","[2022-02-27 20:28:35,384 INFO] Step 4520/10000; acc:  58.16; ppl:  8.48; xent: 2.14; lr: 1.00000; 8907/9164 tok/s;    171 sec\n","[2022-02-27 20:28:36,150 INFO] Step 4540/10000; acc:  57.44; ppl:  9.62; xent: 2.26; lr: 1.00000; 8893/9370 tok/s;    172 sec\n","[2022-02-27 20:28:36,906 INFO] Step 4560/10000; acc:  60.45; ppl:  7.53; xent: 2.02; lr: 1.00000; 8600/9186 tok/s;    172 sec\n","[2022-02-27 20:28:37,562 INFO] Step 4580/10000; acc:  63.13; ppl:  6.29; xent: 1.84; lr: 1.00000; 8114/8957 tok/s;    173 sec\n","[2022-02-27 20:28:38,334 INFO] Step 4600/10000; acc:  58.51; ppl:  8.36; xent: 2.12; lr: 1.00000; 9033/9378 tok/s;    174 sec\n","[2022-02-27 20:28:39,077 INFO] Step 4620/10000; acc:  60.19; ppl:  7.00; xent: 1.95; lr: 1.00000; 8658/9200 tok/s;    174 sec\n","[2022-02-27 20:28:39,704 INFO] Step 4640/10000; acc:  64.19; ppl:  5.72; xent: 1.74; lr: 1.00000; 8274/9186 tok/s;    175 sec\n","[2022-02-27 20:28:40,432 INFO] Step 4660/10000; acc:  61.01; ppl:  7.22; xent: 1.98; lr: 1.00000; 8911/9384 tok/s;    176 sec\n","[2022-02-27 20:28:41,234 INFO] Step 4680/10000; acc:  56.87; ppl:  9.39; xent: 2.24; lr: 1.00000; 8998/8811 tok/s;    177 sec\n","[2022-02-27 20:28:41,897 INFO] Step 4700/10000; acc:  63.96; ppl:  5.97; xent: 1.79; lr: 1.00000; 8405/8896 tok/s;    177 sec\n","[2022-02-27 20:28:42,677 INFO] Step 4720/10000; acc:  57.98; ppl:  8.37; xent: 2.12; lr: 1.00000; 9188/9411 tok/s;    178 sec\n","[2022-02-27 20:28:43,467 INFO] Step 4740/10000; acc:  58.01; ppl:  8.61; xent: 2.15; lr: 1.00000; 8657/8730 tok/s;    179 sec\n","[2022-02-27 20:28:44,170 INFO] Step 4760/10000; acc:  62.53; ppl:  6.23; xent: 1.83; lr: 1.00000; 8673/9269 tok/s;    180 sec\n","[2022-02-27 20:28:44,893 INFO] Step 4780/10000; acc:  58.75; ppl:  7.69; xent: 2.04; lr: 1.00000; 9104/9268 tok/s;    180 sec\n","[2022-02-27 20:28:45,666 INFO] Step 4800/10000; acc:  57.28; ppl:  8.84; xent: 2.18; lr: 1.00000; 9220/8998 tok/s;    181 sec\n","[2022-02-27 20:28:46,410 INFO] Step 4820/10000; acc:  61.72; ppl:  6.44; xent: 1.86; lr: 1.00000; 8421/8541 tok/s;    182 sec\n","[2022-02-27 20:28:47,121 INFO] Step 4840/10000; acc:  58.52; ppl:  8.21; xent: 2.11; lr: 1.00000; 9202/9388 tok/s;    182 sec\n","[2022-02-27 20:28:47,931 INFO] Step 4860/10000; acc:  57.31; ppl:  8.59; xent: 2.15; lr: 1.00000; 8369/8770 tok/s;    183 sec\n","[2022-02-27 20:28:48,680 INFO] Step 4880/10000; acc:  60.30; ppl:  7.04; xent: 1.95; lr: 1.00000; 8807/9270 tok/s;    184 sec\n","[2022-02-27 20:28:49,349 INFO] Step 4900/10000; acc:  62.73; ppl:  6.18; xent: 1.82; lr: 1.00000; 8055/9006 tok/s;    185 sec\n","[2022-02-27 20:28:50,131 INFO] Step 4920/10000; acc:  57.08; ppl:  8.79; xent: 2.17; lr: 1.00000; 9149/9441 tok/s;    185 sec\n","[2022-02-27 20:28:50,868 INFO] Step 4940/10000; acc:  60.47; ppl:  6.87; xent: 1.93; lr: 1.00000; 8811/9031 tok/s;    186 sec\n","[2022-02-27 20:28:51,498 INFO] Step 4960/10000; acc:  64.88; ppl:  5.35; xent: 1.68; lr: 1.00000; 8202/9300 tok/s;    187 sec\n","[2022-02-27 20:28:52,225 INFO] Step 4980/10000; acc:  59.35; ppl:  7.29; xent: 1.99; lr: 1.00000; 8930/9330 tok/s;    188 sec\n","[2022-02-27 20:28:52,748 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 9\n","[2022-02-27 20:28:53,081 INFO] Step 5000/10000; acc:  57.78; ppl:  8.69; xent: 2.16; lr: 1.00000; 8246/8427 tok/s;    188 sec\n","[2022-02-27 20:28:53,345 INFO] Validation perplexity: 6.08573\n","[2022-02-27 20:28:53,346 INFO] Validation accuracy: 66.1567\n","[2022-02-27 20:28:53,346 INFO] Model is improving ppl: 6.43141 --> 6.08573.\n","[2022-02-27 20:28:53,346 INFO] Model is improving acc: 65.9481 --> 66.1567.\n","[2022-02-27 20:28:53,405 INFO] Saving checkpoint ./Question10/model_step_5000.pt\n","[2022-02-27 20:28:54,233 INFO] Step 5020/10000; acc:  63.98; ppl:  5.76; xent: 1.75; lr: 1.00000; 4680/5062 tok/s;    190 sec\n","[2022-02-27 20:28:54,991 INFO] Step 5040/10000; acc:  58.86; ppl:  7.66; xent: 2.04; lr: 1.00000; 9282/9479 tok/s;    190 sec\n","[2022-02-27 20:28:55,844 INFO] Step 5060/10000; acc:  57.89; ppl:  8.59; xent: 2.15; lr: 1.00000; 7914/8188 tok/s;    191 sec\n","[2022-02-27 20:28:56,556 INFO] Step 5080/10000; acc:  63.13; ppl:  6.14; xent: 1.81; lr: 1.00000; 8782/8969 tok/s;    192 sec\n","[2022-02-27 20:28:57,294 INFO] Step 5100/10000; acc:  59.77; ppl:  7.69; xent: 2.04; lr: 1.00000; 8918/9444 tok/s;    193 sec\n","[2022-02-27 20:28:58,101 INFO] Step 5120/10000; acc:  57.00; ppl:  8.71; xent: 2.16; lr: 1.00000; 8883/9039 tok/s;    193 sec\n","[2022-02-27 20:28:58,857 INFO] Step 5140/10000; acc:  62.55; ppl:  6.03; xent: 1.80; lr: 1.00000; 8136/8423 tok/s;    194 sec\n","[2022-02-27 20:28:59,589 INFO] Step 5160/10000; acc:  60.64; ppl:  7.07; xent: 1.96; lr: 1.00000; 8755/9243 tok/s;    195 sec\n","[2022-02-27 20:29:00,346 INFO] Step 5180/10000; acc:  58.79; ppl:  8.01; xent: 2.08; lr: 1.00000; 8807/9273 tok/s;    196 sec\n","[2022-02-27 20:29:01,098 INFO] Step 5200/10000; acc:  62.19; ppl:  6.26; xent: 1.83; lr: 1.00000; 8705/9035 tok/s;    196 sec\n","[2022-02-27 20:29:01,747 INFO] Step 5220/10000; acc:  64.01; ppl:  5.64; xent: 1.73; lr: 1.00000; 8260/9090 tok/s;    197 sec\n","[2022-02-27 20:29:02,632 INFO] Step 5240/10000; acc:  59.14; ppl:  7.95; xent: 2.07; lr: 1.00000; 7963/8290 tok/s;    198 sec\n","[2022-02-27 20:29:03,382 INFO] Step 5260/10000; acc:  62.47; ppl:  6.35; xent: 1.85; lr: 1.00000; 8636/9212 tok/s;    199 sec\n","[2022-02-27 20:29:04,032 INFO] Step 5280/10000; acc:  66.21; ppl:  5.09; xent: 1.63; lr: 1.00000; 8043/8941 tok/s;    199 sec\n","[2022-02-27 20:29:04,761 INFO] Step 5300/10000; acc:  61.21; ppl:  6.78; xent: 1.91; lr: 1.00000; 8982/9361 tok/s;    200 sec\n","[2022-02-27 20:29:05,594 INFO] Step 5320/10000; acc:  58.21; ppl:  8.39; xent: 2.13; lr: 1.00000; 8688/8488 tok/s;    201 sec\n","[2022-02-27 20:29:06,259 INFO] Step 5340/10000; acc:  65.78; ppl:  5.22; xent: 1.65; lr: 1.00000; 8273/9000 tok/s;    202 sec\n","[2022-02-27 20:29:07,031 INFO] Step 5360/10000; acc:  59.03; ppl:  7.40; xent: 2.00; lr: 1.00000; 9251/9334 tok/s;    202 sec\n","[2022-02-27 20:29:07,817 INFO] Step 5380/10000; acc:  60.14; ppl:  7.42; xent: 2.00; lr: 1.00000; 8696/8552 tok/s;    203 sec\n","[2022-02-27 20:29:08,523 INFO] Step 5400/10000; acc:  63.13; ppl:  5.73; xent: 1.75; lr: 1.00000; 8817/9211 tok/s;    204 sec\n","[2022-02-27 20:29:09,243 INFO] Step 5420/10000; acc:  59.19; ppl:  7.38; xent: 2.00; lr: 1.00000; 9285/9249 tok/s;    205 sec\n","[2022-02-27 20:29:10,014 INFO] Step 5440/10000; acc:  58.37; ppl:  8.33; xent: 2.12; lr: 1.00000; 9314/9414 tok/s;    205 sec\n","[2022-02-27 20:29:10,808 INFO] Step 5460/10000; acc:  62.57; ppl:  5.90; xent: 1.77; lr: 1.00000; 7643/8170 tok/s;    206 sec\n","[2022-02-27 20:29:11,527 INFO] Step 5480/10000; acc:  60.40; ppl:  6.85; xent: 1.92; lr: 1.00000; 8866/9114 tok/s;    207 sec\n","[2022-02-27 20:29:12,308 INFO] Step 5500/10000; acc:  59.12; ppl:  7.61; xent: 2.03; lr: 1.00000; 8470/8942 tok/s;    208 sec\n","[2022-02-27 20:29:13,070 INFO] Step 5520/10000; acc:  61.45; ppl:  6.33; xent: 1.84; lr: 1.00000; 8622/8982 tok/s;    208 sec\n","[2022-02-27 20:29:13,721 INFO] Step 5540/10000; acc:  64.87; ppl:  5.35; xent: 1.68; lr: 1.00000; 8341/9180 tok/s;    209 sec\n","[2022-02-27 20:29:14,511 INFO] Step 5560/10000; acc:  59.31; ppl:  7.51; xent: 2.02; lr: 1.00000; 9071/9319 tok/s;    210 sec\n","[2022-02-27 20:29:14,819 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 10\n","[2022-02-27 20:29:15,268 INFO] Step 5580/10000; acc:  62.11; ppl:  6.06; xent: 1.80; lr: 1.00000; 8604/9157 tok/s;    211 sec\n","[2022-02-27 20:29:15,925 INFO] Step 5600/10000; acc:  66.40; ppl:  4.71; xent: 1.55; lr: 1.00000; 7913/8821 tok/s;    211 sec\n","[2022-02-27 20:29:16,646 INFO] Step 5620/10000; acc:  60.35; ppl:  6.88; xent: 1.93; lr: 1.00000; 9095/9449 tok/s;    212 sec\n","[2022-02-27 20:29:17,181 INFO] Validation perplexity: 5.9934\n","[2022-02-27 20:29:17,183 INFO] Validation accuracy: 67.5707\n","[2022-02-27 20:29:17,183 INFO] Model is improving ppl: 6.08573 --> 5.9934.\n","[2022-02-27 20:29:17,183 INFO] Model is improving acc: 66.1567 --> 67.5707.\n","[2022-02-27 20:29:17,714 INFO] Step 5640/10000; acc:  57.60; ppl:  8.41; xent: 2.13; lr: 1.00000; 6665/6762 tok/s;    213 sec\n","[2022-02-27 20:29:18,337 INFO] Step 5660/10000; acc:  66.54; ppl:  4.93; xent: 1.60; lr: 1.00000; 8640/9401 tok/s;    214 sec\n","[2022-02-27 20:29:19,120 INFO] Step 5680/10000; acc:  59.46; ppl:  7.07; xent: 1.96; lr: 1.00000; 8927/9026 tok/s;    214 sec\n","[2022-02-27 20:29:19,953 INFO] Step 5700/10000; acc:  60.29; ppl:  6.98; xent: 1.94; lr: 1.00000; 8048/8192 tok/s;    215 sec\n","[2022-02-27 20:29:20,652 INFO] Step 5720/10000; acc:  64.08; ppl:  5.58; xent: 1.72; lr: 1.00000; 8996/9195 tok/s;    216 sec\n","[2022-02-27 20:29:21,415 INFO] Step 5740/10000; acc:  60.91; ppl:  7.02; xent: 1.95; lr: 1.00000; 8747/9180 tok/s;    217 sec\n","[2022-02-27 20:29:22,211 INFO] Step 5760/10000; acc:  57.99; ppl:  8.26; xent: 2.11; lr: 1.00000; 9136/9282 tok/s;    218 sec\n","[2022-02-27 20:29:22,987 INFO] Step 5780/10000; acc:  63.96; ppl:  5.54; xent: 1.71; lr: 1.00000; 7702/8484 tok/s;    218 sec\n","[2022-02-27 20:29:23,695 INFO] Step 5800/10000; acc:  61.35; ppl:  6.46; xent: 1.87; lr: 1.00000; 8922/9240 tok/s;    219 sec\n","[2022-02-27 20:29:24,439 INFO] Step 5820/10000; acc:  61.39; ppl:  6.74; xent: 1.91; lr: 1.00000; 8782/9302 tok/s;    220 sec\n","[2022-02-27 20:29:25,177 INFO] Step 5840/10000; acc:  62.20; ppl:  6.07; xent: 1.80; lr: 1.00000; 8898/9130 tok/s;    221 sec\n","[2022-02-27 20:29:25,819 INFO] Step 5860/10000; acc:  65.84; ppl:  4.90; xent: 1.59; lr: 1.00000; 8407/9213 tok/s;    221 sec\n","[2022-02-27 20:29:26,630 INFO] Step 5880/10000; acc:  59.59; ppl:  7.34; xent: 1.99; lr: 1.00000; 8744/9092 tok/s;    222 sec\n","[2022-02-27 20:29:27,357 INFO] Step 5900/10000; acc:  62.03; ppl:  5.86; xent: 1.77; lr: 1.00000; 9012/9453 tok/s;    223 sec\n","[2022-02-27 20:29:28,009 INFO] Step 5920/10000; acc:  67.98; ppl:  4.32; xent: 1.46; lr: 1.00000; 8103/8962 tok/s;    223 sec\n","[2022-02-27 20:29:28,721 INFO] Step 5940/10000; acc:  61.13; ppl:  6.42; xent: 1.86; lr: 1.00000; 9341/9530 tok/s;    224 sec\n","[2022-02-27 20:29:29,542 INFO] Step 5960/10000; acc:  58.75; ppl:  7.68; xent: 2.04; lr: 1.00000; 8888/8652 tok/s;    225 sec\n","[2022-02-27 20:29:30,197 INFO] Step 5980/10000; acc:  67.20; ppl:  4.53; xent: 1.51; lr: 1.00000; 8302/9246 tok/s;    226 sec\n","[2022-02-27 20:29:30,970 INFO] Step 6000/10000; acc:  61.10; ppl:  6.51; xent: 1.87; lr: 1.00000; 9148/9258 tok/s;    226 sec\n","[2022-02-27 20:29:31,029 INFO] Saving checkpoint ./Question10/model_step_6000.pt\n","[2022-02-27 20:29:32,039 INFO] Step 6020/10000; acc:  61.23; ppl:  6.75; xent: 1.91; lr: 1.00000; 6283/6149 tok/s;    227 sec\n","[2022-02-27 20:29:32,768 INFO] Step 6040/10000; acc:  63.48; ppl:  5.31; xent: 1.67; lr: 1.00000; 8682/8635 tok/s;    228 sec\n","[2022-02-27 20:29:33,494 INFO] Step 6060/10000; acc:  60.63; ppl:  6.91; xent: 1.93; lr: 1.00000; 9291/9490 tok/s;    229 sec\n","[2022-02-27 20:29:34,303 INFO] Step 6080/10000; acc:  58.86; ppl:  7.70; xent: 2.04; lr: 1.00000; 9046/9273 tok/s;    230 sec\n","[2022-02-27 20:29:35,078 INFO] Step 6100/10000; acc:  63.71; ppl:  5.18; xent: 1.64; lr: 1.00000; 7815/8430 tok/s;    230 sec\n","[2022-02-27 20:29:35,796 INFO] Step 6120/10000; acc:  62.28; ppl:  6.22; xent: 1.83; lr: 1.00000; 8821/9208 tok/s;    231 sec\n","[2022-02-27 20:29:36,529 INFO] Step 6140/10000; acc:  60.65; ppl:  6.67; xent: 1.90; lr: 1.00000; 8945/9442 tok/s;    232 sec\n","[2022-02-27 20:29:37,310 INFO] Step 6160/10000; acc:  63.15; ppl:  5.54; xent: 1.71; lr: 1.00000; 8333/8721 tok/s;    233 sec\n","[2022-02-27 20:29:37,979 INFO] Step 6180/10000; acc:  65.08; ppl:  5.05; xent: 1.62; lr: 1.00000; 7982/9125 tok/s;    233 sec\n","[2022-02-27 20:29:38,766 INFO] Step 6200/10000; acc:  60.39; ppl:  7.24; xent: 1.98; lr: 1.00000; 9125/9101 tok/s;    234 sec\n","[2022-02-27 20:29:39,078 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 11\n","[2022-02-27 20:29:39,552 INFO] Step 6220/10000; acc:  62.45; ppl:  5.84; xent: 1.76; lr: 1.00000; 8272/8859 tok/s;    235 sec\n","[2022-02-27 20:29:40,194 INFO] Step 6240/10000; acc:  67.76; ppl:  4.27; xent: 1.45; lr: 1.00000; 8192/8817 tok/s;    236 sec\n","[2022-02-27 20:29:40,847 INFO] Validation perplexity: 5.86904\n","[2022-02-27 20:29:40,847 INFO] Validation accuracy: 68.0575\n","[2022-02-27 20:29:40,848 INFO] Model is improving ppl: 5.9934 --> 5.86904.\n","[2022-02-27 20:29:40,848 INFO] Model is improving acc: 67.5707 --> 68.0575.\n","[2022-02-27 20:29:41,155 INFO] Step 6260/10000; acc:  60.75; ppl:  6.56; xent: 1.88; lr: 1.00000; 6845/7148 tok/s;    237 sec\n","[2022-02-27 20:29:41,987 INFO] Step 6280/10000; acc:  58.18; ppl:  7.52; xent: 2.02; lr: 1.00000; 8546/8557 tok/s;    237 sec\n","[2022-02-27 20:29:42,619 INFO] Step 6300/10000; acc:  67.00; ppl:  4.45; xent: 1.49; lr: 1.00000; 8618/9404 tok/s;    238 sec\n","[2022-02-27 20:29:43,398 INFO] Step 6320/10000; acc:  61.66; ppl:  6.27; xent: 1.84; lr: 1.00000; 9083/9183 tok/s;    239 sec\n","[2022-02-27 20:29:44,231 INFO] Step 6340/10000; acc:  61.21; ppl:  6.46; xent: 1.87; lr: 1.00000; 8127/8305 tok/s;    240 sec\n","[2022-02-27 20:29:44,944 INFO] Step 6360/10000; acc:  64.84; ppl:  5.16; xent: 1.64; lr: 1.00000; 8792/9040 tok/s;    240 sec\n","[2022-02-27 20:29:45,687 INFO] Step 6380/10000; acc:  60.55; ppl:  6.60; xent: 1.89; lr: 1.00000; 8972/9489 tok/s;    241 sec\n","[2022-02-27 20:29:46,473 INFO] Step 6400/10000; acc:  58.49; ppl:  7.82; xent: 2.06; lr: 1.00000; 9275/9397 tok/s;    242 sec\n","[2022-02-27 20:29:47,228 INFO] Step 6420/10000; acc:  65.04; ppl:  5.00; xent: 1.61; lr: 1.00000; 7769/8447 tok/s;    243 sec\n","[2022-02-27 20:29:47,953 INFO] Step 6440/10000; acc:  63.43; ppl:  5.49; xent: 1.70; lr: 1.00000; 8455/9091 tok/s;    243 sec\n","[2022-02-27 20:29:48,695 INFO] Step 6460/10000; acc:  62.72; ppl:  5.98; xent: 1.79; lr: 1.00000; 8617/9165 tok/s;    244 sec\n","[2022-02-27 20:29:49,429 INFO] Step 6480/10000; acc:  64.45; ppl:  5.36; xent: 1.68; lr: 1.00000; 8910/9534 tok/s;    245 sec\n","[2022-02-27 20:29:50,084 INFO] Step 6500/10000; acc:  67.79; ppl:  4.41; xent: 1.48; lr: 1.00000; 8341/8920 tok/s;    245 sec\n","[2022-02-27 20:29:50,912 INFO] Step 6520/10000; acc:  60.47; ppl:  6.83; xent: 1.92; lr: 1.00000; 8623/8894 tok/s;    246 sec\n","[2022-02-27 20:29:51,638 INFO] Step 6540/10000; acc:  62.77; ppl:  5.58; xent: 1.72; lr: 1.00000; 9189/9194 tok/s;    247 sec\n","[2022-02-27 20:29:52,304 INFO] Step 6560/10000; acc:  68.91; ppl:  4.03; xent: 1.39; lr: 1.00000; 8029/8953 tok/s;    248 sec\n","[2022-02-27 20:29:53,026 INFO] Step 6580/10000; acc:  62.50; ppl:  5.95; xent: 1.78; lr: 1.00000; 9275/9494 tok/s;    248 sec\n","[2022-02-27 20:29:53,861 INFO] Step 6600/10000; acc:  60.00; ppl:  6.96; xent: 1.94; lr: 1.00000; 8772/8572 tok/s;    249 sec\n","[2022-02-27 20:29:54,516 INFO] Step 6620/10000; acc:  67.24; ppl:  4.28; xent: 1.46; lr: 1.00000; 8313/9184 tok/s;    250 sec\n","[2022-02-27 20:29:55,297 INFO] Step 6640/10000; acc:  61.01; ppl:  6.25; xent: 1.83; lr: 1.00000; 9103/9194 tok/s;    251 sec\n","[2022-02-27 20:29:56,061 INFO] Step 6660/10000; acc:  62.23; ppl:  6.34; xent: 1.85; lr: 1.00000; 8893/8525 tok/s;    251 sec\n","[2022-02-27 20:29:56,764 INFO] Step 6680/10000; acc:  65.95; ppl:  4.64; xent: 1.54; lr: 1.00000; 8828/9262 tok/s;    252 sec\n","[2022-02-27 20:29:57,475 INFO] Step 6700/10000; acc:  61.21; ppl:  6.63; xent: 1.89; lr: 1.00000; 9395/9371 tok/s;    253 sec\n","[2022-02-27 20:29:58,290 INFO] Step 6720/10000; acc:  59.00; ppl:  7.17; xent: 1.97; lr: 1.00000; 8808/8995 tok/s;    254 sec\n","[2022-02-27 20:29:59,064 INFO] Step 6740/10000; acc:  65.18; ppl:  4.84; xent: 1.58; lr: 1.00000; 7800/8435 tok/s;    254 sec\n","[2022-02-27 20:29:59,801 INFO] Step 6760/10000; acc:  63.05; ppl:  5.70; xent: 1.74; lr: 1.00000; 8598/9165 tok/s;    255 sec\n","[2022-02-27 20:30:00,564 INFO] Step 6780/10000; acc:  61.28; ppl:  6.28; xent: 1.84; lr: 1.00000; 8624/9136 tok/s;    256 sec\n","[2022-02-27 20:30:01,299 INFO] Step 6800/10000; acc:  64.28; ppl:  5.05; xent: 1.62; lr: 1.00000; 8876/9144 tok/s;    257 sec\n","[2022-02-27 20:30:01,954 INFO] Step 6820/10000; acc:  65.92; ppl:  4.60; xent: 1.53; lr: 1.00000; 8159/9241 tok/s;    257 sec\n","[2022-02-27 20:30:02,732 INFO] Step 6840/10000; acc:  61.79; ppl:  6.37; xent: 1.85; lr: 1.00000; 9106/9122 tok/s;    258 sec\n","[2022-02-27 20:30:03,042 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 12\n","[2022-02-27 20:30:03,535 INFO] Step 6860/10000; acc:  63.31; ppl:  5.42; xent: 1.69; lr: 1.00000; 8114/8724 tok/s;    259 sec\n","[2022-02-27 20:30:04,231 INFO] Validation perplexity: 5.79746\n","[2022-02-27 20:30:04,231 INFO] Validation accuracy: 68.6602\n","[2022-02-27 20:30:04,231 INFO] Model is improving ppl: 5.86904 --> 5.79746.\n","[2022-02-27 20:30:04,232 INFO] Model is improving acc: 68.0575 --> 68.6602.\n","[2022-02-27 20:30:04,405 INFO] Step 6880/10000; acc:  68.28; ppl:  3.94; xent: 1.37; lr: 1.00000; 6088/6428 tok/s;    260 sec\n","[2022-02-27 20:30:05,129 INFO] Step 6900/10000; acc:  63.06; ppl:  5.71; xent: 1.74; lr: 1.00000; 9132/9541 tok/s;    260 sec\n","[2022-02-27 20:30:05,985 INFO] Step 6920/10000; acc:  60.24; ppl:  6.95; xent: 1.94; lr: 1.00000; 8480/8400 tok/s;    261 sec\n","[2022-02-27 20:30:06,639 INFO] Step 6940/10000; acc:  67.45; ppl:  4.40; xent: 1.48; lr: 1.00000; 8352/9288 tok/s;    262 sec\n","[2022-02-27 20:30:07,399 INFO] Step 6960/10000; acc:  61.81; ppl:  5.97; xent: 1.79; lr: 1.00000; 9319/9513 tok/s;    263 sec\n","[2022-02-27 20:30:08,236 INFO] Step 6980/10000; acc:  61.97; ppl:  6.16; xent: 1.82; lr: 1.00000; 8086/8258 tok/s;    264 sec\n","[2022-02-27 20:30:08,973 INFO] Step 7000/10000; acc:  65.74; ppl:  4.87; xent: 1.58; lr: 1.00000; 8524/8735 tok/s;    264 sec\n","[2022-02-27 20:30:09,035 INFO] Saving checkpoint ./Question10/model_step_7000.pt\n","[2022-02-27 20:30:09,975 INFO] Step 7020/10000; acc:  61.61; ppl:  5.85; xent: 1.77; lr: 1.00000; 6674/7087 tok/s;    265 sec\n","[2022-02-27 20:30:10,748 INFO] Step 7040/10000; acc:  60.51; ppl:  6.94; xent: 1.94; lr: 1.00000; 9416/9573 tok/s;    266 sec\n","[2022-02-27 20:30:11,495 INFO] Step 7060/10000; acc:  66.67; ppl:  4.49; xent: 1.50; lr: 1.00000; 7842/8529 tok/s;    267 sec\n","[2022-02-27 20:30:12,208 INFO] Step 7080/10000; acc:  64.70; ppl:  5.08; xent: 1.62; lr: 1.00000; 8497/9127 tok/s;    268 sec\n","[2022-02-27 20:30:12,941 INFO] Step 7100/10000; acc:  65.96; ppl:  5.04; xent: 1.62; lr: 1.00000; 8590/9137 tok/s;    268 sec\n","[2022-02-27 20:30:13,693 INFO] Step 7120/10000; acc:  64.53; ppl:  4.92; xent: 1.59; lr: 1.00000; 8673/9055 tok/s;    269 sec\n","[2022-02-27 20:30:14,348 INFO] Step 7140/10000; acc:  68.03; ppl:  4.23; xent: 1.44; lr: 1.00000; 8212/9121 tok/s;    270 sec\n","[2022-02-27 20:30:15,165 INFO] Step 7160/10000; acc:  60.18; ppl:  6.67; xent: 1.90; lr: 1.00000; 8807/9147 tok/s;    271 sec\n","[2022-02-27 20:30:15,868 INFO] Step 7180/10000; acc:  65.24; ppl:  4.98; xent: 1.61; lr: 1.00000; 9536/9320 tok/s;    271 sec\n","[2022-02-27 20:30:16,532 INFO] Step 7200/10000; acc:  69.21; ppl:  3.89; xent: 1.36; lr: 1.00000; 8262/8693 tok/s;    272 sec\n","[2022-02-27 20:30:17,297 INFO] Step 7220/10000; acc:  61.66; ppl:  5.81; xent: 1.76; lr: 1.00000; 8861/9246 tok/s;    273 sec\n","[2022-02-27 20:30:18,135 INFO] Step 7240/10000; acc:  60.37; ppl:  6.65; xent: 1.89; lr: 1.00000; 8773/8775 tok/s;    273 sec\n","[2022-02-27 20:30:18,782 INFO] Step 7260/10000; acc:  69.14; ppl:  3.91; xent: 1.36; lr: 1.00000; 8417/9136 tok/s;    274 sec\n","[2022-02-27 20:30:19,550 INFO] Step 7280/10000; acc:  62.16; ppl:  5.55; xent: 1.71; lr: 1.00000; 9254/9347 tok/s;    275 sec\n","[2022-02-27 20:30:20,325 INFO] Step 7300/10000; acc:  63.06; ppl:  5.93; xent: 1.78; lr: 1.00000; 8705/8386 tok/s;    276 sec\n","[2022-02-27 20:30:21,041 INFO] Step 7320/10000; acc:  66.15; ppl:  4.53; xent: 1.51; lr: 1.00000; 8701/9199 tok/s;    276 sec\n","[2022-02-27 20:30:21,766 INFO] Step 7340/10000; acc:  61.89; ppl:  5.86; xent: 1.77; lr: 1.00000; 9241/9414 tok/s;    277 sec\n","[2022-02-27 20:30:22,585 INFO] Step 7360/10000; acc:  60.26; ppl:  6.63; xent: 1.89; lr: 1.00000; 8827/8977 tok/s;    278 sec\n","[2022-02-27 20:30:23,337 INFO] Step 7380/10000; acc:  66.42; ppl:  4.40; xent: 1.48; lr: 1.00000; 7920/8529 tok/s;    279 sec\n","[2022-02-27 20:30:24,076 INFO] Step 7400/10000; acc:  63.54; ppl:  5.39; xent: 1.69; lr: 1.00000; 8421/9126 tok/s;    279 sec\n","[2022-02-27 20:30:24,817 INFO] Step 7420/10000; acc:  64.15; ppl:  5.48; xent: 1.70; lr: 1.00000; 8699/9095 tok/s;    280 sec\n","[2022-02-27 20:30:25,566 INFO] Step 7440/10000; acc:  65.42; ppl:  4.75; xent: 1.56; lr: 1.00000; 8689/9039 tok/s;    281 sec\n","[2022-02-27 20:30:26,221 INFO] Step 7460/10000; acc:  66.98; ppl:  4.30; xent: 1.46; lr: 1.00000; 8201/9154 tok/s;    282 sec\n","[2022-02-27 20:30:27,008 INFO] Step 7480/10000; acc:  62.18; ppl:  6.21; xent: 1.83; lr: 1.00000; 9063/9219 tok/s;    282 sec\n","[2022-02-27 20:30:27,300 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 13\n","[2022-02-27 20:30:27,784 INFO] Step 7500/10000; acc:  63.43; ppl:  5.30; xent: 1.67; lr: 1.00000; 8477/8907 tok/s;    283 sec\n","[2022-02-27 20:30:28,051 INFO] Validation perplexity: 5.69537\n","[2022-02-27 20:30:28,051 INFO] Validation accuracy: 69.0079\n","[2022-02-27 20:30:28,051 INFO] Model is improving ppl: 5.79746 --> 5.69537.\n","[2022-02-27 20:30:28,051 INFO] Model is improving acc: 68.6602 --> 69.0079.\n","[2022-02-27 20:30:28,656 INFO] Step 7520/10000; acc:  70.57; ppl:  3.58; xent: 1.27; lr: 1.00000; 6068/6399 tok/s;    284 sec\n","[2022-02-27 20:30:29,384 INFO] Step 7540/10000; acc:  64.23; ppl:  5.25; xent: 1.66; lr: 1.00000; 9101/9531 tok/s;    285 sec\n","[2022-02-27 20:30:30,279 INFO] Step 7560/10000; acc:  60.33; ppl:  6.65; xent: 1.90; lr: 1.00000; 8214/8250 tok/s;    286 sec\n","[2022-02-27 20:30:30,937 INFO] Step 7580/10000; acc:  69.67; ppl:  3.85; xent: 1.35; lr: 1.00000; 8451/8950 tok/s;    286 sec\n","[2022-02-27 20:30:31,713 INFO] Step 7600/10000; acc:  61.65; ppl:  5.94; xent: 1.78; lr: 1.00000; 9195/9539 tok/s;    287 sec\n","[2022-02-27 20:30:32,513 INFO] Step 7620/10000; acc:  62.00; ppl:  5.99; xent: 1.79; lr: 1.00000; 8474/8527 tok/s;    288 sec\n","[2022-02-27 20:30:33,223 INFO] Step 7640/10000; acc:  67.33; ppl:  4.39; xent: 1.48; lr: 1.00000; 8628/9400 tok/s;    289 sec\n","[2022-02-27 20:30:33,960 INFO] Step 7660/10000; acc:  64.18; ppl:  5.05; xent: 1.62; lr: 1.00000; 8930/9366 tok/s;    289 sec\n","[2022-02-27 20:30:34,744 INFO] Step 7680/10000; acc:  61.57; ppl:  6.37; xent: 1.85; lr: 1.00000; 9071/9228 tok/s;    290 sec\n","[2022-02-27 20:30:35,499 INFO] Step 7700/10000; acc:  66.75; ppl:  4.30; xent: 1.46; lr: 1.00000; 7821/8402 tok/s;    291 sec\n","[2022-02-27 20:30:36,197 INFO] Step 7720/10000; acc:  66.01; ppl:  4.64; xent: 1.53; lr: 1.00000; 8752/9322 tok/s;    292 sec\n","[2022-02-27 20:30:36,963 INFO] Step 7740/10000; acc:  64.42; ppl:  5.10; xent: 1.63; lr: 1.00000; 8348/8894 tok/s;    292 sec\n","[2022-02-27 20:30:37,705 INFO] Step 7760/10000; acc:  65.83; ppl:  4.62; xent: 1.53; lr: 1.00000; 8800/9224 tok/s;    293 sec\n","[2022-02-27 20:30:38,356 INFO] Step 7780/10000; acc:  68.47; ppl:  3.92; xent: 1.37; lr: 1.00000; 8331/9217 tok/s;    294 sec\n","[2022-02-27 20:30:39,172 INFO] Step 7800/10000; acc:  61.57; ppl:  6.17; xent: 1.82; lr: 1.00000; 8844/9201 tok/s;    295 sec\n","[2022-02-27 20:30:39,874 INFO] Step 7820/10000; acc:  65.42; ppl:  4.70; xent: 1.55; lr: 1.00000; 9594/9254 tok/s;    295 sec\n","[2022-02-27 20:30:40,544 INFO] Step 7840/10000; acc:  71.49; ppl:  3.43; xent: 1.23; lr: 1.00000; 8101/8678 tok/s;    296 sec\n","[2022-02-27 20:30:41,286 INFO] Step 7860/10000; acc:  63.57; ppl:  5.22; xent: 1.65; lr: 1.00000; 9106/9423 tok/s;    297 sec\n","[2022-02-27 20:30:42,095 INFO] Step 7880/10000; acc:  62.18; ppl:  5.96; xent: 1.78; lr: 1.00000; 9139/8831 tok/s;    297 sec\n","[2022-02-27 20:30:42,764 INFO] Step 7900/10000; acc:  68.94; ppl:  3.79; xent: 1.33; lr: 1.00000; 8241/9002 tok/s;    298 sec\n","[2022-02-27 20:30:43,525 INFO] Step 7920/10000; acc:  62.20; ppl:  5.71; xent: 1.74; lr: 1.00000; 9486/9323 tok/s;    299 sec\n","[2022-02-27 20:30:44,299 INFO] Step 7940/10000; acc:  63.35; ppl:  5.73; xent: 1.75; lr: 1.00000; 8701/8854 tok/s;    300 sec\n","[2022-02-27 20:30:45,022 INFO] Step 7960/10000; acc:  66.43; ppl:  4.46; xent: 1.49; lr: 1.00000; 8411/9053 tok/s;    300 sec\n","[2022-02-27 20:30:45,761 INFO] Step 7980/10000; acc:  63.89; ppl:  5.17; xent: 1.64; lr: 1.00000; 8849/9113 tok/s;    301 sec\n","[2022-02-27 20:30:46,558 INFO] Step 8000/10000; acc:  61.00; ppl:  6.04; xent: 1.80; lr: 1.00000; 8874/9038 tok/s;    302 sec\n","[2022-02-27 20:30:46,617 INFO] Saving checkpoint ./Question10/model_step_8000.pt\n","[2022-02-27 20:30:47,599 INFO] Step 8020/10000; acc:  68.22; ppl:  4.02; xent: 1.39; lr: 1.00000; 5711/6188 tok/s;    303 sec\n","[2022-02-27 20:30:48,301 INFO] Step 8040/10000; acc:  65.51; ppl:  4.79; xent: 1.57; lr: 1.00000; 8884/9477 tok/s;    304 sec\n","[2022-02-27 20:30:49,044 INFO] Step 8060/10000; acc:  64.47; ppl:  5.26; xent: 1.66; lr: 1.00000; 8714/9130 tok/s;    304 sec\n","[2022-02-27 20:30:49,208 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 14\n","[2022-02-27 20:30:49,804 INFO] Step 8080/10000; acc:  66.44; ppl:  4.34; xent: 1.47; lr: 1.00000; 8581/9173 tok/s;    305 sec\n","[2022-02-27 20:30:50,482 INFO] Step 8100/10000; acc:  67.15; ppl:  4.28; xent: 1.45; lr: 1.00000; 7983/8691 tok/s;    306 sec\n","[2022-02-27 20:30:51,290 INFO] Step 8120/10000; acc:  61.47; ppl:  6.28; xent: 1.84; lr: 1.00000; 8880/9216 tok/s;    307 sec\n","[2022-02-27 20:30:51,696 INFO] Validation perplexity: 5.73173\n","[2022-02-27 20:30:51,697 INFO] Validation accuracy: 68.4979\n","[2022-02-27 20:30:51,697 INFO] Decreasing patience: 4/5\n","[2022-02-27 20:30:52,271 INFO] Step 8140/10000; acc:  64.36; ppl:  4.90; xent: 1.59; lr: 1.00000; 6735/6872 tok/s;    308 sec\n","[2022-02-27 20:30:52,967 INFO] Step 8160/10000; acc:  71.14; ppl:  3.55; xent: 1.27; lr: 1.00000; 7639/8099 tok/s;    308 sec\n","[2022-02-27 20:30:53,708 INFO] Step 8180/10000; acc:  64.19; ppl:  4.97; xent: 1.60; lr: 1.00000; 8917/9176 tok/s;    309 sec\n","[2022-02-27 20:30:54,589 INFO] Step 8200/10000; acc:  61.87; ppl:  5.89; xent: 1.77; lr: 1.00000; 8285/8270 tok/s;    310 sec\n","[2022-02-27 20:30:55,238 INFO] Step 8220/10000; acc:  70.27; ppl:  3.70; xent: 1.31; lr: 1.00000; 8611/9195 tok/s;    311 sec\n","[2022-02-27 20:30:56,030 INFO] Step 8240/10000; acc:  61.98; ppl:  5.73; xent: 1.75; lr: 1.00000; 9160/9469 tok/s;    311 sec\n","[2022-02-27 20:30:56,826 INFO] Step 8260/10000; acc:  62.09; ppl:  5.77; xent: 1.75; lr: 1.00000; 8563/8823 tok/s;    312 sec\n","[2022-02-27 20:30:57,518 INFO] Step 8280/10000; acc:  68.35; ppl:  3.94; xent: 1.37; lr: 1.00000; 8655/9492 tok/s;    313 sec\n","[2022-02-27 20:30:58,245 INFO] Step 8300/10000; acc:  65.30; ppl:  4.73; xent: 1.55; lr: 1.00000; 8896/9075 tok/s;    314 sec\n","[2022-02-27 20:30:59,010 INFO] Step 8320/10000; acc:  63.37; ppl:  5.66; xent: 1.73; lr: 1.00000; 9077/9446 tok/s;    314 sec\n","[2022-02-27 20:30:59,765 INFO] Step 8340/10000; acc:  68.36; ppl:  4.03; xent: 1.39; lr: 1.00000; 7886/8463 tok/s;    315 sec\n","[2022-02-27 20:31:00,470 INFO] Step 8360/10000; acc:  66.19; ppl:  4.46; xent: 1.49; lr: 1.00000; 8727/9273 tok/s;    316 sec\n","[2022-02-27 20:31:01,250 INFO] Step 8380/10000; acc:  65.21; ppl:  4.84; xent: 1.58; lr: 1.00000; 8241/8693 tok/s;    317 sec\n","[2022-02-27 20:31:01,977 INFO] Step 8400/10000; acc:  66.65; ppl:  4.24; xent: 1.44; lr: 1.00000; 9099/9494 tok/s;    317 sec\n","[2022-02-27 20:31:02,637 INFO] Step 8420/10000; acc:  70.11; ppl:  3.72; xent: 1.31; lr: 1.00000; 8286/9033 tok/s;    318 sec\n","[2022-02-27 20:31:03,468 INFO] Step 8440/10000; acc:  60.88; ppl:  6.48; xent: 1.87; lr: 1.00000; 8861/9041 tok/s;    319 sec\n","[2022-02-27 20:31:04,191 INFO] Step 8460/10000; acc:  66.84; ppl:  4.36; xent: 1.47; lr: 1.00000; 9300/8964 tok/s;    320 sec\n","[2022-02-27 20:31:04,850 INFO] Step 8480/10000; acc:  72.00; ppl:  3.22; xent: 1.17; lr: 1.00000; 8113/9015 tok/s;    320 sec\n","[2022-02-27 20:31:05,580 INFO] Step 8500/10000; acc:  64.81; ppl:  5.00; xent: 1.61; lr: 1.00000; 9206/9422 tok/s;    321 sec\n","[2022-02-27 20:31:06,389 INFO] Step 8520/10000; acc:  62.91; ppl:  5.63; xent: 1.73; lr: 1.00000; 9055/8657 tok/s;    322 sec\n","[2022-02-27 20:31:07,046 INFO] Step 8540/10000; acc:  70.41; ppl:  3.57; xent: 1.27; lr: 1.00000; 8533/8888 tok/s;    322 sec\n","[2022-02-27 20:31:07,823 INFO] Step 8560/10000; acc:  61.70; ppl:  5.71; xent: 1.74; lr: 1.00000; 9438/9573 tok/s;    323 sec\n","[2022-02-27 20:31:08,631 INFO] Step 8580/10000; acc:  62.64; ppl:  5.75; xent: 1.75; lr: 1.00000; 8519/8725 tok/s;    324 sec\n","[2022-02-27 20:31:09,338 INFO] Step 8600/10000; acc:  67.18; ppl:  4.06; xent: 1.40; lr: 1.00000; 8553/9400 tok/s;    325 sec\n","[2022-02-27 20:31:10,063 INFO] Step 8620/10000; acc:  64.54; ppl:  4.91; xent: 1.59; lr: 1.00000; 8956/9303 tok/s;    325 sec\n","[2022-02-27 20:31:10,835 INFO] Step 8640/10000; acc:  62.52; ppl:  5.50; xent: 1.70; lr: 1.00000; 9041/9164 tok/s;    326 sec\n","[2022-02-27 20:31:11,570 INFO] Step 8660/10000; acc:  68.23; ppl:  3.81; xent: 1.34; lr: 1.00000; 8001/8698 tok/s;    327 sec\n","[2022-02-27 20:31:12,294 INFO] Step 8680/10000; acc:  66.03; ppl:  4.59; xent: 1.52; lr: 1.00000; 8508/9380 tok/s;    328 sec\n","[2022-02-27 20:31:13,055 INFO] Step 8700/10000; acc:  65.09; ppl:  5.07; xent: 1.62; lr: 1.00000; 8489/8688 tok/s;    328 sec\n","[2022-02-27 20:31:13,225 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 15\n","[2022-02-27 20:31:13,863 INFO] Step 8720/10000; acc:  65.89; ppl:  4.34; xent: 1.47; lr: 1.00000; 8123/8696 tok/s;    329 sec\n","[2022-02-27 20:31:14,502 INFO] Step 8740/10000; acc:  69.18; ppl:  3.83; xent: 1.34; lr: 1.00000; 8541/9028 tok/s;    330 sec\n","[2022-02-27 20:31:15,117 INFO] Validation perplexity: 5.71175\n","[2022-02-27 20:31:15,117 INFO] Validation accuracy: 68.7993\n","[2022-02-27 20:31:15,118 INFO] Decreasing patience: 3/5\n","[2022-02-27 20:31:15,556 INFO] Step 8760/10000; acc:  61.36; ppl:  5.82; xent: 1.76; lr: 1.00000; 6773/7076 tok/s;    331 sec\n","[2022-02-27 20:31:16,297 INFO] Step 8780/10000; acc:  65.00; ppl:  4.62; xent: 1.53; lr: 1.00000; 9048/8891 tok/s;    332 sec\n","[2022-02-27 20:31:16,927 INFO] Step 8800/10000; acc:  72.51; ppl:  3.16; xent: 1.15; lr: 1.00000; 8495/9239 tok/s;    332 sec\n","[2022-02-27 20:31:17,673 INFO] Step 8820/10000; acc:  65.08; ppl:  4.75; xent: 1.56; lr: 1.00000; 8932/9247 tok/s;    333 sec\n","[2022-02-27 20:31:18,527 INFO] Step 8840/10000; acc:  61.68; ppl:  5.85; xent: 1.77; lr: 1.00000; 8604/8581 tok/s;    334 sec\n","[2022-02-27 20:31:19,203 INFO] Step 8860/10000; acc:  70.18; ppl:  3.52; xent: 1.26; lr: 1.00000; 8236/8860 tok/s;    335 sec\n","[2022-02-27 20:31:20,003 INFO] Step 8880/10000; acc:  62.43; ppl:  5.47; xent: 1.70; lr: 1.00000; 9011/9363 tok/s;    335 sec\n","[2022-02-27 20:31:20,791 INFO] Step 8900/10000; acc:  62.83; ppl:  5.80; xent: 1.76; lr: 1.00000; 8636/8925 tok/s;    336 sec\n","[2022-02-27 20:31:21,469 INFO] Step 8920/10000; acc:  69.31; ppl:  3.66; xent: 1.30; lr: 1.00000; 8728/9351 tok/s;    337 sec\n","[2022-02-27 20:31:22,200 INFO] Step 8940/10000; acc:  66.20; ppl:  4.35; xent: 1.47; lr: 1.00000; 8613/9087 tok/s;    338 sec\n","[2022-02-27 20:31:22,953 INFO] Step 8960/10000; acc:  64.69; ppl:  5.20; xent: 1.65; lr: 1.00000; 9006/9500 tok/s;    338 sec\n","[2022-02-27 20:31:23,710 INFO] Step 8980/10000; acc:  68.33; ppl:  3.85; xent: 1.35; lr: 1.00000; 7902/8725 tok/s;    339 sec\n","[2022-02-27 20:31:24,415 INFO] Step 9000/10000; acc:  67.69; ppl:  4.19; xent: 1.43; lr: 1.00000; 8849/9140 tok/s;    340 sec\n","[2022-02-27 20:31:24,474 INFO] Saving checkpoint ./Question10/model_step_9000.pt\n","[2022-02-27 20:31:25,451 INFO] Step 9020/10000; acc:  65.27; ppl:  4.82; xent: 1.57; lr: 1.00000; 6293/6590 tok/s;    341 sec\n","[2022-02-27 20:31:26,206 INFO] Step 9040/10000; acc:  67.28; ppl:  4.22; xent: 1.44; lr: 1.00000; 8864/9121 tok/s;    342 sec\n","[2022-02-27 20:31:26,873 INFO] Step 9060/10000; acc:  70.00; ppl:  3.57; xent: 1.27; lr: 1.00000; 8311/9082 tok/s;    342 sec\n","[2022-02-27 20:31:27,692 INFO] Step 9080/10000; acc:  61.47; ppl:  6.00; xent: 1.79; lr: 1.00000; 9075/9182 tok/s;    343 sec\n","[2022-02-27 20:31:28,421 INFO] Step 9100/10000; acc:  66.12; ppl:  4.44; xent: 1.49; lr: 1.00000; 9258/8977 tok/s;    344 sec\n","[2022-02-27 20:31:29,074 INFO] Step 9120/10000; acc:  72.99; ppl:  3.06; xent: 1.12; lr: 1.00000; 8189/8918 tok/s;    344 sec\n","[2022-02-27 20:31:29,813 INFO] Step 9140/10000; acc:  64.79; ppl:  4.86; xent: 1.58; lr: 1.00000; 9109/9353 tok/s;    345 sec\n","[2022-02-27 20:31:30,627 INFO] Step 9160/10000; acc:  63.81; ppl:  5.31; xent: 1.67; lr: 1.00000; 9049/8537 tok/s;    346 sec\n","[2022-02-27 20:31:31,286 INFO] Step 9180/10000; acc:  70.67; ppl:  3.37; xent: 1.22; lr: 1.00000; 8351/9182 tok/s;    347 sec\n","[2022-02-27 20:31:32,053 INFO] Step 9200/10000; acc:  63.15; ppl:  5.38; xent: 1.68; lr: 1.00000; 9448/9259 tok/s;    347 sec\n","[2022-02-27 20:31:32,938 INFO] Step 9220/10000; acc:  63.87; ppl:  5.33; xent: 1.67; lr: 1.00000; 7615/7883 tok/s;    348 sec\n","[2022-02-27 20:31:33,634 INFO] Step 9240/10000; acc:  68.13; ppl:  3.86; xent: 1.35; lr: 1.00000; 8682/9547 tok/s;    349 sec\n","[2022-02-27 20:31:34,388 INFO] Step 9260/10000; acc:  64.75; ppl:  4.79; xent: 1.57; lr: 1.00000; 8625/9102 tok/s;    350 sec\n","[2022-02-27 20:31:35,161 INFO] Step 9280/10000; acc:  63.10; ppl:  5.31; xent: 1.67; lr: 1.00000; 9091/9255 tok/s;    351 sec\n","[2022-02-27 20:31:35,893 INFO] Step 9300/10000; acc:  69.78; ppl:  3.64; xent: 1.29; lr: 1.00000; 8045/8605 tok/s;    351 sec\n","[2022-02-27 20:31:36,598 INFO] Step 9320/10000; acc:  67.08; ppl:  4.19; xent: 1.43; lr: 1.00000; 8707/9523 tok/s;    352 sec\n","[2022-02-27 20:31:37,352 INFO] Step 9340/10000; acc:  66.34; ppl:  4.49; xent: 1.50; lr: 1.00000; 8483/8722 tok/s;    353 sec\n","[2022-02-27 20:31:37,519 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 16\n","[2022-02-27 20:31:38,149 INFO] Step 9360/10000; acc:  67.07; ppl:  4.16; xent: 1.43; lr: 1.00000; 8224/8823 tok/s;    354 sec\n","[2022-02-27 20:31:38,862 INFO] Validation perplexity: 5.71136\n","[2022-02-27 20:31:38,863 INFO] Validation accuracy: 68.5906\n","[2022-02-27 20:31:38,863 INFO] Decreasing patience: 2/5\n","[2022-02-27 20:31:39,030 INFO] Step 9380/10000; acc:  69.43; ppl:  3.67; xent: 1.30; lr: 1.00000; 6222/6498 tok/s;    354 sec\n","[2022-02-27 20:31:39,870 INFO] Step 9400/10000; acc:  63.02; ppl:  5.53; xent: 1.71; lr: 1.00000; 8632/9000 tok/s;    355 sec\n","[2022-02-27 20:31:40,624 INFO] Step 9420/10000; acc:  66.19; ppl:  4.41; xent: 1.48; lr: 1.00000; 8964/8728 tok/s;    356 sec\n","[2022-02-27 20:31:41,274 INFO] Step 9440/10000; acc:  72.42; ppl:  3.07; xent: 1.12; lr: 1.00000; 8287/9099 tok/s;    357 sec\n","[2022-02-27 20:31:42,010 INFO] Step 9460/10000; acc:  65.98; ppl:  4.51; xent: 1.51; lr: 1.00000; 9060/9503 tok/s;    357 sec\n","[2022-02-27 20:31:42,920 INFO] Step 9480/10000; acc:  62.44; ppl:  5.56; xent: 1.71; lr: 1.00000; 8054/8086 tok/s;    358 sec\n","[2022-02-27 20:31:43,597 INFO] Step 9500/10000; acc:  70.50; ppl:  3.35; xent: 1.21; lr: 1.00000; 8255/8788 tok/s;    359 sec\n","[2022-02-27 20:31:44,394 INFO] Step 9520/10000; acc:  63.17; ppl:  5.07; xent: 1.62; lr: 1.00000; 9122/9430 tok/s;    360 sec\n","[2022-02-27 20:31:45,188 INFO] Step 9540/10000; acc:  64.34; ppl:  5.42; xent: 1.69; lr: 1.00000; 8578/8958 tok/s;    361 sec\n","[2022-02-27 20:31:45,869 INFO] Step 9560/10000; acc:  69.85; ppl:  3.53; xent: 1.26; lr: 1.00000; 8642/9343 tok/s;    361 sec\n","[2022-02-27 20:31:46,582 INFO] Step 9580/10000; acc:  68.57; ppl:  3.89; xent: 1.36; lr: 1.00000; 8753/9207 tok/s;    362 sec\n","[2022-02-27 20:31:47,344 INFO] Step 9600/10000; acc:  65.63; ppl:  4.66; xent: 1.54; lr: 1.00000; 8769/9254 tok/s;    363 sec\n","[2022-02-27 20:31:48,096 INFO] Step 9620/10000; acc:  69.78; ppl:  3.47; xent: 1.25; lr: 1.00000; 7902/8503 tok/s;    363 sec\n","[2022-02-27 20:31:48,818 INFO] Step 9640/10000; acc:  67.19; ppl:  4.21; xent: 1.44; lr: 1.00000; 8547/9254 tok/s;    364 sec\n","[2022-02-27 20:31:49,576 INFO] Step 9660/10000; acc:  66.49; ppl:  4.57; xent: 1.52; lr: 1.00000; 8627/8934 tok/s;    365 sec\n","[2022-02-27 20:31:50,298 INFO] Step 9680/10000; acc:  68.18; ppl:  3.95; xent: 1.37; lr: 1.00000; 9419/9193 tok/s;    366 sec\n","[2022-02-27 20:31:50,965 INFO] Step 9700/10000; acc:  70.91; ppl:  3.49; xent: 1.25; lr: 1.00000; 8507/8889 tok/s;    366 sec\n","[2022-02-27 20:31:51,820 INFO] Step 9720/10000; acc:  61.60; ppl:  5.98; xent: 1.79; lr: 1.00000; 8810/9125 tok/s;    367 sec\n","[2022-02-27 20:31:52,559 INFO] Step 9740/10000; acc:  67.35; ppl:  4.01; xent: 1.39; lr: 1.00000; 9075/9236 tok/s;    368 sec\n","[2022-02-27 20:31:53,196 INFO] Step 9760/10000; acc:  73.87; ppl:  2.96; xent: 1.08; lr: 1.00000; 8351/9040 tok/s;    369 sec\n","[2022-02-27 20:31:53,911 INFO] Step 9780/10000; acc:  66.42; ppl:  4.36; xent: 1.47; lr: 1.00000; 9378/9466 tok/s;    369 sec\n","[2022-02-27 20:31:54,719 INFO] Step 9800/10000; acc:  65.00; ppl:  5.12; xent: 1.63; lr: 1.00000; 9077/8588 tok/s;    370 sec\n","[2022-02-27 20:31:55,384 INFO] Step 9820/10000; acc:  71.42; ppl:  3.27; xent: 1.19; lr: 1.00000; 8282/9173 tok/s;    371 sec\n","[2022-02-27 20:31:56,158 INFO] Step 9840/10000; acc:  63.41; ppl:  5.31; xent: 1.67; lr: 1.00000; 9398/9497 tok/s;    372 sec\n","[2022-02-27 20:31:56,967 INFO] Step 9860/10000; acc:  64.41; ppl:  4.94; xent: 1.60; lr: 1.00000; 8367/8633 tok/s;    372 sec\n","[2022-02-27 20:31:57,655 INFO] Step 9880/10000; acc:  70.18; ppl:  3.48; xent: 1.25; lr: 1.00000; 8671/9351 tok/s;    373 sec\n","[2022-02-27 20:31:58,394 INFO] Step 9900/10000; acc:  66.63; ppl:  4.32; xent: 1.46; lr: 1.00000; 8662/9173 tok/s;    374 sec\n","[2022-02-27 20:31:59,163 INFO] Step 9920/10000; acc:  64.44; ppl:  5.09; xent: 1.63; lr: 1.00000; 8942/9178 tok/s;    375 sec\n","[2022-02-27 20:31:59,905 INFO] Step 9940/10000; acc:  69.87; ppl:  3.44; xent: 1.24; lr: 1.00000; 7929/8585 tok/s;    375 sec\n","[2022-02-27 20:32:00,618 INFO] Step 9960/10000; acc:  66.39; ppl:  4.21; xent: 1.44; lr: 1.00000; 8637/9313 tok/s;    376 sec\n","[2022-02-27 20:32:01,374 INFO] Step 9980/10000; acc:  66.33; ppl:  4.60; xent: 1.53; lr: 1.00000; 8558/8870 tok/s;    377 sec\n","[2022-02-27 20:32:01,528 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 17\n","[2022-02-27 20:32:02,160 INFO] Step 10000/10000; acc:  67.40; ppl:  4.04; xent: 1.40; lr: 1.00000; 8411/8783 tok/s;    378 sec\n","[2022-02-27 20:32:02,414 INFO] Validation perplexity: 5.76075\n","[2022-02-27 20:32:02,414 INFO] Validation accuracy: 68.9847\n","[2022-02-27 20:32:02,415 INFO] Decreasing patience: 1/5\n","[2022-02-27 20:32:02,487 INFO] Saving checkpoint ./Question10/model_step_10000.pt\n"]},{"data":{"text/plain":[]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_train -config Question10/config-base.yaml --early_stopping 1"]},{"cell_type":"markdown","metadata":{"id":"lEsq0WXMPrkZ"},"source":["Augmenter le nombre de training steps permet de renforcer l'apprentissage et souvent d'augmenter le score BLEU. Cependant, il est possible d'atteindre un stade où augmenter le nombre de training steps est sans influence voire avec une influence négative sur la performance BLEU. A 7000 steps, le score bleu rediminue et réaugmente à 8000 steps avant de rediminuer\n"," On peut utiliser une méthode d'early stopping afin de déterminer le nombre idéal de training steps avec l'option early7.2.3_stopping. En effet, cette option permet de s'arrêter si la performance n'a pas augmenté au bout du nombre spécifié de training steps et donc de déterminer le nombre idéal de training steps selon les besoins."]},{"cell_type":"markdown","metadata":{"id":"YABSiOuk0jRG"},"source":["## Question 11"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125427,"status":"ok","timestamp":1645994315748,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"Sk5DdKLZ0pyZ","outputId":"d4786614-4f1a-4bc4-8436-340b30c6a8ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:36:31,933 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 20:36:31,934 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 20:36:31,935 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 20:36:31,935 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 20:36:31,936 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 20:36:31,936 INFO] Loading vocab from text file...\n","[2022-02-27 20:36:31,937 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 20:36:31,965 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 20:36:31,971 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 20:36:31,991 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 20:36:31,995 INFO] Building fields with vocab in counters...\n","[2022-02-27 20:36:32,006 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 20:36:32,020 INFO]  * src vocab size: 10415.\n","[2022-02-27 20:36:32,020 INFO]  * src vocab size = 10415\n","[2022-02-27 20:36:32,021 INFO]  * tgt vocab size = 8198\n","[2022-02-27 20:36:32,025 INFO] Building model...\n","[2022-02-27 20:36:34,821 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, num_layers=2, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","        (1): LSTMCell(256, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 20:36:34,823 INFO] encoder: 6247884\n","[2022-02-27 20:36:34,823 INFO] decoder: 7770558\n","[2022-02-27 20:36:34,823 INFO] * number of parameters: 14018442\n","[2022-02-27 20:36:34,826 INFO] Starting training on GPU: [0]\n","[2022-02-27 20:36:34,826 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 20:36:34,826 INFO] train's transforms: TransformPipe()\n","[2022-02-27 20:36:34,827 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 20:36:35,980 INFO] Step 20/ 2000; acc:   9.17; ppl: 2080.03; xent: 7.64; lr: 1.00000; 5074/5351 tok/s;      1 sec\n","[2022-02-27 20:36:37,399 INFO] Step 40/ 2000; acc:   7.61; ppl: 920.14; xent: 6.82; lr: 1.00000; 5199/5390 tok/s;      3 sec\n","[2022-02-27 20:36:38,526 INFO] Step 60/ 2000; acc:  10.16; ppl: 407.98; xent: 6.01; lr: 1.00000; 5377/5720 tok/s;      4 sec\n","[2022-02-27 20:36:39,610 INFO] Step 80/ 2000; acc:  14.63; ppl: 320.45; xent: 5.77; lr: 1.00000; 5503/5625 tok/s;      5 sec\n","[2022-02-27 20:36:41,028 INFO] Step 100/ 2000; acc:  14.32; ppl: 322.44; xent: 5.78; lr: 1.00000; 5371/5710 tok/s;      6 sec\n","[2022-02-27 20:36:42,074 INFO] Step 120/ 2000; acc:  20.50; ppl: 169.60; xent: 5.13; lr: 1.00000; 5342/5583 tok/s;      7 sec\n","[2022-02-27 20:36:43,223 INFO] Step 140/ 2000; acc:  19.55; ppl: 199.30; xent: 5.29; lr: 1.00000; 5682/5734 tok/s;      8 sec\n","[2022-02-27 20:36:44,594 INFO] Step 160/ 2000; acc:  17.41; ppl: 227.29; xent: 5.43; lr: 1.00000; 5897/5814 tok/s;     10 sec\n","[2022-02-27 20:36:45,613 INFO] Step 180/ 2000; acc:  24.78; ppl: 111.13; xent: 4.71; lr: 1.00000; 5189/5894 tok/s;     11 sec\n","[2022-02-27 20:36:46,661 INFO] Step 200/ 2000; acc:  25.11; ppl: 120.45; xent: 4.79; lr: 1.00000; 5512/5835 tok/s;     12 sec\n","[2022-02-27 20:36:48,059 INFO] Step 220/ 2000; acc:  22.94; ppl: 144.92; xent: 4.98; lr: 1.00000; 5468/5478 tok/s;     13 sec\n","[2022-02-27 20:36:49,096 INFO] Step 240/ 2000; acc:  30.68; ppl: 84.38; xent: 4.44; lr: 1.00000; 5459/6063 tok/s;     14 sec\n","[2022-02-27 20:36:50,208 INFO] Step 260/ 2000; acc:  29.60; ppl: 92.64; xent: 4.53; lr: 1.00000; 5216/5656 tok/s;     15 sec\n","[2022-02-27 20:36:51,521 INFO] Step 280/ 2000; acc:  27.23; ppl: 117.42; xent: 4.77; lr: 1.00000; 5647/5619 tok/s;     17 sec\n","[2022-02-27 20:36:52,618 INFO] Step 300/ 2000; acc:  31.09; ppl: 80.86; xent: 4.39; lr: 1.00000; 5403/5847 tok/s;     18 sec\n","[2022-02-27 20:36:53,741 INFO] Step 320/ 2000; acc:  31.18; ppl: 78.67; xent: 4.37; lr: 1.00000; 5482/5893 tok/s;     19 sec\n","[2022-02-27 20:36:54,852 INFO] Step 340/ 2000; acc:  30.90; ppl: 74.73; xent: 4.31; lr: 1.00000; 5376/5508 tok/s;     20 sec\n","[2022-02-27 20:36:56,177 INFO] Step 360/ 2000; acc:  27.76; ppl: 92.89; xent: 4.53; lr: 1.00000; 5650/5658 tok/s;     21 sec\n","[2022-02-27 20:36:57,283 INFO] Step 380/ 2000; acc:  33.25; ppl: 65.52; xent: 4.18; lr: 1.00000; 5595/5743 tok/s;     22 sec\n","[2022-02-27 20:36:58,381 INFO] Step 400/ 2000; acc:  34.96; ppl: 56.50; xent: 4.03; lr: 1.00000; 5417/5563 tok/s;     24 sec\n","[2022-02-27 20:36:59,746 INFO] Step 420/ 2000; acc:  27.16; ppl: 94.54; xent: 4.55; lr: 1.00000; 5685/5689 tok/s;     25 sec\n","[2022-02-27 20:37:00,763 INFO] Step 440/ 2000; acc:  37.03; ppl: 47.53; xent: 3.86; lr: 1.00000; 5467/5827 tok/s;     26 sec\n","[2022-02-27 20:37:01,900 INFO] Step 460/ 2000; acc:  32.47; ppl: 73.57; xent: 4.30; lr: 1.00000; 5769/5676 tok/s;     27 sec\n","[2022-02-27 20:37:03,361 INFO] Step 480/ 2000; acc:  28.89; ppl: 77.78; xent: 4.35; lr: 1.00000; 5443/5519 tok/s;     29 sec\n","[2022-02-27 20:37:04,370 INFO] Step 500/ 2000; acc:  37.97; ppl: 45.23; xent: 3.81; lr: 1.00000; 5232/5748 tok/s;     30 sec\n","[2022-02-27 20:37:05,460 INFO] Step 520/ 2000; acc:  36.46; ppl: 52.53; xent: 3.96; lr: 1.00000; 5275/5686 tok/s;     31 sec\n","[2022-02-27 20:37:06,859 INFO] Step 540/ 2000; acc:  32.19; ppl: 69.57; xent: 4.24; lr: 1.00000; 5538/5581 tok/s;     32 sec\n","[2022-02-27 20:37:07,925 INFO] Step 560/ 2000; acc:  37.74; ppl: 46.24; xent: 3.83; lr: 1.00000; 5401/5910 tok/s;     33 sec\n","[2022-02-27 20:37:08,827 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 20:37:09,034 INFO] Step 580/ 2000; acc:  37.52; ppl: 49.61; xent: 3.90; lr: 1.00000; 5301/5674 tok/s;     34 sec\n","[2022-02-27 20:37:10,355 INFO] Step 600/ 2000; acc:  32.83; ppl: 68.17; xent: 4.22; lr: 1.00000; 5553/5583 tok/s;     36 sec\n","[2022-02-27 20:37:11,432 INFO] Step 620/ 2000; acc:  37.33; ppl: 47.35; xent: 3.86; lr: 1.00000; 5531/5693 tok/s;     37 sec\n","[2022-02-27 20:37:11,676 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 20:37:12,045 INFO] Validation perplexity: 26.8346\n","[2022-02-27 20:37:12,045 INFO] Validation accuracy: 45.2248\n","[2022-02-27 20:37:12,109 INFO] Saving checkpoint ./Question11/model_step_625.pt\n","[2022-02-27 20:37:13,232 INFO] Step 640/ 2000; acc:  36.84; ppl: 46.03; xent: 3.83; lr: 1.00000; 3403/3802 tok/s;     38 sec\n","[2022-02-27 20:37:14,374 INFO] Step 660/ 2000; acc:  38.12; ppl: 43.41; xent: 3.77; lr: 1.00000; 5106/5408 tok/s;     40 sec\n","[2022-02-27 20:37:15,756 INFO] Step 680/ 2000; acc:  34.38; ppl: 55.04; xent: 4.01; lr: 1.00000; 5277/5394 tok/s;     41 sec\n","[2022-02-27 20:37:16,875 INFO] Step 700/ 2000; acc:  38.24; ppl: 44.51; xent: 3.80; lr: 1.00000; 5412/5656 tok/s;     42 sec\n","[2022-02-27 20:37:17,954 INFO] Step 720/ 2000; acc:  39.94; ppl: 41.09; xent: 3.72; lr: 1.00000; 5532/5771 tok/s;     43 sec\n","[2022-02-27 20:37:19,403 INFO] Step 740/ 2000; acc:  33.24; ppl: 60.44; xent: 4.10; lr: 1.00000; 5369/5661 tok/s;     45 sec\n","[2022-02-27 20:37:20,422 INFO] Step 760/ 2000; acc:  42.45; ppl: 33.90; xent: 3.52; lr: 1.00000; 5521/5760 tok/s;     46 sec\n","[2022-02-27 20:37:21,574 INFO] Step 780/ 2000; acc:  38.82; ppl: 45.67; xent: 3.82; lr: 1.00000; 5671/5662 tok/s;     47 sec\n","[2022-02-27 20:37:22,931 INFO] Step 800/ 2000; acc:  35.54; ppl: 54.11; xent: 3.99; lr: 1.00000; 5821/5767 tok/s;     48 sec\n","[2022-02-27 20:37:23,921 INFO] Step 820/ 2000; acc:  43.79; ppl: 28.81; xent: 3.36; lr: 1.00000; 5206/6116 tok/s;     49 sec\n","[2022-02-27 20:37:25,007 INFO] Step 840/ 2000; acc:  41.44; ppl: 35.45; xent: 3.57; lr: 1.00000; 5225/5608 tok/s;     50 sec\n","[2022-02-27 20:37:26,384 INFO] Step 860/ 2000; acc:  37.00; ppl: 45.94; xent: 3.83; lr: 1.00000; 5622/5595 tok/s;     52 sec\n","[2022-02-27 20:37:27,443 INFO] Step 880/ 2000; acc:  43.89; ppl: 30.53; xent: 3.42; lr: 1.00000; 5385/5912 tok/s;     53 sec\n","[2022-02-27 20:37:28,539 INFO] Step 900/ 2000; acc:  41.94; ppl: 34.34; xent: 3.54; lr: 1.00000; 5368/5670 tok/s;     54 sec\n","[2022-02-27 20:37:29,869 INFO] Step 920/ 2000; acc:  37.29; ppl: 45.63; xent: 3.82; lr: 1.00000; 5627/5758 tok/s;     55 sec\n","[2022-02-27 20:37:30,962 INFO] Step 940/ 2000; acc:  42.40; ppl: 34.29; xent: 3.53; lr: 1.00000; 5529/5774 tok/s;     56 sec\n","[2022-02-27 20:37:32,083 INFO] Step 960/ 2000; acc:  41.92; ppl: 33.39; xent: 3.51; lr: 1.00000; 5567/5949 tok/s;     57 sec\n","[2022-02-27 20:37:33,233 INFO] Step 980/ 2000; acc:  42.92; ppl: 29.71; xent: 3.39; lr: 1.00000; 5127/5289 tok/s;     58 sec\n","[2022-02-27 20:37:34,566 INFO] Step 1000/ 2000; acc:  38.05; ppl: 42.18; xent: 3.74; lr: 1.00000; 5494/5471 tok/s;     60 sec\n","[2022-02-27 20:37:35,608 INFO] Step 1020/ 2000; acc:  42.40; ppl: 31.28; xent: 3.44; lr: 1.00000; 5880/6057 tok/s;     61 sec\n","[2022-02-27 20:37:36,710 INFO] Step 1040/ 2000; acc:  43.52; ppl: 28.50; xent: 3.35; lr: 1.00000; 5447/5600 tok/s;     62 sec\n","[2022-02-27 20:37:38,133 INFO] Step 1060/ 2000; acc:  36.39; ppl: 45.84; xent: 3.83; lr: 1.00000; 5563/5640 tok/s;     63 sec\n","[2022-02-27 20:37:39,134 INFO] Step 1080/ 2000; acc:  46.77; ppl: 24.46; xent: 3.20; lr: 1.00000; 5629/5828 tok/s;     64 sec\n","[2022-02-27 20:37:40,279 INFO] Step 1100/ 2000; acc:  41.33; ppl: 35.84; xent: 3.58; lr: 1.00000; 5747/5743 tok/s;     65 sec\n","[2022-02-27 20:37:41,694 INFO] Step 1120/ 2000; acc:  38.89; ppl: 39.19; xent: 3.67; lr: 1.00000; 5565/5596 tok/s;     67 sec\n","[2022-02-27 20:37:42,697 INFO] Step 1140/ 2000; acc:  47.15; ppl: 24.10; xent: 3.18; lr: 1.00000; 5255/5852 tok/s;     68 sec\n","[2022-02-27 20:37:43,766 INFO] Step 1160/ 2000; acc:  43.92; ppl: 30.18; xent: 3.41; lr: 1.00000; 5341/5822 tok/s;     69 sec\n","[2022-02-27 20:37:45,203 INFO] Step 1180/ 2000; acc:  38.98; ppl: 39.74; xent: 3.68; lr: 1.00000; 5341/5545 tok/s;     70 sec\n","[2022-02-27 20:37:46,229 INFO] Step 1200/ 2000; acc:  46.35; ppl: 24.42; xent: 3.20; lr: 1.00000; 5560/5905 tok/s;     71 sec\n","[2022-02-27 20:37:47,144 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 20:37:47,333 INFO] Step 1220/ 2000; acc:  44.44; ppl: 27.94; xent: 3.33; lr: 1.00000; 5245/5747 tok/s;     73 sec\n","[2022-02-27 20:37:48,648 INFO] Step 1240/ 2000; acc:  40.39; ppl: 36.44; xent: 3.60; lr: 1.00000; 5567/5646 tok/s;     74 sec\n","[2022-02-27 20:37:49,626 INFO] Validation perplexity: 16.387\n","[2022-02-27 20:37:49,627 INFO] Validation accuracy: 52.1558\n","[2022-02-27 20:37:49,688 INFO] Saving checkpoint ./Question11/model_step_1250.pt\n","[2022-02-27 20:37:50,358 INFO] Step 1260/ 2000; acc:  44.92; ppl: 26.96; xent: 3.29; lr: 1.00000; 3508/3654 tok/s;     76 sec\n","[2022-02-27 20:37:51,519 INFO] Step 1280/ 2000; acc:  43.98; ppl: 29.44; xent: 3.38; lr: 1.00000; 5364/5763 tok/s;     77 sec\n","[2022-02-27 20:37:52,662 INFO] Step 1300/ 2000; acc:  45.47; ppl: 25.47; xent: 3.24; lr: 1.00000; 5185/5439 tok/s;     78 sec\n","[2022-02-27 20:37:54,080 INFO] Step 1320/ 2000; acc:  40.49; ppl: 32.25; xent: 3.47; lr: 1.00000; 5187/5349 tok/s;     79 sec\n","[2022-02-27 20:37:55,174 INFO] Step 1340/ 2000; acc:  45.22; ppl: 26.53; xent: 3.28; lr: 1.00000; 5602/5795 tok/s;     80 sec\n","[2022-02-27 20:37:56,337 INFO] Step 1360/ 2000; acc:  45.74; ppl: 24.47; xent: 3.20; lr: 1.00000; 5130/5263 tok/s;     82 sec\n","[2022-02-27 20:37:57,733 INFO] Step 1380/ 2000; acc:  39.69; ppl: 37.78; xent: 3.63; lr: 1.00000; 5572/5903 tok/s;     83 sec\n","[2022-02-27 20:37:58,769 INFO] Step 1400/ 2000; acc:  48.70; ppl: 20.64; xent: 3.03; lr: 1.00000; 5386/5698 tok/s;     84 sec\n","[2022-02-27 20:37:59,925 INFO] Step 1420/ 2000; acc:  44.02; ppl: 29.02; xent: 3.37; lr: 1.00000; 5563/5577 tok/s;     85 sec\n","[2022-02-27 20:38:01,318 INFO] Step 1440/ 2000; acc:  41.58; ppl: 32.53; xent: 3.48; lr: 1.00000; 5520/5598 tok/s;     86 sec\n","[2022-02-27 20:38:02,274 INFO] Step 1460/ 2000; acc:  50.28; ppl: 19.18; xent: 2.95; lr: 1.00000; 5358/6032 tok/s;     87 sec\n","[2022-02-27 20:38:03,366 INFO] Step 1480/ 2000; acc:  46.42; ppl: 23.71; xent: 3.17; lr: 1.00000; 5148/5586 tok/s;     89 sec\n","[2022-02-27 20:38:04,742 INFO] Step 1500/ 2000; acc:  42.31; ppl: 30.75; xent: 3.43; lr: 1.00000; 5677/5634 tok/s;     90 sec\n","[2022-02-27 20:38:05,801 INFO] Step 1520/ 2000; acc:  47.71; ppl: 21.45; xent: 3.07; lr: 1.00000; 5417/5996 tok/s;     91 sec\n","[2022-02-27 20:38:06,936 INFO] Step 1540/ 2000; acc:  47.42; ppl: 21.30; xent: 3.06; lr: 1.00000; 5205/5648 tok/s;     92 sec\n","[2022-02-27 20:38:08,267 INFO] Step 1560/ 2000; acc:  41.86; ppl: 30.67; xent: 3.42; lr: 1.00000; 5673/5670 tok/s;     93 sec\n","[2022-02-27 20:38:09,399 INFO] Step 1580/ 2000; acc:  46.81; ppl: 22.69; xent: 3.12; lr: 1.00000; 5391/5742 tok/s;     95 sec\n","[2022-02-27 20:38:10,469 INFO] Step 1600/ 2000; acc:  46.71; ppl: 22.45; xent: 3.11; lr: 1.00000; 5939/6045 tok/s;     96 sec\n","[2022-02-27 20:38:11,605 INFO] Step 1620/ 2000; acc:  48.21; ppl: 20.10; xent: 3.00; lr: 1.00000; 5192/5395 tok/s;     97 sec\n","[2022-02-27 20:38:12,908 INFO] Step 1640/ 2000; acc:  41.75; ppl: 30.76; xent: 3.43; lr: 1.00000; 5710/5649 tok/s;     98 sec\n","[2022-02-27 20:38:13,990 INFO] Step 1660/ 2000; acc:  45.92; ppl: 22.62; xent: 3.12; lr: 1.00000; 5682/5907 tok/s;     99 sec\n","[2022-02-27 20:38:15,069 INFO] Step 1680/ 2000; acc:  47.72; ppl: 21.66; xent: 3.08; lr: 1.00000; 5513/5633 tok/s;    100 sec\n","[2022-02-27 20:38:16,484 INFO] Step 1700/ 2000; acc:  41.38; ppl: 30.08; xent: 3.40; lr: 1.00000; 5481/5533 tok/s;    102 sec\n","[2022-02-27 20:38:17,491 INFO] Step 1720/ 2000; acc:  50.33; ppl: 17.50; xent: 2.86; lr: 1.00000; 5513/5880 tok/s;    103 sec\n","[2022-02-27 20:38:18,652 INFO] Step 1740/ 2000; acc:  46.19; ppl: 23.80; xent: 3.17; lr: 1.00000; 5609/5681 tok/s;    104 sec\n","[2022-02-27 20:38:20,097 INFO] Step 1760/ 2000; acc:  42.54; ppl: 29.02; xent: 3.37; lr: 1.00000; 5474/5491 tok/s;    105 sec\n","[2022-02-27 20:38:21,089 INFO] Step 1780/ 2000; acc:  51.03; ppl: 17.89; xent: 2.88; lr: 1.00000; 5274/5910 tok/s;    106 sec\n","[2022-02-27 20:38:22,161 INFO] Step 1800/ 2000; acc:  47.60; ppl: 21.25; xent: 3.06; lr: 1.00000; 5320/5761 tok/s;    107 sec\n","[2022-02-27 20:38:23,564 INFO] Step 1820/ 2000; acc:  43.53; ppl: 27.20; xent: 3.30; lr: 1.00000; 5438/5547 tok/s;    109 sec\n","[2022-02-27 20:38:24,577 INFO] Step 1840/ 2000; acc:  51.04; ppl: 17.17; xent: 2.84; lr: 1.00000; 5592/6005 tok/s;    110 sec\n","[2022-02-27 20:38:25,491 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 20:38:25,725 INFO] Step 1860/ 2000; acc:  47.90; ppl: 20.03; xent: 3.00; lr: 1.00000; 5048/5487 tok/s;    111 sec\n","[2022-02-27 20:38:26,955 INFO] Validation perplexity: 12.4539\n","[2022-02-27 20:38:26,955 INFO] Validation accuracy: 55.6328\n","[2022-02-27 20:38:27,015 INFO] Saving checkpoint ./Question11/model_step_1875.pt\n","[2022-02-27 20:38:27,713 INFO] Step 1880/ 2000; acc:  42.94; ppl: 26.74; xent: 3.29; lr: 1.00000; 3742/3819 tok/s;    113 sec\n","[2022-02-27 20:38:28,804 INFO] Step 1900/ 2000; acc:  48.56; ppl: 19.71; xent: 2.98; lr: 1.00000; 5522/5815 tok/s;    114 sec\n","[2022-02-27 20:38:29,953 INFO] Step 1920/ 2000; acc:  47.91; ppl: 20.50; xent: 3.02; lr: 1.00000; 5443/5816 tok/s;    115 sec\n","[2022-02-27 20:38:31,081 INFO] Step 1940/ 2000; acc:  49.05; ppl: 18.68; xent: 2.93; lr: 1.00000; 5290/5461 tok/s;    116 sec\n","[2022-02-27 20:38:32,464 INFO] Step 1960/ 2000; acc:  43.21; ppl: 24.57; xent: 3.20; lr: 1.00000; 5339/5585 tok/s;    118 sec\n","[2022-02-27 20:38:33,579 INFO] Step 1980/ 2000; acc:  48.24; ppl: 20.26; xent: 3.01; lr: 1.00000; 5517/5743 tok/s;    119 sec\n","[2022-02-27 20:38:34,665 INFO] Step 2000/ 2000; acc:  50.51; ppl: 17.45; xent: 2.86; lr: 1.00000; 5522/5734 tok/s;    120 sec\n","[2022-02-27 20:38:34,727 INFO] Saving checkpoint ./Question11/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_train -config Question11/config-base2.yaml"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6996,"status":"ok","timestamp":1645994333815,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"P4mGAfbW44So","outputId":"727e1b62-093a-4bcb-e70f-c39da8c46ac5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:38:48,683 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:38:53,322 INFO] PRED AVG SCORE: -1.0319, PRED PPL: 2.8063\n","BLEU = 15.65, 52.0/23.1/13.8/6.5 (BP=0.863, ratio=0.872, hyp_len=3116, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model Question11/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question11/pred2000.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question11/pred2000.txt"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169268,"status":"ok","timestamp":1645994515329,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"pOb_WL6J5E5F","outputId":"3fab12c3-82c5-4c71-90a4-c4da79bf8d8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:39:07,684 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 20:39:07,686 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 20:39:07,686 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 20:39:07,687 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 20:39:07,688 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 20:39:07,688 INFO] Loading vocab from text file...\n","[2022-02-27 20:39:07,688 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 20:39:07,719 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 20:39:07,724 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 20:39:07,744 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 20:39:07,749 INFO] Building fields with vocab in counters...\n","[2022-02-27 20:39:07,759 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 20:39:07,773 INFO]  * src vocab size: 10415.\n","[2022-02-27 20:39:07,774 INFO]  * src vocab size = 10415\n","[2022-02-27 20:39:07,774 INFO]  * tgt vocab size = 8198\n","[2022-02-27 20:39:07,778 INFO] Building model...\n","[2022-02-27 20:39:10,461 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, num_layers=3, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","        (1): LSTMCell(256, 256)\n","        (2): LSTMCell(256, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 20:39:10,462 INFO] encoder: 6643148\n","[2022-02-27 20:39:10,462 INFO] decoder: 8296894\n","[2022-02-27 20:39:10,462 INFO] * number of parameters: 14940042\n","[2022-02-27 20:39:10,464 INFO] Starting training on GPU: [0]\n","[2022-02-27 20:39:10,464 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 20:39:10,465 INFO] train's transforms: TransformPipe()\n","[2022-02-27 20:39:10,465 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 20:39:12,108 INFO] Step 20/ 2000; acc:   8.43; ppl: 1859.11; xent: 7.53; lr: 1.00000; 3863/4034 tok/s;      2 sec\n","[2022-02-27 20:39:13,559 INFO] Step 40/ 2000; acc:   9.69; ppl: 724.14; xent: 6.58; lr: 1.00000; 3914/4117 tok/s;      3 sec\n","[2022-02-27 20:39:15,239 INFO] Step 60/ 2000; acc:   8.96; ppl: 520.11; xent: 6.25; lr: 1.00000; 4001/4213 tok/s;      5 sec\n","[2022-02-27 20:39:17,137 INFO] Step 80/ 2000; acc:   9.01; ppl: 455.06; xent: 6.12; lr: 1.00000; 3685/3755 tok/s;      7 sec\n","[2022-02-27 20:39:18,607 INFO] Step 100/ 2000; acc:  10.94; ppl: 343.41; xent: 5.84; lr: 1.00000; 3874/4074 tok/s;      8 sec\n","[2022-02-27 20:39:20,278 INFO] Step 120/ 2000; acc:   9.08; ppl: 350.71; xent: 5.86; lr: 1.00000; 4202/4255 tok/s;     10 sec\n","[2022-02-27 20:39:22,112 INFO] Step 140/ 2000; acc:  10.06; ppl: 409.24; xent: 6.01; lr: 1.00000; 3889/4054 tok/s;     12 sec\n","[2022-02-27 20:39:23,577 INFO] Step 160/ 2000; acc:  12.96; ppl: 274.22; xent: 5.61; lr: 1.00000; 4102/4324 tok/s;     13 sec\n","[2022-02-27 20:39:25,055 INFO] Step 180/ 2000; acc:  17.63; ppl: 207.27; xent: 5.33; lr: 1.00000; 4045/4196 tok/s;     15 sec\n","[2022-02-27 20:39:26,887 INFO] Step 200/ 2000; acc:  15.31; ppl: 280.74; xent: 5.64; lr: 1.00000; 4182/4331 tok/s;     16 sec\n","[2022-02-27 20:39:28,369 INFO] Step 220/ 2000; acc:  20.55; ppl: 164.75; xent: 5.10; lr: 1.00000; 3714/4116 tok/s;     18 sec\n","[2022-02-27 20:39:29,861 INFO] Step 240/ 2000; acc:  19.27; ppl: 165.36; xent: 5.11; lr: 1.00000; 4099/4348 tok/s;     19 sec\n","[2022-02-27 20:39:31,570 INFO] Step 260/ 2000; acc:  19.84; ppl: 179.59; xent: 5.19; lr: 1.00000; 3827/3959 tok/s;     21 sec\n","[2022-02-27 20:39:33,080 INFO] Step 280/ 2000; acc:  21.42; ppl: 156.09; xent: 5.05; lr: 1.00000; 4360/4525 tok/s;     23 sec\n","[2022-02-27 20:39:34,622 INFO] Step 300/ 2000; acc:  23.99; ppl: 130.46; xent: 4.87; lr: 1.00000; 3977/4163 tok/s;     24 sec\n","[2022-02-27 20:39:36,309 INFO] Step 320/ 2000; acc:  21.49; ppl: 137.16; xent: 4.92; lr: 1.00000; 3921/4270 tok/s;     26 sec\n","[2022-02-27 20:39:37,945 INFO] Step 340/ 2000; acc:  22.89; ppl: 125.49; xent: 4.83; lr: 1.00000; 3967/4038 tok/s;     27 sec\n","[2022-02-27 20:39:39,272 INFO] Step 360/ 2000; acc:  27.80; ppl: 99.34; xent: 4.60; lr: 1.00000; 4364/4371 tok/s;     29 sec\n","[2022-02-27 20:39:40,976 INFO] Step 380/ 2000; acc:  24.50; ppl: 117.10; xent: 4.76; lr: 1.00000; 3987/4088 tok/s;     31 sec\n","[2022-02-27 20:39:42,755 INFO] Step 400/ 2000; acc:  24.67; ppl: 126.46; xent: 4.84; lr: 1.00000; 3977/4019 tok/s;     32 sec\n","[2022-02-27 20:39:44,168 INFO] Step 420/ 2000; acc:  30.04; ppl: 83.10; xent: 4.42; lr: 1.00000; 4013/4250 tok/s;     34 sec\n","[2022-02-27 20:39:45,788 INFO] Step 440/ 2000; acc:  26.67; ppl: 97.50; xent: 4.58; lr: 1.00000; 4361/4297 tok/s;     35 sec\n","[2022-02-27 20:39:47,632 INFO] Step 460/ 2000; acc:  26.01; ppl: 103.44; xent: 4.64; lr: 1.00000; 3854/4015 tok/s;     37 sec\n","[2022-02-27 20:39:49,085 INFO] Step 480/ 2000; acc:  31.12; ppl: 76.61; xent: 4.34; lr: 1.00000; 4154/4150 tok/s;     39 sec\n","[2022-02-27 20:39:50,634 INFO] Step 500/ 2000; acc:  30.07; ppl: 78.90; xent: 4.37; lr: 1.00000; 3813/4116 tok/s;     40 sec\n","[2022-02-27 20:39:52,561 INFO] Step 520/ 2000; acc:  27.00; ppl: 95.61; xent: 4.56; lr: 1.00000; 3996/4125 tok/s;     42 sec\n","[2022-02-27 20:39:54,009 INFO] Step 540/ 2000; acc:  34.30; ppl: 59.11; xent: 4.08; lr: 1.00000; 3810/4254 tok/s;     44 sec\n","[2022-02-27 20:39:55,549 INFO] Step 560/ 2000; acc:  32.98; ppl: 65.04; xent: 4.18; lr: 1.00000; 4014/4202 tok/s;     45 sec\n","[2022-02-27 20:39:56,923 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 20:39:57,215 INFO] Step 580/ 2000; acc:  31.34; ppl: 76.23; xent: 4.33; lr: 1.00000; 3991/4061 tok/s;     47 sec\n","[2022-02-27 20:39:58,868 INFO] Step 600/ 2000; acc:  31.70; ppl: 66.46; xent: 4.20; lr: 1.00000; 3958/4222 tok/s;     48 sec\n","[2022-02-27 20:40:00,365 INFO] Step 620/ 2000; acc:  33.30; ppl: 59.97; xent: 4.09; lr: 1.00000; 4106/4267 tok/s;     50 sec\n","[2022-02-27 20:40:00,705 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 20:40:01,243 INFO] Validation perplexity: 36.0604\n","[2022-02-27 20:40:01,243 INFO] Validation accuracy: 39.4993\n","[2022-02-27 20:40:01,306 INFO] Saving checkpoint ./Question11/model_step_625.pt\n","[2022-02-27 20:40:02,864 INFO] Step 640/ 2000; acc:  31.23; ppl: 72.37; xent: 4.28; lr: 1.00000; 2630/2805 tok/s;     52 sec\n","[2022-02-27 20:40:04,472 INFO] Step 660/ 2000; acc:  32.61; ppl: 59.64; xent: 4.09; lr: 1.00000; 3926/4080 tok/s;     54 sec\n","[2022-02-27 20:40:05,925 INFO] Step 680/ 2000; acc:  35.92; ppl: 51.38; xent: 3.94; lr: 1.00000; 3907/4046 tok/s;     55 sec\n","[2022-02-27 20:40:07,555 INFO] Step 700/ 2000; acc:  31.94; ppl: 62.38; xent: 4.13; lr: 1.00000; 4090/4310 tok/s;     57 sec\n","[2022-02-27 20:40:09,431 INFO] Step 720/ 2000; acc:  32.81; ppl: 62.38; xent: 4.13; lr: 1.00000; 3751/3821 tok/s;     59 sec\n","[2022-02-27 20:40:10,874 INFO] Step 740/ 2000; acc:  35.19; ppl: 54.29; xent: 3.99; lr: 1.00000; 3972/4216 tok/s;     60 sec\n","[2022-02-27 20:40:12,539 INFO] Step 760/ 2000; acc:  33.39; ppl: 57.14; xent: 4.05; lr: 1.00000; 4275/4251 tok/s;     62 sec\n","[2022-02-27 20:40:14,374 INFO] Step 780/ 2000; acc:  31.77; ppl: 63.58; xent: 4.15; lr: 1.00000; 3854/4182 tok/s;     64 sec\n","[2022-02-27 20:40:15,787 INFO] Step 800/ 2000; acc:  38.55; ppl: 46.32; xent: 3.84; lr: 1.00000; 4203/4261 tok/s;     65 sec\n","[2022-02-27 20:40:17,378 INFO] Step 820/ 2000; acc:  37.13; ppl: 44.62; xent: 3.80; lr: 1.00000; 3645/3987 tok/s;     67 sec\n","[2022-02-27 20:40:19,156 INFO] Step 840/ 2000; acc:  31.28; ppl: 67.44; xent: 4.21; lr: 1.00000; 4254/4393 tok/s;     69 sec\n","[2022-02-27 20:40:20,617 INFO] Step 860/ 2000; acc:  40.15; ppl: 38.66; xent: 3.65; lr: 1.00000; 3821/4072 tok/s;     70 sec\n","[2022-02-27 20:40:22,107 INFO] Step 880/ 2000; acc:  36.81; ppl: 44.04; xent: 3.78; lr: 1.00000; 4133/4446 tok/s;     72 sec\n","[2022-02-27 20:40:23,851 INFO] Step 900/ 2000; acc:  35.13; ppl: 49.95; xent: 3.91; lr: 1.00000; 3785/3935 tok/s;     73 sec\n","[2022-02-27 20:40:25,469 INFO] Step 920/ 2000; acc:  36.15; ppl: 47.17; xent: 3.85; lr: 1.00000; 4112/4323 tok/s;     75 sec\n","[2022-02-27 20:40:26,927 INFO] Step 940/ 2000; acc:  37.92; ppl: 41.86; xent: 3.73; lr: 1.00000; 4259/4379 tok/s;     76 sec\n","[2022-02-27 20:40:28,614 INFO] Step 960/ 2000; acc:  35.32; ppl: 51.03; xent: 3.93; lr: 1.00000; 4018/4209 tok/s;     78 sec\n","[2022-02-27 20:40:30,230 INFO] Step 980/ 2000; acc:  36.76; ppl: 44.79; xent: 3.80; lr: 1.00000; 3972/4038 tok/s;     80 sec\n","[2022-02-27 20:40:31,669 INFO] Step 1000/ 2000; acc:  39.40; ppl: 39.01; xent: 3.66; lr: 1.00000; 3995/3971 tok/s;     81 sec\n","[2022-02-27 20:40:33,320 INFO] Step 1020/ 2000; acc:  36.90; ppl: 41.00; xent: 3.71; lr: 1.00000; 4066/4221 tok/s;     83 sec\n","[2022-02-27 20:40:35,054 INFO] Step 1040/ 2000; acc:  33.97; ppl: 51.76; xent: 3.95; lr: 1.00000; 4054/4045 tok/s;     85 sec\n","[2022-02-27 20:40:36,424 INFO] Step 1060/ 2000; acc:  38.93; ppl: 39.50; xent: 3.68; lr: 1.00000; 4184/4473 tok/s;     86 sec\n","[2022-02-27 20:40:38,090 INFO] Step 1080/ 2000; acc:  36.07; ppl: 44.94; xent: 3.81; lr: 1.00000; 4316/4176 tok/s;     88 sec\n","[2022-02-27 20:40:40,019 INFO] Step 1100/ 2000; acc:  33.50; ppl: 49.93; xent: 3.91; lr: 1.00000; 3721/3981 tok/s;     90 sec\n","[2022-02-27 20:40:41,461 INFO] Step 1120/ 2000; acc:  39.81; ppl: 37.79; xent: 3.63; lr: 1.00000; 4166/4199 tok/s;     91 sec\n","[2022-02-27 20:40:42,959 INFO] Step 1140/ 2000; acc:  38.02; ppl: 39.38; xent: 3.67; lr: 1.00000; 3944/4210 tok/s;     92 sec\n","[2022-02-27 20:40:44,788 INFO] Step 1160/ 2000; acc:  34.68; ppl: 50.99; xent: 3.93; lr: 1.00000; 4137/4319 tok/s;     94 sec\n","[2022-02-27 20:40:46,199 INFO] Step 1180/ 2000; acc:  41.02; ppl: 34.08; xent: 3.53; lr: 1.00000; 3852/4383 tok/s;     96 sec\n","[2022-02-27 20:40:47,720 INFO] Step 1200/ 2000; acc:  39.75; ppl: 36.32; xent: 3.59; lr: 1.00000; 4016/4249 tok/s;     97 sec\n","[2022-02-27 20:40:49,146 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 20:40:49,462 INFO] Step 1220/ 2000; acc:  36.79; ppl: 47.06; xent: 3.85; lr: 1.00000; 3797/3872 tok/s;     99 sec\n","[2022-02-27 20:40:51,066 INFO] Step 1240/ 2000; acc:  37.42; ppl: 41.62; xent: 3.73; lr: 1.00000; 4130/4330 tok/s;    101 sec\n","[2022-02-27 20:40:52,349 INFO] Validation perplexity: 21.866\n","[2022-02-27 20:40:52,349 INFO] Validation accuracy: 46.1289\n","[2022-02-27 20:40:52,410 INFO] Saving checkpoint ./Question11/model_step_1250.pt\n","[2022-02-27 20:40:53,428 INFO] Step 1260/ 2000; acc:  38.40; ppl: 37.76; xent: 3.63; lr: 1.00000; 2621/2704 tok/s;    103 sec\n","[2022-02-27 20:40:55,136 INFO] Step 1280/ 2000; acc:  36.83; ppl: 42.35; xent: 3.75; lr: 1.00000; 3844/4127 tok/s;    105 sec\n","[2022-02-27 20:40:56,766 INFO] Step 1300/ 2000; acc:  38.79; ppl: 34.39; xent: 3.54; lr: 1.00000; 3923/4039 tok/s;    106 sec\n","[2022-02-27 20:40:58,248 INFO] Step 1320/ 2000; acc:  41.48; ppl: 33.00; xent: 3.50; lr: 1.00000; 3874/4013 tok/s;    108 sec\n","[2022-02-27 20:40:59,806 INFO] Step 1340/ 2000; acc:  37.72; ppl: 40.43; xent: 3.70; lr: 1.00000; 4341/4475 tok/s;    109 sec\n","[2022-02-27 20:41:01,709 INFO] Step 1360/ 2000; acc:  37.30; ppl: 40.89; xent: 3.71; lr: 1.00000; 3685/3829 tok/s;    111 sec\n","[2022-02-27 20:41:03,170 INFO] Step 1380/ 2000; acc:  41.07; ppl: 32.86; xent: 3.49; lr: 1.00000; 3904/4089 tok/s;    113 sec\n","[2022-02-27 20:41:04,856 INFO] Step 1400/ 2000; acc:  37.59; ppl: 38.38; xent: 3.65; lr: 1.00000; 4180/4232 tok/s;    114 sec\n","[2022-02-27 20:41:06,693 INFO] Step 1420/ 2000; acc:  36.42; ppl: 43.32; xent: 3.77; lr: 1.00000; 3829/4091 tok/s;    116 sec\n","[2022-02-27 20:41:08,227 INFO] Step 1440/ 2000; acc:  43.05; ppl: 29.38; xent: 3.38; lr: 1.00000; 3790/3999 tok/s;    118 sec\n","[2022-02-27 20:41:09,677 INFO] Step 1460/ 2000; acc:  41.45; ppl: 31.96; xent: 3.46; lr: 1.00000; 3901/4439 tok/s;    119 sec\n","[2022-02-27 20:41:11,453 INFO] Step 1480/ 2000; acc:  37.18; ppl: 42.58; xent: 3.75; lr: 1.00000; 4230/4277 tok/s;    121 sec\n","[2022-02-27 20:41:12,930 INFO] Step 1500/ 2000; acc:  44.12; ppl: 26.37; xent: 3.27; lr: 1.00000; 3813/4043 tok/s;    122 sec\n","[2022-02-27 20:41:14,402 INFO] Step 1520/ 2000; acc:  41.28; ppl: 30.81; xent: 3.43; lr: 1.00000; 4225/4515 tok/s;    124 sec\n","[2022-02-27 20:41:16,190 INFO] Step 1540/ 2000; acc:  38.63; ppl: 36.45; xent: 3.60; lr: 1.00000; 3717/3892 tok/s;    126 sec\n","[2022-02-27 20:41:17,740 INFO] Step 1560/ 2000; acc:  40.63; ppl: 33.64; xent: 3.52; lr: 1.00000; 4367/4303 tok/s;    127 sec\n","[2022-02-27 20:41:19,190 INFO] Step 1580/ 2000; acc:  41.33; ppl: 30.84; xent: 3.43; lr: 1.00000; 4328/4409 tok/s;    129 sec\n","[2022-02-27 20:41:20,961 INFO] Step 1600/ 2000; acc:  39.07; ppl: 35.11; xent: 3.56; lr: 1.00000; 3856/4129 tok/s;    130 sec\n","[2022-02-27 20:41:22,580 INFO] Step 1620/ 2000; acc:  41.15; ppl: 30.27; xent: 3.41; lr: 1.00000; 3975/4084 tok/s;    132 sec\n","[2022-02-27 20:41:23,986 INFO] Step 1640/ 2000; acc:  42.74; ppl: 29.19; xent: 3.37; lr: 1.00000; 4090/4104 tok/s;    134 sec\n","[2022-02-27 20:41:25,617 INFO] Step 1660/ 2000; acc:  40.62; ppl: 32.61; xent: 3.48; lr: 1.00000; 4150/4248 tok/s;    135 sec\n","[2022-02-27 20:41:27,388 INFO] Step 1680/ 2000; acc:  38.70; ppl: 37.32; xent: 3.62; lr: 1.00000; 3966/4007 tok/s;    137 sec\n","[2022-02-27 20:41:28,785 INFO] Step 1700/ 2000; acc:  42.72; ppl: 28.87; xent: 3.36; lr: 1.00000; 4067/4232 tok/s;    138 sec\n","[2022-02-27 20:41:30,525 INFO] Step 1720/ 2000; acc:  39.24; ppl: 32.61; xent: 3.48; lr: 1.00000; 4057/4104 tok/s;    140 sec\n","[2022-02-27 20:41:32,414 INFO] Step 1740/ 2000; acc:  39.18; ppl: 35.82; xent: 3.58; lr: 1.00000; 3744/3997 tok/s;    142 sec\n","[2022-02-27 20:41:33,843 INFO] Step 1760/ 2000; acc:  44.50; ppl: 26.91; xent: 3.29; lr: 1.00000; 4185/4225 tok/s;    143 sec\n","[2022-02-27 20:41:35,375 INFO] Step 1780/ 2000; acc:  42.56; ppl: 29.12; xent: 3.37; lr: 1.00000; 3832/4142 tok/s;    145 sec\n","[2022-02-27 20:41:37,190 INFO] Step 1800/ 2000; acc:  37.42; ppl: 38.88; xent: 3.66; lr: 1.00000; 4183/4295 tok/s;    147 sec\n","[2022-02-27 20:41:38,613 INFO] Step 1820/ 2000; acc:  43.86; ppl: 26.74; xent: 3.29; lr: 1.00000; 3826/4328 tok/s;    148 sec\n","[2022-02-27 20:41:40,146 INFO] Step 1840/ 2000; acc:  43.75; ppl: 25.68; xent: 3.25; lr: 1.00000; 3991/4199 tok/s;    150 sec\n","[2022-02-27 20:41:41,518 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 20:41:41,841 INFO] Step 1860/ 2000; acc:  41.02; ppl: 33.69; xent: 3.52; lr: 1.00000; 3850/3981 tok/s;    151 sec\n","[2022-02-27 20:41:43,618 INFO] Validation perplexity: 17.3973\n","[2022-02-27 20:41:43,618 INFO] Validation accuracy: 50\n","[2022-02-27 20:41:43,694 INFO] Saving checkpoint ./Question11/model_step_1875.pt\n","[2022-02-27 20:41:44,304 INFO] Step 1880/ 2000; acc:  40.89; ppl: 30.66; xent: 3.42; lr: 1.00000; 2708/2806 tok/s;    154 sec\n","[2022-02-27 20:41:45,813 INFO] Step 1900/ 2000; acc:  43.38; ppl: 26.67; xent: 3.28; lr: 1.00000; 4110/4278 tok/s;    155 sec\n","[2022-02-27 20:41:47,574 INFO] Step 1920/ 2000; acc:  40.37; ppl: 33.37; xent: 3.51; lr: 1.00000; 3804/4059 tok/s;    157 sec\n","[2022-02-27 20:41:49,199 INFO] Step 1940/ 2000; acc:  42.70; ppl: 27.46; xent: 3.31; lr: 1.00000; 3958/4089 tok/s;    159 sec\n","[2022-02-27 20:41:50,642 INFO] Step 1960/ 2000; acc:  44.67; ppl: 25.46; xent: 3.24; lr: 1.00000; 4003/4152 tok/s;    160 sec\n","[2022-02-27 20:41:52,271 INFO] Step 1980/ 2000; acc:  42.94; ppl: 28.26; xent: 3.34; lr: 1.00000; 4177/4279 tok/s;    162 sec\n","[2022-02-27 20:41:54,134 INFO] Step 2000/ 2000; acc:  40.60; ppl: 31.29; xent: 3.44; lr: 1.00000; 3784/3930 tok/s;    164 sec\n","[2022-02-27 20:41:54,201 INFO] Saving checkpoint ./Question11/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_train -config Question11/config-base3.yaml"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8493,"status":"ok","timestamp":1645994530659,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"vjwox-zx5JH0","outputId":"c6b28685-d8a9-4c0c-85a3-73a1151b76b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:42:04,216 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:42:10,136 INFO] PRED AVG SCORE: -1.2179, PRED PPL: 3.3802\n","BLEU = 10.47, 42.4/14.9/8.3/2.8 (BP=0.951, ratio=0.952, hyp_len=3402, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model Question11/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question11/pred2000_3.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question11/pred2000_3.txt"]},{"cell_type":"markdown","metadata":{"id":"YTGUCNIL501S"},"source":["Avec 1 couche pour 2000 steps, on avait un score bleu de 18,32%\n","Pour 2 couches, on a une score de 15,65 % \n","Pour 3 couches, on a un score de 10,47 %\n","Cela ne permet donc pas toujours d'obtenir des meilleurs résultats car il y a plus de poids à mettre à jour et pas toujours assez d'étapes d'entrainement\n","\n","<span style=\"color:red\">C'esut une bonne observation. Toutefois Il aurait fallu utiliser l'option early_stopping pour bien repondre à cette question. En effet, si l'on utilise pas cette option, on risque de comparer des modèles qui sont sous-entrainé (c'est votre cas) ou sur-entrainés (overfitted),  sans donc pouvoir en tirer des conclusions. Vous aurez pu aussi essayer d'autres combinaisons de couches (e.g. 1+3,2+1,3+3...) pour pouvoir tirer des conclusions plus solides.</span>"]},{"cell_type":"markdown","metadata":{"id":"4QshE7317TQB"},"source":["## Question 12"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100213,"status":"ok","timestamp":1645994962131,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"N9h6jpZK7VaY","outputId":"1cac65f5-e3c0-4646-d8b1-bd5975bbe620"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:47:43,564 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 20:47:43,568 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 20:47:43,568 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 20:47:43,568 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 20:47:43,569 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 20:47:43,570 INFO] Loading vocab from text file...\n","[2022-02-27 20:47:43,570 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 20:47:43,597 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 20:47:43,603 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 20:47:43,622 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 20:47:43,627 INFO] Building fields with vocab in counters...\n","[2022-02-27 20:47:43,637 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 20:47:43,652 INFO]  * src vocab size: 10415.\n","[2022-02-27 20:47:43,652 INFO]  * src vocab size = 10415\n","[2022-02-27 20:47:43,653 INFO]  * tgt vocab size = 8198\n","[2022-02-27 20:47:43,657 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:47:46,343 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 192, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(884, 384)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=384, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 20:47:46,344 INFO] encoder: 6273484\n","[2022-02-27 20:47:46,344 INFO] decoder: 9205950\n","[2022-02-27 20:47:46,345 INFO] * number of parameters: 15479434\n","[2022-02-27 20:47:46,347 INFO] Starting training on GPU: [0]\n","[2022-02-27 20:47:46,347 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 20:47:46,347 INFO] train's transforms: TransformPipe()\n","[2022-02-27 20:47:46,348 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 20:47:47,492 INFO] Step 20/ 2000; acc:   7.98; ppl: 4797.65; xent: 8.48; lr: 1.00000; 6102/6304 tok/s;      1 sec\n","[2022-02-27 20:47:48,337 INFO] Step 40/ 2000; acc:  15.91; ppl: 452.15; xent: 6.11; lr: 1.00000; 6402/7311 tok/s;      2 sec\n","[2022-02-27 20:47:49,254 INFO] Step 60/ 2000; acc:  17.51; ppl: 285.94; xent: 5.66; lr: 1.00000; 7241/7318 tok/s;      3 sec\n","[2022-02-27 20:47:50,326 INFO] Step 80/ 2000; acc:  17.76; ppl: 275.27; xent: 5.62; lr: 1.00000; 6997/7150 tok/s;      4 sec\n","[2022-02-27 20:47:51,163 INFO] Step 100/ 2000; acc:  24.66; ppl: 143.21; xent: 4.96; lr: 1.00000; 6763/7046 tok/s;      5 sec\n","[2022-02-27 20:47:52,026 INFO] Step 120/ 2000; acc:  24.46; ppl: 135.09; xent: 4.91; lr: 1.00000; 6846/7212 tok/s;      6 sec\n","[2022-02-27 20:47:53,077 INFO] Step 140/ 2000; acc:  19.56; ppl: 210.25; xent: 5.35; lr: 1.00000; 7651/7630 tok/s;      7 sec\n","[2022-02-27 20:47:53,918 INFO] Step 160/ 2000; acc:  29.20; ppl: 108.52; xent: 4.69; lr: 1.00000; 7137/7170 tok/s;      8 sec\n","[2022-02-27 20:47:54,762 INFO] Step 180/ 2000; acc:  30.61; ppl: 90.06; xent: 4.50; lr: 1.00000; 6436/7564 tok/s;      8 sec\n","[2022-02-27 20:47:55,742 INFO] Step 200/ 2000; acc:  28.45; ppl: 100.81; xent: 4.61; lr: 1.00000; 7369/7158 tok/s;      9 sec\n","[2022-02-27 20:47:56,722 INFO] Step 220/ 2000; acc:  30.64; ppl: 92.49; xent: 4.53; lr: 1.00000; 6567/7043 tok/s;     10 sec\n","[2022-02-27 20:47:57,527 INFO] Step 240/ 2000; acc:  34.83; ppl: 73.88; xent: 4.30; lr: 1.00000; 7138/7636 tok/s;     11 sec\n","[2022-02-27 20:47:58,483 INFO] Step 260/ 2000; acc:  32.57; ppl: 69.68; xent: 4.24; lr: 1.00000; 6711/7180 tok/s;     12 sec\n","[2022-02-27 20:47:59,462 INFO] Step 280/ 2000; acc:  32.48; ppl: 81.30; xent: 4.40; lr: 1.00000; 6955/7045 tok/s;     13 sec\n","[2022-02-27 20:48:00,267 INFO] Step 300/ 2000; acc:  36.61; ppl: 64.71; xent: 4.17; lr: 1.00000; 6992/7623 tok/s;     14 sec\n","[2022-02-27 20:48:01,235 INFO] Step 320/ 2000; acc:  33.62; ppl: 65.43; xent: 4.18; lr: 1.00000; 7147/7600 tok/s;     15 sec\n","[2022-02-27 20:48:02,260 INFO] Step 340/ 2000; acc:  32.90; ppl: 73.04; xent: 4.29; lr: 1.00000; 6917/6888 tok/s;     16 sec\n","[2022-02-27 20:48:03,083 INFO] Step 360/ 2000; acc:  37.53; ppl: 55.36; xent: 4.01; lr: 1.00000; 6836/7069 tok/s;     17 sec\n","[2022-02-27 20:48:04,030 INFO] Step 380/ 2000; acc:  36.93; ppl: 50.35; xent: 3.92; lr: 1.00000; 7064/7366 tok/s;     18 sec\n","[2022-02-27 20:48:05,074 INFO] Step 400/ 2000; acc:  30.93; ppl: 83.62; xent: 4.43; lr: 1.00000; 7229/7144 tok/s;     19 sec\n","[2022-02-27 20:48:05,880 INFO] Step 420/ 2000; acc:  40.21; ppl: 43.41; xent: 3.77; lr: 1.00000; 7123/7259 tok/s;     20 sec\n","[2022-02-27 20:48:06,714 INFO] Step 440/ 2000; acc:  39.80; ppl: 42.45; xent: 3.75; lr: 1.00000; 7000/7533 tok/s;     20 sec\n","[2022-02-27 20:48:07,817 INFO] Step 460/ 2000; acc:  31.87; ppl: 66.28; xent: 4.19; lr: 1.00000; 7282/7166 tok/s;     21 sec\n","[2022-02-27 20:48:08,715 INFO] Step 480/ 2000; acc:  37.95; ppl: 47.70; xent: 3.87; lr: 1.00000; 6558/7105 tok/s;     22 sec\n","[2022-02-27 20:48:09,523 INFO] Step 500/ 2000; acc:  40.86; ppl: 39.03; xent: 3.66; lr: 1.00000; 6722/7482 tok/s;     23 sec\n","[2022-02-27 20:48:10,503 INFO] Step 520/ 2000; acc:  37.42; ppl: 52.03; xent: 3.95; lr: 1.00000; 7389/7017 tok/s;     24 sec\n","[2022-02-27 20:48:11,451 INFO] Step 540/ 2000; acc:  38.69; ppl: 47.44; xent: 3.86; lr: 1.00000; 6880/7266 tok/s;     25 sec\n","[2022-02-27 20:48:12,291 INFO] Step 560/ 2000; acc:  41.43; ppl: 38.10; xent: 3.64; lr: 1.00000; 6951/7526 tok/s;     26 sec\n","[2022-02-27 20:48:13,086 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 20:48:13,253 INFO] Step 580/ 2000; acc:  40.73; ppl: 40.43; xent: 3.70; lr: 1.00000; 6766/7170 tok/s;     27 sec\n","[2022-02-27 20:48:14,241 INFO] Step 600/ 2000; acc:  38.26; ppl: 47.63; xent: 3.86; lr: 1.00000; 6797/7141 tok/s;     28 sec\n","[2022-02-27 20:48:15,070 INFO] Step 620/ 2000; acc:  41.30; ppl: 36.34; xent: 3.59; lr: 1.00000; 6715/7418 tok/s;     29 sec\n","[2022-02-27 20:48:15,291 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 20:48:15,608 INFO] Validation perplexity: 22.1476\n","[2022-02-27 20:48:15,608 INFO] Validation accuracy: 47.0561\n","[2022-02-27 20:48:15,670 INFO] Saving checkpoint ./Question12/model_step_625.pt\n","[2022-02-27 20:48:16,648 INFO] Step 640/ 2000; acc:  39.64; ppl: 40.23; xent: 3.69; lr: 1.00000; 4429/4486 tok/s;     30 sec\n","[2022-02-27 20:48:17,754 INFO] Step 660/ 2000; acc:  38.66; ppl: 42.41; xent: 3.75; lr: 1.00000; 6278/6371 tok/s;     31 sec\n","[2022-02-27 20:48:18,596 INFO] Step 680/ 2000; acc:  42.44; ppl: 30.64; xent: 3.42; lr: 1.00000; 6403/7311 tok/s;     32 sec\n","[2022-02-27 20:48:19,485 INFO] Step 700/ 2000; acc:  41.47; ppl: 35.72; xent: 3.58; lr: 1.00000; 7439/7429 tok/s;     33 sec\n","[2022-02-27 20:48:20,568 INFO] Step 720/ 2000; acc:  37.38; ppl: 46.55; xent: 3.84; lr: 1.00000; 6965/7332 tok/s;     34 sec\n","[2022-02-27 20:48:21,395 INFO] Step 740/ 2000; acc:  45.64; ppl: 29.09; xent: 3.37; lr: 1.00000; 6952/7088 tok/s;     35 sec\n","[2022-02-27 20:48:22,236 INFO] Step 760/ 2000; acc:  44.36; ppl: 31.13; xent: 3.44; lr: 1.00000; 7081/7407 tok/s;     36 sec\n","[2022-02-27 20:48:23,317 INFO] Step 780/ 2000; acc:  37.22; ppl: 48.32; xent: 3.88; lr: 1.00000; 7416/7313 tok/s;     37 sec\n","[2022-02-27 20:48:24,261 INFO] Step 800/ 2000; acc:  45.27; ppl: 27.61; xent: 3.32; lr: 1.00000; 6162/6675 tok/s;     38 sec\n","[2022-02-27 20:48:25,043 INFO] Step 820/ 2000; acc:  46.77; ppl: 25.54; xent: 3.24; lr: 1.00000; 6839/7860 tok/s;     39 sec\n","[2022-02-27 20:48:25,978 INFO] Step 840/ 2000; acc:  42.96; ppl: 31.81; xent: 3.46; lr: 1.00000; 7623/7391 tok/s;     40 sec\n","[2022-02-27 20:48:26,955 INFO] Step 860/ 2000; acc:  43.32; ppl: 33.74; xent: 3.52; lr: 1.00000; 6670/7035 tok/s;     41 sec\n","[2022-02-27 20:48:27,777 INFO] Step 880/ 2000; acc:  46.09; ppl: 26.96; xent: 3.29; lr: 1.00000; 7010/7695 tok/s;     41 sec\n","[2022-02-27 20:48:28,714 INFO] Step 900/ 2000; acc:  44.76; ppl: 27.19; xent: 3.30; lr: 1.00000; 6937/7235 tok/s;     42 sec\n","[2022-02-27 20:48:29,676 INFO] Step 920/ 2000; acc:  41.35; ppl: 33.50; xent: 3.51; lr: 1.00000; 7188/7386 tok/s;     43 sec\n","[2022-02-27 20:48:30,511 INFO] Step 940/ 2000; acc:  46.17; ppl: 27.21; xent: 3.30; lr: 1.00000; 6817/7532 tok/s;     44 sec\n","[2022-02-27 20:48:31,476 INFO] Step 960/ 2000; acc:  43.94; ppl: 29.37; xent: 3.38; lr: 1.00000; 7287/7366 tok/s;     45 sec\n","[2022-02-27 20:48:32,542 INFO] Step 980/ 2000; acc:  43.74; ppl: 29.32; xent: 3.38; lr: 1.00000; 6501/6503 tok/s;     46 sec\n","[2022-02-27 20:48:33,370 INFO] Step 1000/ 2000; acc:  47.32; ppl: 23.19; xent: 3.14; lr: 1.00000; 6702/7090 tok/s;     47 sec\n","[2022-02-27 20:48:34,241 INFO] Step 1020/ 2000; acc:  44.94; ppl: 25.74; xent: 3.25; lr: 1.00000; 7607/7860 tok/s;     48 sec\n","[2022-02-27 20:48:35,342 INFO] Step 1040/ 2000; acc:  41.09; ppl: 36.31; xent: 3.59; lr: 1.00000; 6976/6939 tok/s;     49 sec\n","[2022-02-27 20:48:36,151 INFO] Step 1060/ 2000; acc:  47.34; ppl: 23.68; xent: 3.16; lr: 1.00000; 7176/7236 tok/s;     50 sec\n","[2022-02-27 20:48:37,003 INFO] Step 1080/ 2000; acc:  48.36; ppl: 21.27; xent: 3.06; lr: 1.00000; 6943/7320 tok/s;     51 sec\n","[2022-02-27 20:48:38,064 INFO] Step 1100/ 2000; acc:  41.64; ppl: 33.29; xent: 3.51; lr: 1.00000; 7556/7494 tok/s;     52 sec\n","[2022-02-27 20:48:38,938 INFO] Step 1120/ 2000; acc:  46.58; ppl: 24.01; xent: 3.18; lr: 1.00000; 6745/7242 tok/s;     53 sec\n","[2022-02-27 20:48:39,736 INFO] Step 1140/ 2000; acc:  48.49; ppl: 20.28; xent: 3.01; lr: 1.00000; 6762/7758 tok/s;     53 sec\n","[2022-02-27 20:48:40,776 INFO] Step 1160/ 2000; acc:  44.58; ppl: 26.61; xent: 3.28; lr: 1.00000; 6871/6775 tok/s;     54 sec\n","[2022-02-27 20:48:41,755 INFO] Step 1180/ 2000; acc:  46.10; ppl: 25.79; xent: 3.25; lr: 1.00000; 6590/7043 tok/s;     55 sec\n","[2022-02-27 20:48:42,546 INFO] Step 1200/ 2000; acc:  48.84; ppl: 21.41; xent: 3.06; lr: 1.00000; 7349/7620 tok/s;     56 sec\n","[2022-02-27 20:48:43,322 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 20:48:43,489 INFO] Step 1220/ 2000; acc:  45.97; ppl: 23.64; xent: 3.16; lr: 1.00000; 6838/7338 tok/s;     57 sec\n","[2022-02-27 20:48:44,498 INFO] Step 1240/ 2000; acc:  45.57; ppl: 25.34; xent: 3.23; lr: 1.00000; 6661/6921 tok/s;     58 sec\n","[2022-02-27 20:48:45,268 INFO] Validation perplexity: 12.8146\n","[2022-02-27 20:48:45,268 INFO] Validation accuracy: 55.4474\n","[2022-02-27 20:48:45,330 INFO] Saving checkpoint ./Question12/model_step_1250.pt\n","[2022-02-27 20:48:45,934 INFO] Step 1260/ 2000; acc:  49.20; ppl: 20.00; xent: 3.00; lr: 1.00000; 3903/4388 tok/s;     60 sec\n","[2022-02-27 20:48:46,876 INFO] Step 1280/ 2000; acc:  45.57; ppl: 25.32; xent: 3.23; lr: 1.00000; 7495/7454 tok/s;     61 sec\n","[2022-02-27 20:48:47,966 INFO] Step 1300/ 2000; acc:  45.19; ppl: 25.91; xent: 3.25; lr: 1.00000; 6441/6553 tok/s;     62 sec\n","[2022-02-27 20:48:48,828 INFO] Step 1320/ 2000; acc:  50.30; ppl: 17.50; xent: 2.86; lr: 1.00000; 6345/6933 tok/s;     62 sec\n","[2022-02-27 20:48:49,734 INFO] Step 1340/ 2000; acc:  47.90; ppl: 20.81; xent: 3.04; lr: 1.00000; 7352/7587 tok/s;     63 sec\n","[2022-02-27 20:48:50,783 INFO] Step 1360/ 2000; acc:  44.54; ppl: 26.81; xent: 3.29; lr: 1.00000; 7237/7346 tok/s;     64 sec\n","[2022-02-27 20:48:51,618 INFO] Step 1380/ 2000; acc:  50.32; ppl: 19.26; xent: 2.96; lr: 1.00000; 6854/7158 tok/s;     65 sec\n","[2022-02-27 20:48:52,494 INFO] Step 1400/ 2000; acc:  50.52; ppl: 18.87; xent: 2.94; lr: 1.00000; 6738/7166 tok/s;     66 sec\n","[2022-02-27 20:48:53,576 INFO] Step 1420/ 2000; acc:  44.70; ppl: 27.14; xent: 3.30; lr: 1.00000; 7278/7266 tok/s;     67 sec\n","[2022-02-27 20:48:54,508 INFO] Step 1440/ 2000; acc:  51.37; ppl: 17.93; xent: 2.89; lr: 1.00000; 6094/6690 tok/s;     68 sec\n","[2022-02-27 20:48:55,273 INFO] Step 1460/ 2000; acc:  53.56; ppl: 15.20; xent: 2.72; lr: 1.00000; 6953/7539 tok/s;     69 sec\n","[2022-02-27 20:48:56,246 INFO] Step 1480/ 2000; acc:  48.73; ppl: 19.82; xent: 2.99; lr: 1.00000; 7215/7376 tok/s;     70 sec\n","[2022-02-27 20:48:57,234 INFO] Step 1500/ 2000; acc:  48.41; ppl: 20.68; xent: 3.03; lr: 1.00000; 6628/7028 tok/s;     71 sec\n","[2022-02-27 20:48:58,078 INFO] Step 1520/ 2000; acc:  51.08; ppl: 16.86; xent: 2.82; lr: 1.00000; 6868/7689 tok/s;     72 sec\n","[2022-02-27 20:48:59,023 INFO] Step 1540/ 2000; acc:  49.93; ppl: 18.24; xent: 2.90; lr: 1.00000; 6962/7144 tok/s;     73 sec\n","[2022-02-27 20:49:00,000 INFO] Step 1560/ 2000; acc:  46.84; ppl: 22.91; xent: 3.13; lr: 1.00000; 7173/7080 tok/s;     74 sec\n","[2022-02-27 20:49:00,834 INFO] Step 1580/ 2000; acc:  51.49; ppl: 16.86; xent: 2.83; lr: 1.00000; 6912/7651 tok/s;     74 sec\n","[2022-02-27 20:49:01,781 INFO] Step 1600/ 2000; acc:  49.09; ppl: 18.55; xent: 2.92; lr: 1.00000; 7504/7468 tok/s;     75 sec\n","[2022-02-27 20:49:02,817 INFO] Step 1620/ 2000; acc:  47.79; ppl: 21.24; xent: 3.06; lr: 1.00000; 6783/6759 tok/s;     76 sec\n","[2022-02-27 20:49:03,659 INFO] Step 1640/ 2000; acc:  52.74; ppl: 15.70; xent: 2.75; lr: 1.00000; 6605/6947 tok/s;     77 sec\n","[2022-02-27 20:49:04,559 INFO] Step 1660/ 2000; acc:  49.36; ppl: 16.86; xent: 2.83; lr: 1.00000; 7386/7682 tok/s;     78 sec\n","[2022-02-27 20:49:05,645 INFO] Step 1680/ 2000; acc:  47.03; ppl: 21.21; xent: 3.05; lr: 1.00000; 6949/6971 tok/s;     79 sec\n","[2022-02-27 20:49:06,479 INFO] Step 1700/ 2000; acc:  52.32; ppl: 15.70; xent: 2.75; lr: 1.00000; 6882/6906 tok/s;     80 sec\n","[2022-02-27 20:49:07,327 INFO] Step 1720/ 2000; acc:  53.52; ppl: 14.58; xent: 2.68; lr: 1.00000; 6856/7429 tok/s;     81 sec\n","[2022-02-27 20:49:08,403 INFO] Step 1740/ 2000; acc:  45.30; ppl: 24.15; xent: 3.18; lr: 1.00000; 7467/7325 tok/s;     82 sec\n","[2022-02-27 20:49:09,299 INFO] Step 1760/ 2000; acc:  51.78; ppl: 15.13; xent: 2.72; lr: 1.00000; 6544/7099 tok/s;     83 sec\n","[2022-02-27 20:49:10,084 INFO] Step 1780/ 2000; acc:  53.63; ppl: 13.85; xent: 2.63; lr: 1.00000; 6887/7652 tok/s;     84 sec\n","[2022-02-27 20:49:11,125 INFO] Step 1800/ 2000; acc:  49.15; ppl: 17.67; xent: 2.87; lr: 1.00000; 6818/6965 tok/s;     85 sec\n","[2022-02-27 20:49:12,086 INFO] Step 1820/ 2000; acc:  50.70; ppl: 17.34; xent: 2.85; lr: 1.00000; 6650/7072 tok/s;     86 sec\n","[2022-02-27 20:49:12,869 INFO] Step 1840/ 2000; acc:  52.95; ppl: 14.30; xent: 2.66; lr: 1.00000; 7398/7590 tok/s;     87 sec\n","[2022-02-27 20:49:13,644 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 20:49:13,821 INFO] Step 1860/ 2000; acc:  50.67; ppl: 16.59; xent: 2.81; lr: 1.00000; 6754/7320 tok/s;     87 sec\n","[2022-02-27 20:49:14,949 INFO] Validation perplexity: 9.88105\n","[2022-02-27 20:49:14,950 INFO] Validation accuracy: 59.2026\n","[2022-02-27 20:49:15,007 INFO] Saving checkpoint ./Question12/model_step_1875.pt\n","[2022-02-27 20:49:15,418 INFO] Step 1880/ 2000; acc:  50.27; ppl: 17.45; xent: 2.86; lr: 1.00000; 4273/4483 tok/s;     89 sec\n","[2022-02-27 20:49:16,277 INFO] Step 1900/ 2000; acc:  53.91; ppl: 13.14; xent: 2.58; lr: 1.00000; 6568/7315 tok/s;     90 sec\n","[2022-02-27 20:49:17,251 INFO] Step 1920/ 2000; acc:  49.28; ppl: 18.36; xent: 2.91; lr: 1.00000; 7296/7220 tok/s;     91 sec\n","[2022-02-27 20:49:18,323 INFO] Step 1940/ 2000; acc:  48.72; ppl: 18.53; xent: 2.92; lr: 1.00000; 6577/6682 tok/s;     92 sec\n","[2022-02-27 20:49:19,177 INFO] Step 1960/ 2000; acc:  54.67; ppl: 13.14; xent: 2.58; lr: 1.00000; 6474/6822 tok/s;     93 sec\n","[2022-02-27 20:49:20,087 INFO] Step 1980/ 2000; acc:  52.04; ppl: 14.67; xent: 2.69; lr: 1.00000; 7343/7775 tok/s;     94 sec\n","[2022-02-27 20:49:21,159 INFO] Step 2000/ 2000; acc:  47.88; ppl: 19.66; xent: 2.98; lr: 1.00000; 7070/7204 tok/s;     95 sec\n","[2022-02-27 20:49:21,221 INFO] Saving checkpoint ./Question12/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_train -config Question12/config-base384.yaml"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8822,"status":"ok","timestamp":1645994978444,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"XZgQppcG7flZ","outputId":"a262648e-84df-4a8d-aa2d-27af2146faf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:49:31,683 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:49:37,874 INFO] PRED AVG SCORE: -0.9189, PRED PPL: 2.5066\n","BLEU = 20.05, 55.3/26.8/17.1/9.3 (BP=0.910, ratio=0.914, hyp_len=3267, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model Question12/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question12/pred2000_384.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question12/pred2000_384.txt"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100214,"status":"ok","timestamp":1645995083571,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"1_J-NYtG8XP8","outputId":"8c63310f-b679-43ca-ebf7-194fd1c35ca5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-02-27 20:49:44,968 INFO] Missing transforms field for train data, set to default: [].\n","[2022-02-27 20:49:44,969 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-02-27 20:49:44,970 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-02-27 20:49:44,970 INFO] Parsed 2 corpora from -data.\n","[2022-02-27 20:49:44,971 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-02-27 20:49:44,971 INFO] Loading vocab from text file...\n","[2022-02-27 20:49:44,972 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-02-27 20:49:45,001 INFO] Loaded src vocab has 10413 tokens.\n","[2022-02-27 20:49:45,010 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-02-27 20:49:45,032 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-02-27 20:49:45,036 INFO] Building fields with vocab in counters...\n","[2022-02-27 20:49:45,046 INFO]  * tgt vocab size: 8198.\n","[2022-02-27 20:49:45,059 INFO]  * src vocab size: 10415.\n","[2022-02-27 20:49:45,060 INFO]  * src vocab size = 10415\n","[2022-02-27 20:49:45,061 INFO]  * tgt vocab size = 8198\n","[2022-02-27 20:49:45,065 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:49:47,716 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(10415, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 192, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(884, 384)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=384, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-02-27 20:49:47,717 INFO] encoder: 6273484\n","[2022-02-27 20:49:47,717 INFO] decoder: 9205950\n","[2022-02-27 20:49:47,717 INFO] * number of parameters: 15479434\n","[2022-02-27 20:49:47,719 INFO] Starting training on GPU: [0]\n","[2022-02-27 20:49:47,719 INFO] Start training loop and validate every 625 steps...\n","[2022-02-27 20:49:47,720 INFO] train's transforms: TransformPipe()\n","[2022-02-27 20:49:47,720 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-02-27 20:49:48,744 INFO] Step 20/ 2000; acc:   8.41; ppl: 2330.40; xent: 7.75; lr: 1.00000; 6296/6350 tok/s;      1 sec\n","[2022-02-27 20:49:49,800 INFO] Step 40/ 2000; acc:  12.55; ppl: 609.33; xent: 6.41; lr: 1.00000; 6821/6994 tok/s;      2 sec\n","[2022-02-27 20:49:50,663 INFO] Step 60/ 2000; acc:  18.74; ppl: 249.70; xent: 5.52; lr: 1.00000; 6671/7502 tok/s;      3 sec\n","[2022-02-27 20:49:51,608 INFO] Step 80/ 2000; acc:  19.10; ppl: 241.01; xent: 5.48; lr: 1.00000; 6803/6921 tok/s;      4 sec\n","[2022-02-27 20:49:52,577 INFO] Step 100/ 2000; acc:  20.29; ppl: 211.19; xent: 5.35; lr: 1.00000; 6985/7261 tok/s;      5 sec\n","[2022-02-27 20:49:53,503 INFO] Step 120/ 2000; acc:  22.61; ppl: 145.78; xent: 4.98; lr: 1.00000; 7265/7374 tok/s;      6 sec\n","[2022-02-27 20:49:54,452 INFO] Step 140/ 2000; acc:  24.48; ppl: 137.66; xent: 4.92; lr: 1.00000; 6683/7012 tok/s;      7 sec\n","[2022-02-27 20:49:55,405 INFO] Step 160/ 2000; acc:  25.12; ppl: 136.28; xent: 4.91; lr: 1.00000; 7011/7448 tok/s;      8 sec\n","[2022-02-27 20:49:56,301 INFO] Step 180/ 2000; acc:  27.98; ppl: 100.45; xent: 4.61; lr: 1.00000; 7432/7536 tok/s;      9 sec\n","[2022-02-27 20:49:57,208 INFO] Step 200/ 2000; acc:  31.48; ppl: 89.56; xent: 4.49; lr: 1.00000; 7161/7499 tok/s;      9 sec\n","[2022-02-27 20:49:58,075 INFO] Step 220/ 2000; acc:  32.37; ppl: 83.44; xent: 4.42; lr: 1.00000; 7020/7119 tok/s;     10 sec\n","[2022-02-27 20:49:59,061 INFO] Step 240/ 2000; acc:  29.90; ppl: 84.67; xent: 4.44; lr: 1.00000; 6478/6981 tok/s;     11 sec\n","[2022-02-27 20:49:59,965 INFO] Step 260/ 2000; acc:  33.93; ppl: 64.77; xent: 4.17; lr: 1.00000; 6951/7665 tok/s;     12 sec\n","[2022-02-27 20:50:00,764 INFO] Step 280/ 2000; acc:  37.18; ppl: 57.83; xent: 4.06; lr: 1.00000; 6676/6995 tok/s;     13 sec\n","[2022-02-27 20:50:01,831 INFO] Step 300/ 2000; acc:  30.92; ppl: 83.32; xent: 4.42; lr: 1.00000; 6974/7077 tok/s;     14 sec\n","[2022-02-27 20:50:02,655 INFO] Step 320/ 2000; acc:  37.40; ppl: 53.10; xent: 3.97; lr: 1.00000; 6925/7821 tok/s;     15 sec\n","[2022-02-27 20:50:03,611 INFO] Step 340/ 2000; acc:  34.58; ppl: 63.03; xent: 4.14; lr: 1.00000; 6803/6748 tok/s;     16 sec\n","[2022-02-27 20:50:04,648 INFO] Step 360/ 2000; acc:  32.12; ppl: 70.99; xent: 4.26; lr: 1.00000; 7030/7338 tok/s;     17 sec\n","[2022-02-27 20:50:05,473 INFO] Step 380/ 2000; acc:  39.13; ppl: 46.54; xent: 3.84; lr: 1.00000; 7242/7298 tok/s;     18 sec\n","[2022-02-27 20:50:06,399 INFO] Step 400/ 2000; acc:  35.87; ppl: 50.85; xent: 3.93; lr: 1.00000; 6950/6986 tok/s;     19 sec\n","[2022-02-27 20:50:07,352 INFO] Step 420/ 2000; acc:  35.22; ppl: 60.95; xent: 4.11; lr: 1.00000; 7103/7360 tok/s;     20 sec\n","[2022-02-27 20:50:08,277 INFO] Step 440/ 2000; acc:  37.61; ppl: 51.68; xent: 3.95; lr: 1.00000; 7390/7303 tok/s;     21 sec\n","[2022-02-27 20:50:09,216 INFO] Step 460/ 2000; acc:  38.18; ppl: 49.16; xent: 3.90; lr: 1.00000; 6740/6800 tok/s;     21 sec\n","[2022-02-27 20:50:10,186 INFO] Step 480/ 2000; acc:  36.96; ppl: 53.22; xent: 3.97; lr: 1.00000; 6822/7102 tok/s;     22 sec\n","[2022-02-27 20:50:11,132 INFO] Step 500/ 2000; acc:  37.66; ppl: 48.29; xent: 3.88; lr: 1.00000; 6975/7303 tok/s;     23 sec\n","[2022-02-27 20:50:12,048 INFO] Step 520/ 2000; acc:  39.97; ppl: 41.25; xent: 3.72; lr: 1.00000; 7112/7517 tok/s;     24 sec\n","[2022-02-27 20:50:12,905 INFO] Step 540/ 2000; acc:  39.47; ppl: 46.89; xent: 3.85; lr: 1.00000; 7171/7467 tok/s;     25 sec\n","[2022-02-27 20:50:13,869 INFO] Step 560/ 2000; acc:  39.76; ppl: 43.06; xent: 3.76; lr: 1.00000; 6822/7212 tok/s;     26 sec\n","[2022-02-27 20:50:14,547 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-02-27 20:50:14,792 INFO] Step 580/ 2000; acc:  41.10; ppl: 38.03; xent: 3.64; lr: 1.00000; 6817/7368 tok/s;     27 sec\n","[2022-02-27 20:50:15,619 INFO] Step 600/ 2000; acc:  43.61; ppl: 32.39; xent: 3.48; lr: 1.00000; 6361/6922 tok/s;     28 sec\n","[2022-02-27 20:50:16,683 INFO] Step 620/ 2000; acc:  36.43; ppl: 53.05; xent: 3.97; lr: 1.00000; 6944/6986 tok/s;     29 sec\n","[2022-02-27 20:50:16,895 INFO] valid's transforms: TransformPipe()\n","[2022-02-27 20:50:17,209 INFO] Validation perplexity: 20.8919\n","[2022-02-27 20:50:17,210 INFO] Validation accuracy: 50.0464\n","[2022-02-27 20:50:17,271 INFO] Saving checkpoint ./Question12/model_step_625.pt\n","[2022-02-27 20:50:18,101 INFO] Step 640/ 2000; acc:  43.28; ppl: 32.53; xent: 3.48; lr: 1.00000; 4045/4474 tok/s;     30 sec\n","[2022-02-27 20:50:19,072 INFO] Step 660/ 2000; acc:  41.47; ppl: 36.53; xent: 3.60; lr: 1.00000; 6593/6756 tok/s;     31 sec\n","[2022-02-27 20:50:20,135 INFO] Step 680/ 2000; acc:  38.10; ppl: 45.59; xent: 3.82; lr: 1.00000; 6720/6728 tok/s;     32 sec\n","[2022-02-27 20:50:20,955 INFO] Step 700/ 2000; acc:  44.22; ppl: 28.76; xent: 3.36; lr: 1.00000; 7029/7737 tok/s;     33 sec\n","[2022-02-27 20:50:21,878 INFO] Step 720/ 2000; acc:  42.41; ppl: 33.29; xent: 3.51; lr: 1.00000; 6997/7164 tok/s;     34 sec\n","[2022-02-27 20:50:22,851 INFO] Step 740/ 2000; acc:  40.29; ppl: 40.62; xent: 3.70; lr: 1.00000; 7046/7337 tok/s;     35 sec\n","[2022-02-27 20:50:23,781 INFO] Step 760/ 2000; acc:  42.38; ppl: 35.08; xent: 3.56; lr: 1.00000; 7358/7428 tok/s;     36 sec\n","[2022-02-27 20:50:24,755 INFO] Step 780/ 2000; acc:  43.24; ppl: 32.60; xent: 3.48; lr: 1.00000; 6438/6692 tok/s;     37 sec\n","[2022-02-27 20:50:25,656 INFO] Step 800/ 2000; acc:  41.63; ppl: 38.16; xent: 3.64; lr: 1.00000; 7294/7578 tok/s;     38 sec\n","[2022-02-27 20:50:26,614 INFO] Step 820/ 2000; acc:  43.07; ppl: 30.96; xent: 3.43; lr: 1.00000; 6717/7338 tok/s;     39 sec\n","[2022-02-27 20:50:27,525 INFO] Step 840/ 2000; acc:  44.06; ppl: 29.21; xent: 3.37; lr: 1.00000; 7082/7527 tok/s;     40 sec\n","[2022-02-27 20:50:28,385 INFO] Step 860/ 2000; acc:  45.25; ppl: 30.39; xent: 3.41; lr: 1.00000; 7161/7239 tok/s;     41 sec\n","[2022-02-27 20:50:29,355 INFO] Step 880/ 2000; acc:  43.56; ppl: 29.90; xent: 3.40; lr: 1.00000; 6669/6975 tok/s;     42 sec\n","[2022-02-27 20:50:30,267 INFO] Step 900/ 2000; acc:  45.49; ppl: 25.66; xent: 3.24; lr: 1.00000; 6943/7561 tok/s;     43 sec\n","[2022-02-27 20:50:31,057 INFO] Step 920/ 2000; acc:  48.64; ppl: 23.56; xent: 3.16; lr: 1.00000; 6813/7239 tok/s;     43 sec\n","[2022-02-27 20:50:32,111 INFO] Step 940/ 2000; acc:  40.89; ppl: 38.78; xent: 3.66; lr: 1.00000; 7195/7244 tok/s;     44 sec\n","[2022-02-27 20:50:32,945 INFO] Step 960/ 2000; acc:  46.52; ppl: 23.73; xent: 3.17; lr: 1.00000; 6951/7663 tok/s;     45 sec\n","[2022-02-27 20:50:33,922 INFO] Step 980/ 2000; acc:  45.62; ppl: 25.61; xent: 3.24; lr: 1.00000; 6587/6592 tok/s;     46 sec\n","[2022-02-27 20:50:34,918 INFO] Step 1000/ 2000; acc:  41.20; ppl: 33.31; xent: 3.51; lr: 1.00000; 7171/7356 tok/s;     47 sec\n","[2022-02-27 20:50:35,769 INFO] Step 1020/ 2000; acc:  48.13; ppl: 21.64; xent: 3.07; lr: 1.00000; 6917/7143 tok/s;     48 sec\n","[2022-02-27 20:50:36,673 INFO] Step 1040/ 2000; acc:  46.25; ppl: 24.86; xent: 3.21; lr: 1.00000; 7199/7111 tok/s;     49 sec\n","[2022-02-27 20:50:37,660 INFO] Step 1060/ 2000; acc:  43.36; ppl: 30.13; xent: 3.41; lr: 1.00000; 7035/7256 tok/s;     50 sec\n","[2022-02-27 20:50:38,593 INFO] Step 1080/ 2000; acc:  44.40; ppl: 28.78; xent: 3.36; lr: 1.00000; 7413/7301 tok/s;     51 sec\n","[2022-02-27 20:50:39,513 INFO] Step 1100/ 2000; acc:  45.54; ppl: 25.34; xent: 3.23; lr: 1.00000; 6884/7060 tok/s;     52 sec\n","[2022-02-27 20:50:40,428 INFO] Step 1120/ 2000; acc:  44.64; ppl: 28.40; xent: 3.35; lr: 1.00000; 7135/7521 tok/s;     53 sec\n","[2022-02-27 20:50:41,370 INFO] Step 1140/ 2000; acc:  45.41; ppl: 25.56; xent: 3.24; lr: 1.00000; 6966/7351 tok/s;     54 sec\n","[2022-02-27 20:50:42,293 INFO] Step 1160/ 2000; acc:  47.44; ppl: 21.96; xent: 3.09; lr: 1.00000; 7003/7486 tok/s;     55 sec\n","[2022-02-27 20:50:43,185 INFO] Step 1180/ 2000; acc:  46.87; ppl: 24.02; xent: 3.18; lr: 1.00000; 6817/7251 tok/s;     55 sec\n","[2022-02-27 20:50:44,152 INFO] Step 1200/ 2000; acc:  45.50; ppl: 24.91; xent: 3.22; lr: 1.00000; 6727/7140 tok/s;     56 sec\n","[2022-02-27 20:50:44,786 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-02-27 20:50:45,043 INFO] Step 1220/ 2000; acc:  47.87; ppl: 21.26; xent: 3.06; lr: 1.00000; 7054/7426 tok/s;     57 sec\n","[2022-02-27 20:50:45,855 INFO] Step 1240/ 2000; acc:  49.51; ppl: 20.32; xent: 3.01; lr: 1.00000; 6548/6864 tok/s;     58 sec\n","[2022-02-27 20:50:46,718 INFO] Validation perplexity: 13.2306\n","[2022-02-27 20:50:46,718 INFO] Validation accuracy: 55.2851\n","[2022-02-27 20:50:46,782 INFO] Saving checkpoint ./Question12/model_step_1250.pt\n","[2022-02-27 20:50:47,510 INFO] Step 1260/ 2000; acc:  43.20; ppl: 31.39; xent: 3.45; lr: 1.00000; 4452/4576 tok/s;     60 sec\n","[2022-02-27 20:50:48,361 INFO] Step 1280/ 2000; acc:  49.75; ppl: 18.11; xent: 2.90; lr: 1.00000; 6803/7588 tok/s;     61 sec\n","[2022-02-27 20:50:49,368 INFO] Step 1300/ 2000; acc:  47.81; ppl: 21.03; xent: 3.05; lr: 1.00000; 6438/6630 tok/s;     62 sec\n","[2022-02-27 20:50:50,401 INFO] Step 1320/ 2000; acc:  44.73; ppl: 26.23; xent: 3.27; lr: 1.00000; 6948/7120 tok/s;     63 sec\n","[2022-02-27 20:50:51,255 INFO] Step 1340/ 2000; acc:  50.84; ppl: 16.71; xent: 2.82; lr: 1.00000; 6863/7300 tok/s;     64 sec\n","[2022-02-27 20:50:52,176 INFO] Step 1360/ 2000; acc:  48.32; ppl: 21.08; xent: 3.05; lr: 1.00000; 6994/7027 tok/s;     64 sec\n","[2022-02-27 20:50:53,156 INFO] Step 1380/ 2000; acc:  45.26; ppl: 24.86; xent: 3.21; lr: 1.00000; 6993/7299 tok/s;     65 sec\n","[2022-02-27 20:50:54,087 INFO] Step 1400/ 2000; acc:  48.48; ppl: 21.41; xent: 3.06; lr: 1.00000; 7320/7445 tok/s;     66 sec\n","[2022-02-27 20:50:55,125 INFO] Step 1420/ 2000; acc:  48.62; ppl: 20.09; xent: 3.00; lr: 1.00000; 5942/6380 tok/s;     67 sec\n","[2022-02-27 20:50:56,039 INFO] Step 1440/ 2000; acc:  48.38; ppl: 21.79; xent: 3.08; lr: 1.00000; 7058/7174 tok/s;     68 sec\n","[2022-02-27 20:50:56,968 INFO] Step 1460/ 2000; acc:  48.55; ppl: 19.13; xent: 2.95; lr: 1.00000; 6745/7525 tok/s;     69 sec\n","[2022-02-27 20:50:57,881 INFO] Step 1480/ 2000; acc:  49.09; ppl: 18.44; xent: 2.91; lr: 1.00000; 7056/7496 tok/s;     70 sec\n","[2022-02-27 20:50:58,742 INFO] Step 1500/ 2000; acc:  49.16; ppl: 20.00; xent: 3.00; lr: 1.00000; 7194/7349 tok/s;     71 sec\n","[2022-02-27 20:50:59,716 INFO] Step 1520/ 2000; acc:  49.66; ppl: 18.23; xent: 2.90; lr: 1.00000; 6702/7050 tok/s;     72 sec\n","[2022-02-27 20:51:00,618 INFO] Step 1540/ 2000; acc:  50.42; ppl: 18.51; xent: 2.92; lr: 1.00000; 7086/7468 tok/s;     73 sec\n","[2022-02-27 20:51:01,433 INFO] Step 1560/ 2000; acc:  54.04; ppl: 14.21; xent: 2.65; lr: 1.00000; 6656/7126 tok/s;     74 sec\n","[2022-02-27 20:51:02,481 INFO] Step 1580/ 2000; acc:  45.22; ppl: 24.84; xent: 3.21; lr: 1.00000; 7292/7402 tok/s;     75 sec\n","[2022-02-27 20:51:03,319 INFO] Step 1600/ 2000; acc:  51.92; ppl: 14.76; xent: 2.69; lr: 1.00000; 7077/7423 tok/s;     76 sec\n","[2022-02-27 20:51:04,304 INFO] Step 1620/ 2000; acc:  50.59; ppl: 16.67; xent: 2.81; lr: 1.00000; 6561/6591 tok/s;     77 sec\n","[2022-02-27 20:51:05,310 INFO] Step 1640/ 2000; acc:  45.60; ppl: 23.24; xent: 3.15; lr: 1.00000; 7214/7399 tok/s;     78 sec\n","[2022-02-27 20:51:06,144 INFO] Step 1660/ 2000; acc:  53.29; ppl: 14.65; xent: 2.68; lr: 1.00000; 7062/7299 tok/s;     78 sec\n","[2022-02-27 20:51:07,092 INFO] Step 1680/ 2000; acc:  50.84; ppl: 16.22; xent: 2.79; lr: 1.00000; 6769/6825 tok/s;     79 sec\n","[2022-02-27 20:51:08,058 INFO] Step 1700/ 2000; acc:  49.06; ppl: 20.14; xent: 3.00; lr: 1.00000; 7032/7244 tok/s;     80 sec\n","[2022-02-27 20:51:08,969 INFO] Step 1720/ 2000; acc:  49.22; ppl: 19.26; xent: 2.96; lr: 1.00000; 7482/7506 tok/s;     81 sec\n","[2022-02-27 20:51:09,915 INFO] Step 1740/ 2000; acc:  50.71; ppl: 16.30; xent: 2.79; lr: 1.00000; 6659/6829 tok/s;     82 sec\n","[2022-02-27 20:51:10,833 INFO] Step 1760/ 2000; acc:  49.88; ppl: 18.49; xent: 2.92; lr: 1.00000; 7157/7450 tok/s;     83 sec\n","[2022-02-27 20:51:11,790 INFO] Step 1780/ 2000; acc:  50.33; ppl: 17.39; xent: 2.86; lr: 1.00000; 6826/7273 tok/s;     84 sec\n","[2022-02-27 20:51:12,708 INFO] Step 1800/ 2000; acc:  52.02; ppl: 14.95; xent: 2.70; lr: 1.00000; 7020/7429 tok/s;     85 sec\n","[2022-02-27 20:51:13,594 INFO] Step 1820/ 2000; acc:  51.09; ppl: 16.10; xent: 2.78; lr: 1.00000; 6765/7277 tok/s;     86 sec\n","[2022-02-27 20:51:14,559 INFO] Step 1840/ 2000; acc:  49.51; ppl: 17.24; xent: 2.85; lr: 1.00000; 6716/7111 tok/s;     87 sec\n","[2022-02-27 20:51:15,203 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-02-27 20:51:15,476 INFO] Step 1860/ 2000; acc:  53.00; ppl: 14.00; xent: 2.64; lr: 1.00000; 6875/7177 tok/s;     88 sec\n","[2022-02-27 20:51:16,466 INFO] Validation perplexity: 9.77697\n","[2022-02-27 20:51:16,466 INFO] Validation accuracy: 59.8748\n","[2022-02-27 20:51:16,530 INFO] Saving checkpoint ./Question12/model_step_1875.pt\n","[2022-02-27 20:51:16,915 INFO] Step 1880/ 2000; acc:  54.35; ppl: 13.78; xent: 2.62; lr: 1.00000; 3733/3859 tok/s;     89 sec\n","[2022-02-27 20:51:17,988 INFO] Step 1900/ 2000; acc:  47.03; ppl: 21.44; xent: 3.07; lr: 1.00000; 6976/7128 tok/s;     90 sec\n","[2022-02-27 20:51:18,837 INFO] Step 1920/ 2000; acc:  54.56; ppl: 12.14; xent: 2.50; lr: 1.00000; 6831/7738 tok/s;     91 sec\n","[2022-02-27 20:51:19,829 INFO] Step 1940/ 2000; acc:  52.29; ppl: 15.29; xent: 2.73; lr: 1.00000; 6570/6783 tok/s;     92 sec\n","[2022-02-27 20:51:20,887 INFO] Step 1960/ 2000; acc:  48.11; ppl: 18.64; xent: 2.93; lr: 1.00000; 6795/7088 tok/s;     93 sec\n","[2022-02-27 20:51:21,710 INFO] Step 1980/ 2000; acc:  55.46; ppl: 12.07; xent: 2.49; lr: 1.00000; 7198/7467 tok/s;     94 sec\n","[2022-02-27 20:51:22,627 INFO] Step 2000/ 2000; acc:  52.51; ppl: 14.66; xent: 2.68; lr: 1.00000; 7055/7186 tok/s;     95 sec\n","[2022-02-27 20:51:22,692 INFO] Saving checkpoint ./Question12/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_train -config Question12/config-base384.yaml"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9121,"status":"ok","timestamp":1645995100574,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"O8_99Arj8aPg","outputId":"b42a2d28-9ea2-4af6-b040-cbbf1ef1a064"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 20:51:33,776 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 20:51:40,137 INFO] PRED AVG SCORE: -0.9113, PRED PPL: 2.4876\n","BLEU = 18.96, 54.8/26.2/16.1/8.4 (BP=0.904, ratio=0.908, hyp_len=3246, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model Question12/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Question12/pred2000_512.txt\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Question12/pred2000_512.txt"]},{"cell_type":"markdown","metadata":{"id":"pg1_kfYh8oe1"},"source":["Avec 256 unités, on a 18,32% pour 2000 étapes.\n","Avec 384 unités, on a 20,05 %\n","Avec 512 unités, on a 18,96%\n","On obtient ainsi des meilleurs résultats.\n","\n","<span style=\"color:red\">Pourquoi? Sinon, vous auraît toujours du utiliser le early stopping (voir commentaire précédente).</span>"]},{"cell_type":"markdown","metadata":{"id":"aheYVc2mQnoQ"},"source":["# Question 14\n"]},{"cell_type":"markdown","metadata":{"id":"qmERCWhXQqqy"},"source":["Le Beam Search est une technique qui permet de rechecher plusieurs candidats à la fois au lieu d'un seul, supposé être le meilleur, dans le cadre de la traduction pour chaque séquence d'entrée à chaque timestep. La méthode repose sur des probabilités conditionnelles et permet de retenir les B meilleures alternatives (celles qui ont la probabilité la plus élevée) où B est le paramètre de Beam Search. La taille par défaut du Beam Search est de 1 (un seul candidat proposé).\n","\n","<span style=\"color:red\"> La Beam Search est un algo qu'on utilise au décodage pour chercher la chaine des tokens cible qui mieux correspond à la source.  Cet algorithme est une approximation efficiente de l'algorithme de parcours en largeur. Pour un beam search initialisé à **B**, le modèle devra sélectionner les **B** séquences les plus susceptibles d'apparaître à chaque étape du décodage (étape de décodage=décodage du token suivant) en utilisant les probabilités d'occurence. Théoriquement, la traduction faite par le modèle serait d'autant meilleure que la taille du beam search est grande, car la recherche sera plus aple. Toutefois, un beam search très grand nécessitera d'avantage de ressources (mémoires et processeurs). </span>"]},{"cell_type":"markdown","metadata":{"id":"2ww7fdNE9omU"},"source":["## Question 15"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70687,"status":"ok","timestamp":1645995830091,"user":{"displayName":"Guillaume Sauvat","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16964327129706648277"},"user_tz":-60},"id":"Ckf6YkYX9mjm","outputId":"266b98ce-9579-41aa-a3c2-7439da656dac"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:02:41,332 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:02:43,071 INFO] PRED AVG SCORE: -1.1794, PRED PPL: 3.2523\n","BLEU = 16.98, 47.7/21.2/12.7/6.5 (BP=1.000, ratio=1.077, hyp_len=3849, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:02:45,047 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:02:47,421 INFO] PRED AVG SCORE: -1.0812, PRED PPL: 2.9483\n","BLEU = 18.51, 50.2/22.8/14.1/7.3 (BP=1.000, ratio=1.013, hyp_len=3620, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:02:49,416 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:02:52,520 INFO] PRED AVG SCORE: -1.0438, PRED PPL: 2.8401\n","BLEU = 19.06, 51.1/24.0/14.8/7.7 (BP=0.986, ratio=0.986, hyp_len=3523, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:02:54,579 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:02:58,463 INFO] PRED AVG SCORE: -1.0302, PRED PPL: 2.8017\n","BLEU = 19.14, 52.2/24.8/15.4/8.0 (BP=0.959, ratio=0.960, hyp_len=3430, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:03:00,545 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:03:05,128 INFO] PRED AVG SCORE: -1.0240, PRED PPL: 2.7843\n","BLEU = 18.32, 52.0/24.3/14.8/7.4 (BP=0.952, ratio=0.953, hyp_len=3405, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:03:07,188 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:03:12,546 INFO] PRED AVG SCORE: -1.0177, PRED PPL: 2.7669\n","BLEU = 18.56, 52.5/24.9/15.2/7.6 (BP=0.941, ratio=0.942, hyp_len=3368, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:03:14,607 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:03:20,623 INFO] PRED AVG SCORE: -1.0168, PRED PPL: 2.7644\n","BLEU = 18.70, 53.0/25.1/15.4/7.9 (BP=0.933, ratio=0.935, hyp_len=3343, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:03:22,677 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:03:29,436 INFO] PRED AVG SCORE: -1.0118, PRED PPL: 2.7505\n","BLEU = 18.44, 52.9/24.9/15.3/7.8 (BP=0.926, ratio=0.929, hyp_len=3319, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:03:31,473 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:03:38,868 INFO] PRED AVG SCORE: -1.0124, PRED PPL: 2.7521\n","BLEU = 18.52, 53.2/25.3/15.6/8.0 (BP=0.914, ratio=0.917, hyp_len=3278, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-02-27 21:03:40,969 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-02-27 21:03:49,235 INFO] PRED AVG SCORE: -1.0115, PRED PPL: 2.7498\n","BLEU = 18.52, 53.5/25.5/15.8/8.1 (BP=0.907, ratio=0.911, hyp_len=3255, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam1.txt --beam_size 1\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam1.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam2.txt --beam_size 2\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam2.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam3.txt --beam_size 3\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam3.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam4.txt --beam_size 4\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam4.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam5.txt --beam_size 5\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam5.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam6.txt --beam_size 6\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam6.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam7.txt --beam_size 7\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam7.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam8.txt --beam_size 8\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam8.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam9.txt --beam_size 9\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam9.txt\n","onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output models/base/pred2000_beam10.txt --beam_size 10\n","perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < models/base/pred2000_beam10.txt"]},{"cell_type":"markdown","metadata":{"id":"JSUkga3Z_mH1"},"source":["Le score est maximum avec une beam de taille 4\n","\n","<span style=\"color:red\"> Comment évolue le score BLEU ? Pourquoi? </span>"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"tp_nmt_sauvat_quillot.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
