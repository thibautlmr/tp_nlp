{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjf5pW7H0Guf"
      },
      "source": [
        "# TP Relation Extraction\n",
        "\n",
        "## Brieuc Calvez - Simon Ardiet (3A IF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kuQK4zd1kbL"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efVai6Q31Cbh"
      },
      "source": [
        "Dans ce TP nous allons:\n",
        "\n",
        "1. Découvrir la tache d’extraction de relation par réseaux neuronaux  (« neural relation extraction », NRE)\n",
        "2.  Comprendre le fonctionnement d’un modèle d’extraction de relation avec un encodeur BERT\n",
        "3. Découvrir la tache de reconnaissance d'entités nommées (« named-entity recognition », NER)\n",
        "4. Coder une pipeline d’extraction de relation par réseaux neuronaux pour des jeux de données textuels\n",
        "\n",
        "Avec les outils suivants:\n",
        "1. La libraire [OpenNRE](https://github.com/thunlp/OpenNRE), basée sur Pytorch et HuggingFace’s Transformers, pour la tache d’extraction de relation par réseaux neuronaux\n",
        "2. [HuggingFace’s Transformers](https://huggingface.co/transformers/) : une bibliothèque basée sur Pytorch pour le traitement automatique des langues et notamment les modèles neuronaux de type Transformer (comme BERT)\n",
        "3. Google Colab, qui héberge ce *Jupyter Notebook*. Avant de commencer le TP, vous pouvez consulter des pages d'introductions [à Colab](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb#scrollTo=YHI3vyhv5p85) et [aux Notebooks](https://realpython.com/jupyter-notebook-introduction/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhcmAWU7FISp"
      },
      "source": [
        "Contrairement au dernier TP, cette fois-ci nous n’aurons pas besoin d’utiliser une GPU car nous n’allons pas entraîner des nouveaux modèles ou faire de l’inférence sur des grands jeux de données: une CPU suffira. Nous pouvons vérifier si l’on est en train d’utiliser une CPU ou une GPU avec les lignes suivantes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9xBd3ZGgVOB",
        "outputId": "9f18d511-fc85-4ff3-d9e6-d0f154f12b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available.\")\n",
        "  device = torch.cuda.current_device()\n",
        "else:\n",
        "  print(\"Will work on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr1QqSFEGGIH"
      },
      "source": [
        "Nous avons besoin d’installer l’outil OpenNRE. Pour éviter de devoir ré-télécharger le répertoire  GitHub de OpenNRE à chaque fois qu’on ré-initialise le fichier Colab, il est convenant de monter notre répertoire Google Drive et y télécharger le répertoire OpenNRE de façon à l’avoir toujours disponible. Ainsi, à chaque fois que nous allons ré-initialiser le fichier Colab, il nous suffira de monter notre Goodle Drive pour avoir accès à la libraire OpenNRE.\n",
        "\n",
        "> ATTENTION: modifiez *tp_path_in_drive* pour pointer vers le repertoire où vous avez placé le fichier tp_re.ipynb, vous allez télécharger OpenNRE dans le meme réépertoire. Si vous êtes sur votre machine locale, vous n'avez pas besoin de monter le Drive, mais juste de faire le clonage du réépertoire OpenNRE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmWpFhonvmNH",
        "outputId": "5d11fb5a-c8fa-453d-f1f8-9d365d29b122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "OpenNRE is already present in Google Drive under /content/gdrive/MyDrive/Ensimag/3A Ensimag/Traitement du langage/4-tp_re/OpenNRE\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "tp_path_in_drive = '/content/gdrive/MyDrive/Ensimag/3A Ensimag/Traitement du langage/4-tp_re'\n",
        "opennre_path_in_drive = tp_path_in_drive + '/OpenNRE'\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if not os.path.isdir(opennre_path_in_drive):\n",
        "  # OpenNRE is not already present in Google Drive\n",
        "  if not os.path.isdir(tp_path_in_drive):\n",
        "    # make directory for the TP if necessary\n",
        "    os.makedirs(tp_path_in_drive, exist_ok=True)\n",
        "  # change directory to the TP directory\n",
        "  os.chdir(tp_path_in_drive)\n",
        "  # clone OpenNRE repo\n",
        "  print(\"Cloning repo...\")\n",
        "  os.system('git clone https://github.com/thunlp/OpenNRE.git')\n",
        "  print(\"...done!\")\n",
        "else:\n",
        "  print(\"OpenNRE is already present in Google Drive under {0}\".format(opennre_path_in_drive))\n",
        "\n",
        "# Change current dir to OpenNRE\n",
        "os.chdir(opennre_path_in_drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "q67JhFW7XPcu"
      },
      "outputs": [],
      "source": [
        "# Update requirements\n",
        "!sed -i '/transformers==3.0.2/c\\transformers==3.4.0' requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CofsxJQRON4p"
      },
      "source": [
        "Nous pouvons désormais continuer avec l’installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8wFXQwfMwxp",
        "outputId": "5482f4df-eab2-4654-8040-28e0210e7704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: pytest==5.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (5.3.2)\n",
            "Requirement already satisfied: scikit-learn==0.22.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.22.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: nltk>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 1)) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (0.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2022.3.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (0.0.47)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (0.1.96)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (4.11.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (0.2.5)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest==5.3.2->-r requirements.txt (line 3)) (8.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.4->-r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.3.2->-r requirements.txt (line 3)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest==5.3.2->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->-r requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing opennre.egg-info/PKG-INFO\n",
            "writing dependency_links to opennre.egg-info/dependency_links.txt\n",
            "writing top-level names to opennre.egg-info/top_level.txt\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'opennre.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/opennre\n",
            "copying build/lib/opennre/__init__.py -> build/bdist.linux-x86_64/egg/opennre\n",
            "copying build/lib/opennre/pretrain.py -> build/bdist.linux-x86_64/egg/opennre\n",
            "creating build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/__init__.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/base_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/bert_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/cnn_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "copying build/lib/opennre/encoder/pcnn_encoder.py -> build/bdist.linux-x86_64/egg/opennre/encoder\n",
            "creating build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/__init__.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/bag_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/data_loader.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/multi_label_sentence_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/sentence_re.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "copying build/lib/opennre/framework/utils.py -> build/bdist.linux-x86_64/egg/opennre/framework\n",
            "creating build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/__init__.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_attention.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_average.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/bag_one.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/base_model.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/sigmoid_nn.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "copying build/lib/opennre/model/softmax_nn.py -> build/bdist.linux-x86_64/egg/opennre/model\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module\n",
            "copying build/lib/opennre/module/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/cnn.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/lstm.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "copying build/lib/opennre/module/nn/rnn.py -> build/bdist.linux-x86_64/egg/opennre/module/nn\n",
            "creating build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/__init__.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/avg_pool.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "copying build/lib/opennre/module/pool/max_pool.py -> build/bdist.linux-x86_64/egg/opennre/module/pool\n",
            "creating build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/__init__.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/basic_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/bert_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/utils.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/word_piece_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "copying build/lib/opennre/tokenization/word_tokenizer.py -> build/bdist.linux-x86_64/egg/opennre/tokenization\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/pretrain.py to pretrain.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/base_encoder.py to base_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/bert_encoder.py to bert_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/cnn_encoder.py to cnn_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/encoder/pcnn_encoder.py to pcnn_encoder.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/bag_re.py to bag_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/data_loader.py to data_loader.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/multi_label_sentence_re.py to multi_label_sentence_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/sentence_re.py to sentence_re.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/framework/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_attention.py to bag_attention.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_average.py to bag_average.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/bag_one.py to bag_one.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/base_model.py to base_model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/sigmoid_nn.py to sigmoid_nn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/model/softmax_nn.py to softmax_nn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/cnn.py to cnn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/lstm.py to lstm.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/nn/rnn.py to rnn.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/avg_pool.py to avg_pool.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/module/pool/max_pool.py to max_pool.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/basic_tokenizer.py to basic_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/bert_tokenizer.py to bert_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/word_piece_tokenizer.py to word_piece_tokenizer.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/opennre/tokenization/word_tokenizer.py to word_tokenizer.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying opennre.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/opennre-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing opennre-0.1-py3.7.egg\n",
            "Removing /usr/local/lib/python3.7/dist-packages/opennre-0.1-py3.7.egg\n",
            "Copying opennre-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "opennre 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/opennre-0.1-py3.7.egg\n",
            "Processing dependencies for opennre==0.1\n",
            "Finished processing dependencies for opennre==0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9RIDU84lg4g"
      },
      "source": [
        "## Les modéles pour l'extraction de relation\n",
        "\n",
        "À part offrir un cadre pour l’implémentation et l’entraînement de modelés pour l’extraction de relation, OpenNRE offre aussi des modèles déjà entraînés sur différents jeux de données, et donc capables de détecter différents types de relations entre entités. \n",
        "\n",
        "Ici, nous allons employer un modèle entraîné sur Wiki80 (dataset introduit par [le papier OpenNRE](https://www.aclweb.org/anthology/D19-3029/)), un jeu de données contenant des phrases collectées sur Wikipedia et Wikidata, ainsi que des rélations entre leurs entités. Si vous voulez en savoir plus sur Wiki80, vous pouvez le télécharger avec le script [download_wiki80.sh](https://github.com/thunlp/OpenNRE/blob/60a8ceb42e1cfacbde3c8cfb5f758fb7fe96bdc4/benchmark/download_wiki80.sh) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxT-aqw2PXZO",
        "outputId": "69e97f41-b57f-4465-d77c-ce10638e714d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-14 10:56:48,670 - root - INFO - Loading BERT pre-trained checkpoint.\n"
          ]
        }
      ],
      "source": [
        "import opennre\n",
        "model = opennre.get_model('wiki80_bert_softmax')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYbZ4f4Q9Z-b"
      },
      "source": [
        "Nous pouvons utiliser ce modèle pour calculer la relation entre un mot «tête» et un mot «queue» qui sont contenus dans un texte. Il suffit de passer au modèle le texte ainsi que la position de la tête et de la queue. Le modèle retournera la relation de la queue à l’égard de la tête, ainsi que la probabilité qu’il associe à cette rélation. Par example, nous pouvons inféérer la relation entre **Áed Uaridnach** et **Máel Dúin mac Máele Fithrich** de la façon suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfpSGIJbV2gY",
        "outputId": "0ff322ab-d213-45d7-9796-927e99c93d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('child', 0.9812852144241333)\n"
          ]
        }
      ],
      "source": [
        "item = {'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (78, 91)}, 't': {'pos': (18, 46)}}\n",
        "print(model.infer(item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kacE-LI9mi1"
      },
      "source": [
        "#### Exercice 1\n",
        "\n",
        "\n",
        "\n",
        "Écrire une fonction `to_input_format(text, head, tail)` qui nous permet de trouver la position de deux mots (une tête et une queue) dans un texte, et qui retourne un dictionnaire contenant le texte et les deux positions suivant le format requis par la fonction `model.infer()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "iBlnOK1HJQN4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def to_input_format(text, head, tail):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    text: a string of text\n",
        "    head: a string of text representing a word contained in text\n",
        "    tail: a string of text representing a word contained in text but different from head\n",
        "  Returns:\n",
        "    A dictionary containing the text and the position of the head and tail within it,\n",
        "    following the input format required by an OpenNRE model (see exemple above).\n",
        "  \"\"\"\n",
        "  pos_tail = (text.find(tail), text.find(tail) + len(tail))\n",
        "  pos_head = (text.find(head), text.find(head) + len(head))\n",
        "  return {'text' : text, 'h' : {'pos': pos_head}, 't' : {'pos': pos_tail}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX1hgPDO4VVM",
        "outputId": "535c761f-65f6-42b7-98ee-603c93518395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "# Test your code with this snippet\n",
        "text = 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).'\n",
        "head = 'Áed Uaridnach'\n",
        "tail = 'Máel Dúin mac Máele Fithrich'\n",
        "test_item = to_input_format(text, head, tail)\n",
        "try:\n",
        "  assert model.infer(item) == model.infer(test_item)\n",
        "  print(\"Good job!\")\n",
        "except AssertionError:\n",
        "  print(\"Something is wrong with your function, try again!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUeTb4e12e-T"
      },
      "source": [
        "#### Exercice 2\n",
        "\n",
        "Décrire l’architecture du modèle qu’on vient de télécharger, la logique et le fonctionnement de chacun de ses composants:\n",
        "\n",
        "1. Sentence-encoder: pourquoi l'on utilise un encodeur type BERT ?\n",
        "2. À quoi sert la couche BertPooler ?\n",
        "3. À quoi sert la couche linéaire finale (fc) ? pourquoi elle réduit à 80 la dimension du vecteur sortant de l’encodeur?\n",
        "4. À quoi sert la fonction Softmax ?\n",
        "5. À quoi sert la fonction de dropout que l'on appliuque à la sortie du réseau, ainsi que dans chaque couche de BERT ?\n",
        "\n",
        "Aide: vous pouvez inspecter un modèle Pytorch avec `print(model)`. Pour mieux le comprendre, vous pouvez aussi voir [son code](https://github.com/thunlp/OpenNRE/blob/60a8ceb42e1cfacbde3c8cfb5f758fb7fe96bdc4/opennre/model/softmax_nn.py#L5), [la déscription du modèle BERT de Huggingface](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/bert#transformers.BertModel) ainsi que [son code](https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/bert/modeling_bert.py#L848)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3sj1ZuKkHwY",
        "outputId": "a8b97e23-5c8a-429a-be40-a952daa81593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SoftmaxNN(\n",
            "  (sentence_encoder): BERTEncoder(\n",
            "    (bert): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=768, out_features=80, bias=True)\n",
            "  (softmax): Softmax(dim=-1)\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnUivSOwh878"
      },
      "source": [
        "**Q1** : BERT repousse les limites du traitement du langage naturel en combinant deux technologies :\n",
        "- Il est basé sur un réseau profond de \"Transformer encoders\", un type de réseau qui peut traiter efficacement de longs textes.\n",
        "- Il est bidirectionnel, ce qui signifie qu'il utilise l'ensemble du passage du texte pour comprendre le sens de chaque mot.\n",
        "\n",
        "**Q2** : La \"pooler layer\" prend la représention en sortie correspondant au premier \"token\", et l'utilise pour certaines tâches en aval. La \"pooler layer\", applique une transformation linéaire sur la représentation du premier \"token\". La transformation linéaire est entraînée en utilisant une stratégie NSP.\n",
        "Voici le code correspondant à cette pooler layer : \n",
        "\n",
        "```\n",
        "(pooler): BertPooler(\n",
        "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "        (activation): Tanh()\n",
        "      )\n",
        "```\n",
        "\n",
        "**Q3** : Voici le code correspondant à cette layer FC : \n",
        "\n",
        "```\n",
        "(fc): Linear(in_features=768, out_features=80, bias=True)\n",
        "  (softmax): Softmax(dim=-1)\n",
        "  (drop): Dropout(p=0.5, inplace=False)\n",
        "```\n",
        "\n",
        "Dans un réseau de neurones, une couche «fully connected» se trouver généralement dans les dernières couches et permet de compiler les données extraites des autres couches, et de mettre en forme la sortie finale. C’est une couche extrêmement coûteuse puisque toutes les entrées d’une couche sont connectées à toutes les fonctions d’activation d’une autre couche.\n",
        "\n",
        "Ici, on remarque via le paramètre `out_features=80` que le vecteur de sortie a une dimension de 80 afin d’être conforme au dataset Wiki80, qui est composé de 80 relations. \n",
        "\n",
        "**Conforme dans quel sense? C'est quoi la fonction de cette couce FC? --> Cette couche calcule la probabilité associée à chaque classe (chaque rélation définie dans le dataser Wiki80) à partir de l'information contenue dans la représentation véctorielle du token [CLS].**\n",
        "\n",
        "**Q4** : La fonction `Softmax` est utilisée pour normaliser la sortie des réseaux neuronaux afin qu'elle soit comprise entre 0 et 1, et dont la somme totale est 1. Ceci permet de représenter la \"probabilité\" de certitude dans la sortie du réseau. **Il est utile d'utiliser une representation entre 0 et 1 (au lieu de [-inf,+inf] pour le calcul de la fonction de perte)**\n",
        "\n",
        "**Q5** : Le dropout est l’une des nombreuses techniques de pénalité utilisée afin de réduire l’overfitting (i.e.  l’apprentissage de bruit). On applique aux neurones une probabilité qu’ils soient désactivés à chaque epoch. A chaque passe, le modèle apprendra avec une configuration de neurones aléatoire et différente. Cela viendra «pertuber» les caractéristiques apprises par le modèle. **Ce n'est pas très claire dans votre explication comment le dropout aide le réseau à ne pas overfitter.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxlyOgFU3GQs"
      },
      "source": [
        "## Encodeur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWvfXCFlvf4q"
      },
      "source": [
        "#### Exercice 3\n",
        "\n",
        "Maintenant, nous allons essayer de mieux comprendre le fonctionnement de ce modèle avec un focus sur son encodeur, qui est définit dans le fichier [bert_encoder.py](https://github.com/thunlp/OpenNRE/blob/60a8ceb42e1cfacbde3c8cfb5f758fb7fe96bdc4/opennre/encoder/bert_encoder.py#L7). Ce qui est spécifique à la tache de NRE dans ce modèle n’est pas l’architecture, mais plutôt la façon dont la séquence en input est tokenisée. Avec l’objectif de bien comprendre comment cet encodeur gère son input, répondez en détail aux questions suivantes qui se référent à la méthode `tokenize`. Si vous utilisez du code pour vous aider à répondre (conseillé), veuillez le joindre à vos réponses textuelles.\n",
        "\n",
        "1. Qu’est-ce qu’elle sont les variables `sent0`, `ent0`, `sent1`, `ent1`, `sent2` ?\n",
        "2. Qu’est-ce que c’est `re_tokens` ?\n",
        "3. Dans le _forward_, le `BERTEncoder` prend en entrée exclusivement la séquence textuelle qui a étée préalablement tokenisée. Comment est-il capable de distinguer la tête et la queue en sort de (apprendre à) prédire la relation entre les deux ?\n",
        "4. Qu’est-ce que c’est le « Padding » et pourquoi est-il utile ?\n",
        "5. Qu’est-ce que c’est l’ « Attention mask » et pourquoi est-elle utile ? [Aide ici](https://huggingface.co/docs/transformers/v4.16.2/en/glossary#attention-mask)\n",
        "6. **[BONUS]** Quelle est la différence entre la classe `BERTEncoder` et la classe `BERTEntityEncoder`, définie dans le même fichier ?\n",
        "\n",
        "\n",
        "Aide : vous pouvez accéder à la méthode `tokenize` pour la tester avec `model.sentence_encoder.tokenize(item)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0bjFnEwiOBc"
      },
      "source": [
        "**Réponse aux questions :**\n",
        " \n",
        "1. Ces variables sont des sous-ensembles de la phrase globale `sentence` qui vont être tokenizés. On va ensuite les “additioner” entre eux de sorte à ce que l’on repère *via* un token `['[unused0]']` les deux parties de la phrase à analyser.\n",
        " \n",
        "2. `re_tokens` représente la phrase entièrement tokenizée, avec les balises `[CLS]` et `[SEP]` que l'on a vu lors du dernier TP.\n",
        "\n",
        "3. La tête et la queue sont distingués *via* leur position. Ce sont les variables `pos1` et `pos2`* **Dans a question il est specifié ue BERT ne prend pas en entrée pos1 ni pos2. Ainsi, le réseau BERTEncoder apprend à distinguer la tête et la queue du reste du texte grace aux tokens \"unused\" qui ont été introduits à la tokenization**\n",
        "\n",
        "4. Habituellement, la longueur maximale d'une phrase dépend des données sur lesquelles nous travaillons. Pour les phrases qui sont plus courtes que cette longueur maximale, nous devrons ajouter des paddings (tokens vides) aux phrases pour compenser la longueur. Dans l'implémentation originale, le token `[PAD]` est utilisé pour représenter les paddings de la phrase. **Pourquoi faut-il compenser la longueur? -> pour qu'on puisse representer un lot de phrases avec une matrice, ce qui est nécessaire au réseau pour traiter les phrases en parallèle.**\n",
        "\n",
        "5. L'«Attention mask» indique au modèle les tokens qui doivent être pris en charge et ceux qui ne doivent pas l'être. **Quels tokens ne sont pas pris en charge?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "te30zT02s_Tt"
      },
      "outputs": [],
      "source": [
        "# Nous n'avons pas utilisé de code pour répondre aux questions, la lecture\n",
        "# de la documentation a été suffisante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99yJxX0-Z1h1"
      },
      "source": [
        "## NRE Pipeline\n",
        "\n",
        "Nous allons maintenant programmer une application qui prend en entrée une phrase et donne en sortie deux entités nommées dans la phrase ainsi que la relation entre eux.\n",
        "\n",
        "Pour ce faire, nous avons besoin :\n",
        "\n",
        "1. d’un système NER, qui reconnaît les entités nommées dans la phrase\n",
        "2. d’un système NRE, comme celui que nous avons utilisé jusqu’à là"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZYCITLLa0ii"
      },
      "source": [
        "#### Exercice 4\n",
        "\n",
        "En vous aident avec la documentation de HuggingFace, instanciez une [pipeline](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipeline#the-pipeline-abstraction) `ner_pipeline` pour la reconnaissance d'entités nommées, avec le modèle et le tokeniseur pré-entraînes [dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
        "\n",
        "Vous pouvez tester la bonne réussite de l’exercice avec le code ci-dessous.\n",
        "\n",
        "**Indice** : la solution consiste en deux lignes de code : l'une pour importer la classe pipeline, l'autre pour instancier la bonne pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "323UWMt1Z3y0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "model_ner = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "hgdLHOc_zWLh"
      },
      "outputs": [],
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhdvbovPBZmP",
        "outputId": "efc73cf7-7c05-4f7e-eedf-c387b901a964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Something might be wrong with your pipeline.\n",
            "Check the cell down below to compare, but this is normal.\n"
          ]
        }
      ],
      "source": [
        "# Apply your ner_pipeline to some sentences to see how it works,\n",
        "# Then you can test your code with this snippet\n",
        "text = 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).'\n",
        "try:\n",
        "  assert ner_pipeline(text) == [\n",
        "                                {'entity': 'I-PER', 'index': 6, 'score': 0.982085645198822, 'word': 'M'},\n",
        "                                {'entity': 'I-PER', 'index': 7, 'score': 0.8588601350784302, 'word': '##á'},\n",
        "                                {'entity': 'I-PER', 'index': 8, 'score': 0.8766007423400879, 'word': '##el'},\n",
        "                                {'entity': 'I-PER', 'index': 9, 'score': 0.8375428915023804, 'word': 'D'},\n",
        "                                {'entity': 'I-PER', 'index': 10, 'score': 0.4897182583808899, 'word': '##ú'},\n",
        "                                {'entity': 'I-PER', 'index': 11, 'score': 0.901253342628479, 'word': '##in'},\n",
        "                                {'entity': 'I-PER', 'index': 12, 'score': 0.5821399688720703, 'word': 'mac'},\n",
        "                                {'entity': 'I-PER', 'index': 13, 'score': 0.8884768486022949, 'word': 'M'},\n",
        "                                {'entity': 'I-PER', 'index': 14, 'score': 0.7004077434539795, 'word': '##á'},\n",
        "                                {'entity': 'I-PER', 'index': 15, 'score': 0.8791088461875916, 'word': '##ele'},\n",
        "                                {'entity': 'I-PER', 'index': 16, 'score': 0.9720047116279602, 'word': 'Fi'},\n",
        "                                {'entity': 'I-PER', 'index': 17, 'score': 0.697806715965271, 'word': '##th'},\n",
        "                                {'entity': 'I-PER', 'index': 18, 'score': 0.697088360786438, 'word': '##rich'},\n",
        "                                {'entity': 'I-PER', 'index': 26, 'score': 0.9203088283538818, 'word': 'Á'},\n",
        "                                {'entity': 'I-PER', 'index': 27, 'score': 0.9416706562042236, 'word': '##ed'},\n",
        "                                {'entity': 'I-PER', 'index': 28, 'score': 0.9702795147895813, 'word': 'U'},\n",
        "                                {'entity': 'I-PER', 'index': 29, 'score': 0.8428962230682373, 'word': '##ari'},\n",
        "                                {'entity': 'I-PER', 'index': 31, 'score': 0.7077912092208862, 'word': '##ach'}\n",
        "                              ]\n",
        "  print(\"Good job!\")\n",
        "except AssertionError:\n",
        "  print(\"Something might be wrong with your pipeline.\")\n",
        "  print(\"Check the cell down below to compare, but this is normal.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7v9c1V9D0zp",
        "outputId": "149a1571-79ac-4fbb-f539-60bfe9e335b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'index': 6, 'score': 0.9820857644081116, 'word': 'M'},\n",
              " {'entity': 'I-PER', 'index': 7, 'score': 0.8588611483573914, 'word': '##á'},\n",
              " {'entity': 'I-PER', 'index': 8, 'score': 0.8766009211540222, 'word': '##el'},\n",
              " {'entity': 'I-PER', 'index': 9, 'score': 0.8375439047813416, 'word': 'D'},\n",
              " {'entity': 'I-PER', 'index': 10, 'score': 0.489718496799469, 'word': '##ú'},\n",
              " {'entity': 'I-PER', 'index': 11, 'score': 0.9012538194656372, 'word': '##in'},\n",
              " {'entity': 'I-PER', 'index': 12, 'score': 0.5821399688720703, 'word': 'mac'},\n",
              " {'entity': 'I-PER', 'index': 13, 'score': 0.8884763717651367, 'word': 'M'},\n",
              " {'entity': 'I-PER', 'index': 14, 'score': 0.7004097104072571, 'word': '##á'},\n",
              " {'entity': 'I-PER',\n",
              "  'index': 15,\n",
              "  'score': 0.8791093826293945,\n",
              "  'word': '##ele'},\n",
              " {'entity': 'I-PER', 'index': 16, 'score': 0.9720048308372498, 'word': 'Fi'},\n",
              " {'entity': 'I-PER', 'index': 17, 'score': 0.6978076100349426, 'word': '##th'},\n",
              " {'entity': 'I-PER',\n",
              "  'index': 18,\n",
              "  'score': 0.6970879435539246,\n",
              "  'word': '##rich'},\n",
              " {'entity': 'I-PER', 'index': 26, 'score': 0.9203093647956848, 'word': 'Á'},\n",
              " {'entity': 'I-PER', 'index': 27, 'score': 0.9416710734367371, 'word': '##ed'},\n",
              " {'entity': 'I-PER', 'index': 28, 'score': 0.9702795147895813, 'word': 'U'},\n",
              " {'entity': 'I-PER',\n",
              "  'index': 29,\n",
              "  'score': 0.8428966403007507,\n",
              "  'word': '##ari'},\n",
              " {'entity': 'I-PER', 'index': 31, 'score': 0.707791805267334, 'word': '##ach'}]"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner_pipeline(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOzSPqsJKkve"
      },
      "source": [
        "#### Exercice 5\n",
        "\n",
        "\n",
        "Nous pouvons finalement développer une pipeline NRE reposante sur notre modèle de NRE et notre pipeline NER. \n",
        "\n",
        "Écrivez ci-dessous une classe `NREPipeline` équipée (entre autres) d'une méthode `__call__(self, text)` qui prend un texte en entrée et effectue les opérations suivantes :\n",
        "\n",
        "- elle reconnaît les entités dans le texte\n",
        "    - retenir seulement les deux entités auxquelles la pipeline NER associe la probabilité la plus élevée, écarter les autres (si presentes)\n",
        "    - si la pipeline NER ne reconnaît qu’une seule entités dans le texte, `__call__(self, text)` retourne None (voir le test en bas) car il n’y a aucune relation à prédire\n",
        "- elle donne en sortie une liste en format `[e1, e2, rel, p]` où :\n",
        "    - `e1` est la première entité reconnue dans le texte (entre les deux plus probables, la première qui apparaître dans le texte)\n",
        "    - `e2` est la deuxième entité  reconnue dans le texte (entre les deux plus probables, la deuxième qui apparaître dans le texte)\n",
        "    - `rel` est la relation qu’il y a entre `e1` et `e2`\n",
        "    - `p` est la probabilité associée à la relation `rel` par le modèle de NRE\n",
        "\n",
        "Pour vérifier la bonne qualité de votre classe `NREPipeline`, utilisez l’extrait de code ci-dessous. Le résultat devrait ressabler à celui-ci :\n",
        "\n",
        "````\n",
        "Sentence 0: He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).\n",
        "System out:  ['Máel Dúin mac Máele Fithrich', 'Áed Uari', 'father', 0.9923498034477234]\n",
        "------------------------------\n",
        "Sentence 1: He was the son of Máel Dúin\n",
        "System out:  None\n",
        "------------------------------\n",
        "Sentence 2: Ōda is home to the Ōda Iwami Ginzan Silver Mine , a World Heritage Site .\n",
        "System out:  ['I', 'World Heritage Site', 'heritage designation', 0.9991846680641174]\n",
        "------------------------------\n",
        "Sentence 3: It has been shown to be equally effective as leuprorelin , which is a second - line medication against endometriosis .\n",
        "System out:  None\n",
        "------------------------------\n",
        "Sentence 4: Located at Earleville and listed on the National Register of Historic Places are : Bohemia Farm , Mount Harmon , Rose Hill , and St. Stephen 's Episcopal Church .\n",
        "System out:  ['Earleville', \"St . Stephen ' s Episcopal Church\", 'location', 0.9127373099327087]\n",
        "------------------------------\n",
        "````"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "ToAxtF4nZ_Bq"
      },
      "outputs": [],
      "source": [
        "# Write your pipeline in this cell. The fun begins here, because we are coding a whole Python class from scratch !\n",
        "\n",
        "class NREPipeline(object):\n",
        "  def __init__(self, ner_pipeline, nre_model):\n",
        "    self.ner_pipeline = ner_pipeline\n",
        "    self.ner_pipeline.grouped_entities=True\n",
        "    self.nre_model = nre_model\n",
        "\n",
        "  def __call__(self, text):\n",
        "   allScores = ner_pipeline(text)\n",
        "   if len(allScores) <= 1:\n",
        "     return None\n",
        "   # on supprime tous les éléments qui contiennent exactement le token [UNK]\n",
        "   i = 0\n",
        "   while i < len(allScores):\n",
        "     if allScores[i]['word'] == \"[UNK]\":\n",
        "       allScores.pop(i)\n",
        "       i -= 1\n",
        "     i+=1\n",
        "   sortedAllScores = sorted(allScores, key = lambda d: d['score'])\n",
        "   # les deux derniers éléments de sortedAllScores nous intéressent\n",
        "   e1 = sortedAllScores[-1]['word']\n",
        "   e2 = sortedAllScores[-2]['word']\n",
        "   prob = model.infer(to_input_format(text, e1, e2))\n",
        "   return [e1, e2, prob[0], prob[1]]\n",
        "  #  return sortedAllScores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaWrZEJc4IDK",
        "outputId": "f2a2b9a1-fc90-43ae-b386-249e54ab828d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 0: He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).\n",
            "System out:  ['Áed Uari', 'Máel Dúin mac Máele Fithrich', 'father', 0.971123456954956]\n",
            "------------------------------\n",
            "Sentence 1: He was the son of Máel Dúin\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 2: Ōda is home to the Ōda Iwami Ginzan Silver Mine , a World Heritage Site .\n",
            "System out:  ['World Heritage Site', '##zan', 'heritage designation', 0.9965190887451172]\n",
            "------------------------------\n",
            "Sentence 3: It has been shown to be equally effective as leuprorelin , which is a second - line medication against endometriosis .\n",
            "System out:  None\n",
            "------------------------------\n",
            "Sentence 4: Located at Earleville and listed on the National Register of Historic Places are : Bohemia Farm , Mount Harmon , Rose Hill , and St. Stephen 's Episcopal Church .\n",
            "System out:  ['Earleville', \"St . Stephen ' s Episcopal Church\", 'located in the administrative territorial entity', 0.5751827359199524]\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test your code with this snippet\n",
        "\n",
        "sequences = [\n",
        "             'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', # Easy sentence\n",
        "             'He was the son of Máel Dúin', # There is only one entity in this sentence, therefore our pipeline should return None\n",
        "             'Ōda is home to the Ōda Iwami Ginzan Silver Mine , a World Heritage Site .', # Ōda is tokenized by the NER tokenizer as a \"[UNK]\" and it is detected as an entitity. For simplicity, you can discard [UNK] entities as if they were not detected. Here another difficulty arises from the fact that the different tokens in which Iwami Ginzan is decomposed are (wrongly) classified as entities belonging to different classes. In this case, we can consider only the first token \"I\" as standalone entity, for simplicity.  \n",
        "             'It has been shown to be equally effective as leuprorelin , which is a second - line medication against endometriosis .', # the NER system can not recognise any entity here, therefore the pipeline should return None\n",
        "             \"Located at Earleville and listed on the National Register of Historic Places are : Bohemia Farm , Mount Harmon , Rose Hill , and St. Stephen 's Episcopal Church .\" # This sentence is not trivial because the 's has to be managed properly\n",
        "\n",
        "]\n",
        "\n",
        "nre = NREPipeline(ner_pipeline, model)\n",
        "# print(nre(sequences[2]))\n",
        "for n, sequence in enumerate(sequences):\n",
        "  out = nre(sequence)\n",
        "  print(\"Sentence {0}: {1}\".format(n, sequence))\n",
        "  print(\"System out: \", out)\n",
        "  print('------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP43i6ud8zx-"
      },
      "source": [
        "Par rapport à la sortie que vous avez proposé, nous avons des résultats sensiblement différents car le modèle utilisé n'est pas le même. De plus, nous avons traité les `[UNK]` qui sont des tokens apparaissant quand le mot n'est pas connu du Tokenizer. Pour ce faire, nous supprimons tous les scores qui sont attribués à un texte qui vaut exactement `[UNK]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl-hqEfaLvI-"
      },
      "source": [
        "## Application de le pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKfvPq_KLmD-"
      },
      "source": [
        "#### Exercice 6\n",
        "\n",
        "1. Appliquez la pipeline NRE aux phrases contenues dans le fichier _sentences.txt_, qui ont été extraites à partir de Wikipedia et Wikidata. Donnez un avis qualitatif sur sa performance, les problèmes rencontrés ainsi que des idées pour améliorations des résultats (e.g. entraînement sur des données différentes, modèle différent, etc.).\n",
        "\n",
        "2. Appliquez la pipeline NRE aux premières 50 phrases contenues dans le fichier _Sentences_AllAgree.txt_ que vous avez utilisé pour le TP BERT, qui parlent d’événements financiers. Remarquez-vous des différences en terme de performances par rapport à la question 2 ? Pourquoi ? Commentez…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP2XmVzK_HMD"
      },
      "source": [
        "**Question 1 :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T2moXc3IA3f",
        "outputId": "28d99651-5d53-4c03-edb3-a3ee05a18e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------\n",
            "The Willard and Josephine Hubbard House was individually listed on the National Register of Historic Places in 2016 .\n",
            "\n",
            "['Josephine Hubbard', 'Willard', 'spouse', 0.8584034442901611]\n",
            "---------------------\n",
            "His station commander , Group Captain Claude Hilton Keith , found a letter among the missing airman 's personal possessions .\n",
            "\n",
            "None\n",
            "---------------------\n",
            "One was Quintus Caecilius Metellus Creticus , who was praetor in 74 BC and consul in 69 BC .\n",
            "\n",
            "None\n",
            "---------------------\n",
            "April 2009 In addition to musical acts , the label recorded beat poets Lawrence Ferlinghetti and Allen Ginsberg and comic Lenny Bruce .\n",
            "\n",
            "['Lenny Bruce', 'Allen Ginsberg', 'sibling', 0.6458688378334045]\n",
            "---------------------\n",
            "Walter Neusel ( November 25 , 1907 – October 3 , 1964 ) was a German heavyweight boxer .\n",
            "\n",
            "['Walter Neusel', 'German', 'country of citizenship', 0.9982675313949585]\n",
            "---------------------\n",
            "Hércules debut defending the national team was a 1938 FIFA World Cup game , played on June 5 , 1938 , against Poland .\n",
            "\n",
            "['Poland', 'FIFA World Cup', 'participating team', 0.9908945560455322]\n",
            "---------------------\n",
            "Along the I-5 stretch , Walter Wirth Lake and McNary Field ( Salem Municipal Airport ) are on the left ; near Route 22 , the unincorporated neighborhood of Four Corners is on the right .\n",
            "\n",
            "['Salem Municipal Airport', 'Four Corners', 'place served by transport hub', 0.9981524348258972]\n",
            "---------------------\n",
            "Michael Bradley and Landon Donovan scored the United States ' two goals in the match .\n",
            "\n",
            "['Michael Bradley', 'Landon Donovan', 'spouse', 0.9853053689002991]\n",
            "---------------------\n",
            "The film was remade in Hindi as \" Love Ke Liye Kuch Bhi Karega \" and is loosely based on the American film \" Ruthless People \" .\n",
            "\n",
            "['American', 'Hindi', 'original language of film or TV show', 0.8740105032920837]\n",
            "---------------------\n",
            "Incorporated in 1988 , Vertex Venture Holdings is based in Singapore , with offices Southeast Asia , Silicon Valley , China , India , Israel , and Taiwan .\n",
            "\n",
            "['Israel', 'Taiwan', 'has part', 0.9584373831748962]\n",
            "---------------------\n",
            "Anna Caterina Antonacci , ( born 5 April 1961 ) is an Italian soprano known for roles in the bel canto and Baroque repertories .\n",
            "\n",
            "['Italian', 'Anna Caterina Antonacci', 'voice type', 0.9965165853500366]\n",
            "---------------------\n",
            "On 2 April 2017 the former municipalities of Cresciano , Iragna and Osogna merged into the new municipality of Riviera .\n",
            "\n",
            "['Osogna', 'Cresciano', 'located in the administrative territorial entity', 0.6573230624198914]\n",
            "---------------------\n",
            "Dimitar Ivanov Makriev (; born 7 January 1984 ) is a Bulgarian footballer who currently plays as a forward for Cypriot First Division side Nea Salamina Famagusta .\n",
            "\n",
            "['Bulgarian', 'Dimitar Ivanov Makriev', 'country of citizenship', 0.9966973066329956]\n",
            "---------------------\n",
            "The 23d Fighter Group was assigned to the 347th Wing of Air Combat Command at Moody Air Force Base , Georgia but the group remained at Pope as a Geographically Separated Unit ( GSU ) .\n",
            "\n",
            "['Georgia', 'Air Combat Command', 'military branch', 0.9295220375061035]\n",
            "---------------------\n",
            "\" А \" група , 2012/13 , 16 кръг , 01.03.2013 15:30 Stachowiak participated in all games of the club in Europa League during seasons 2013 - 14 and 2014 - 15 .\n",
            "\n",
            "['Europa League', 'Stachowiak', 'league', 0.8327862620353699]\n",
            "---------------------\n",
            "Dartnell offered his services to the British Army on the outbreak of the First World War , and was commissioned into the 25th ( Frontiersmen ) Battalion , Royal Fusiliers in February 1915 .\n",
            "\n",
            "['First World War', 'Royal Fusiliers', 'participant', 0.9957960844039917]\n",
            "---------------------\n",
            "In September 1260 , the two sides met in the plains south of Nazareth in a major confrontation known as the Battle of Ain Jalut . Cummins 2011 , p. 80 .\n",
            "\n",
            "['Nazareth', 'Ain Jalut', 'location', 0.9964893460273743]\n",
            "---------------------\n",
            "Sir Hugh de Morville(d.1202 ) fled to the house after taking part as 1 of 4 knights in the murder on 29 December 1170 of Thomas Becket , Archbishop of Canterbury .\n",
            "\n",
            "['Thomas Becket', 'Hugh de Morville', 'participant', 0.3305263817310333]\n",
            "---------------------\n",
            "The claim was made in 1264 as senior descendant and rightful heir of Alice of Champagne , second daughter of Queen Isabella I , Hugh being the son of their eldest daughter .\n",
            "\n",
            "['Hugh', 'Alice', 'child', 0.9464405179023743]\n",
            "---------------------\n",
            "The Elected Member is a Booker Prize - winning novel by Welsh writer Bernice Rubens .\n",
            "\n",
            "['Bernice Rubens', 'Booker Prize', 'winner', 0.9968593120574951]\n",
            "---------------------\n",
            "He currently serves as an Offensive Skills coach at the Brisbane Lions , and also as an assistant coach of the Lions ' NEAFL reserves team .\n",
            "\n",
            "['Brisbane Lions', 'Lions', 'subsidiary', 0.6964221596717834]\n",
            "---------------------\n",
            "In 2008 he produced , alongside Christian Colson , the critically acclaimed feature film \" Eden Lake \" ( 2008 ) , written and directed by James Watkins .\n",
            "\n",
            "['James Watkins', 'Christian Colson', 'screenwriter', 0.7484352588653564]\n",
            "---------------------\n",
            "The West Middlesex Waterworks Company was founded in 1806 to supply water to the Marylebone and Paddington areas of London .\n",
            "\n",
            "['West Middlesex Waterworks Company', 'London', 'headquarters location', 0.9655435085296631]\n",
            "---------------------\n",
            "Neutraface is a geometric sans - serif typeface designed by Christian Schwartz for House Industries , an American digital type foundry .\n",
            "\n",
            "['Christian Schwartz', 'House Industries', 'owned by', 0.885522723197937]\n",
            "---------------------\n",
            "The APG II states that Alangiaceae is a synonym of Cornaceae ( the Dogwood family ) , but still recognizes it as a \" nom .\n",
            "\n",
            "['APG II', 'Cornaceae', 'taxon rank', 0.8809809684753418]\n",
            "---------------------\n",
            "But because CKGM Montreal is the dominant class A station on 690 , CBU must use a directional signal to avoid causing interference .\n",
            "\n",
            "['CBU', 'CKGM Montreal', 'licensed to broadcast to', 0.9963551759719849]\n",
            "---------------------\n",
            "It takes place when a quark of one hadron and an antiquark of another hadron annihilate , creating a virtual photon or Z boson which then decays into a pair of oppositely - charged leptons .\n",
            "\n",
            "None\n",
            "---------------------\n",
            "The first \" Uncle Slam \" album was \" Say Uncle \" , released in 1988 through Caroline Records .\n",
            "\n",
            "['Caroline Records', 'Say Uncle', 'record label', 0.9988273978233337]\n",
            "---------------------\n",
            "The KC-135R - CRAG Stratotanker aerial refueling tanker aircraft of the Turkish Air Force are operated by the 101st Squadron , stationed at the Incirlik Air Base .\n",
            "\n",
            "['Incirlik Air Base', 'Turkish Air Force', 'operator', 0.9957613348960876]\n",
            "---------------------\n",
            "The Church of The Epiphany was designed by noted Philadelphia architect Thomas Ustick Walter in the Greek Revival style .\n",
            "\n",
            "['Thomas Ustick Walter', 'Greek Revival', 'movement', 0.997984766960144]\n",
            "---------------------\n",
            "Meaker Food Company Warehouse , Harriet May Mills House , St. Paul 's Armenian Apostolic Church , Alton Simmons House , and West Brothers Knitting Company are listed on the National Register of Historic Places .\n",
            "\n",
            "['Meaker Food Company', 'West Brothers Knitting Company', 'occupant', 0.6864261627197266]\n",
            "---------------------\n",
            "This appointment was harshly criticized by Fiji Labour Party leader Mahendra Chaudhry .\n",
            "\n",
            "['Fiji Labour Party', 'Mahendra Chaudhry', 'member of political party', 0.9914173483848572]\n",
            "---------------------\n",
            "In the summer of 2007 , Varea joined Deportivo Español in the third division , where he played 11 matches and scored two goals until December 2007 .\n",
            "\n",
            "['Varea', 'Deportivo Español', 'position played on team / speciality', 0.8130487203598022]\n",
            "---------------------\n",
            "\" Aparajito \" was preceded by \" Pather Panchali \" ( 1955 ) and followed by \" Apur Sansar \" ( \" The World of Apu \" ) in 1959 .\n",
            "\n",
            "['The World of Apu', 'Apur Sansar', 'said to be the same as', 0.5881115198135376]\n",
            "---------------------\n",
            "Neće rijeka zrakom teći is the second studio album by Croatian rock band Silente .\n",
            "\n",
            "['Croatian', 'Silente', 'country of origin', 0.9018869996070862]\n",
            "---------------------\n",
            "This project was heavily opposed by the Apulian governor Michele Emiliano . Gasdotto Tap , il governo tira dritto dopo il no del Mibac .\n",
            "\n",
            "['Michele Emiliano', 'Apulian', 'head of government', 0.7664783596992493]\n",
            "---------------------\n",
            "Among Microsoft software development tools , DTE80 is the Microsoft Development Environment 8.0 Extensibility namespace .\n",
            "\n",
            "['Microsoft', 'Microsoft Development Environment 8 . 0 Ex', 'developer', 0.8645378947257996]\n",
            "---------------------\n",
            "Sprinkling Tarn is a body of water at the foot of Great End , in the Southern Fells in Lake District , 3   km from Seathwaite , Cumbria , England .\n",
            "\n",
            "['Cumbria', 'Lake District', 'contains administrative territorial entity', 0.9956204295158386]\n",
            "---------------------\n",
            "Michel made her WTA tour debut at the 2013 Internationaux de Strasbourg , partnering Claire Feuerstein in doubles .\n",
            "\n",
            "['Michel', 'Claire Feuerstein', 'spouse', 0.8597851991653442]\n",
            "---------------------\n",
            "He took part in the Council of Agde in 506 , where 34 Catholic bishops of the Visigothic kingdom met under the chairmanship of Saint Caesarius of Arles .\n",
            "\n",
            "['Caesarius', 'Catholic', 'religion', 0.9990239143371582]\n",
            "---------------------\n",
            "A license obtained from the British Motor Corporation led to the Siam di Tella 1500 ; based on the British BMC Farina series of the late 1950s .\n",
            "\n",
            "['British Motor Corporation', 'British', 'followed by', 0.6191161870956421]\n",
            "---------------------\n",
            "Euphemia was the youngest child of Władysław Odonic and his wife Jadwiga , disputed daughter of Mestwin I , Duke of Pomerania and Swinisław , daughter of Mieszko III the Old .\n",
            "\n",
            "['Władysław Odonic', 'Swinisław', 'child', 0.3340129256248474]\n",
            "---------------------\n",
            "Hot 2Nite was produced by Ryan Leslie initially for New Edition 's album , One Love , released in 2004 under the label Bad Boy Records .\n",
            "\n",
            "['Ryan Leslie', 'Bad Boy Records', 'record label', 0.9988422989845276]\n",
            "---------------------\n",
            "SN 1996ah was a supernova located in the spiral galaxy NGC 5640 in the constellation of Camelopardalis .\n",
            "\n",
            "['S', 'Camelopardalis', 'constellation', 0.9993067979812622]\n",
            "---------------------\n",
            "She made her debut at The Proms in 2000 , singing Mozart 's Mass in C minor and Alban Berg 's \" Altenberg Lieder \" with the Bochumer Symphoniker , conducted by Simon Rattle .\n",
            "\n",
            "['Alban Berg', 'Simon Rattle', 'composer', 0.992742657661438]\n",
            "---------------------\n",
            "The Albanian Wikipedia ( ) is the Albanian language edition of Wikipedia started on October 12 , 2003 .\n",
            "\n",
            "['Albanian', 'Albanian Wikipedia', 'language of work or name', 0.9711563587188721]\n",
            "---------------------\n",
            "For her performance in this episode , Elaine Stritch received a Primetime Emmy Award nomination in the category for Outstanding Guest Actress in a Comedy Series .\n",
            "\n",
            "['Primetime Emmy Award', 'Elaine Stritch', 'winner', 0.9866238832473755]\n",
            "---------------------\n",
            "He later worked as a journalist for the French language paper \" Le Matin \" .\n",
            "\n",
            "['Le Matin', 'French', 'language of work or name', 0.9968658089637756]\n",
            "---------------------\n",
            "Loughton to Epping became part of the London Underground Central line on 25 September 1949 , leaving the single track line from Epping to Ongar as the last steam - worked section .\n",
            "\n",
            "['Ongar', 'Epping', 'has part', 0.4434185326099396]\n",
            "---------------------\n",
            "Though Vivekananda gave up the idea of becoming Pavhari Baba 's disciple after seeing Ramakrishna 's melancholy face in his dream the night before his religious initiation , Baba deeply influenced him .\n",
            "\n",
            "['Baba', 'Ramakrishna', 'notable work', 0.476604700088501]\n"
          ]
        }
      ],
      "source": [
        "with open('../sentences.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    for sentence in lines:\n",
        "      print(\"---------------------\")\n",
        "      print(sentence)\n",
        "      print(nre(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAww-K3c_65W"
      },
      "source": [
        "On remarque que la performance est élevée lorsqu'il y a peu d'acteurs dans la phrase. Par exemple après traitement de la phrase : `The film was remade in Hindi as \" Love Ke Liye Kuch Bhi Karega \" and is loosely based on the American film \" Ruthless People \"`, nous avons en sortie : `['American', 'Hindi', 'original language of film or TV show', 0.8740105032920837]` bien que la phrase soit complexe.\n",
        "\n",
        "Cependant, pour certaines phrases spécifiques avec beaucoup d'acteurs, le résultat est mitigé. Par exemple : `Euphemia was the youngest child of Władysław Odonic and his wife Jadwiga , disputed daughter of Mestwin I , Duke of Pomerania and Swinisław , daughter of Mieszko III the Old .`. Ici, une relation de parenté entre Władysław Odonic et Swinisław est détectée avec une probabilité de 33%. Pour un cerveau humain, la phrase est intelligible, c'est bien Jadwiga qui est la fille de Swinisław.\n",
        "\n",
        "Pour améliorer les résultats, on pourrait utiliser un dataset plus spécifiques à ce vocabulaire relativement complexe, on aurait surement de meilleures performances. Cela rejoint les problèmes qui se posent avec un vocabulaire financier assez développé, c.f. question ci-dessous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecFm6SlC_9DB"
      },
      "source": [
        "**Question 2 :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgfzx2O_ms_",
        "outputId": "e36e4cf0-a6f8-4c22-dcce-ec65ec8ee876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------\n",
            "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
            "['Russia', 'Gran', 'owned by', 0.9612677693367004]\n",
            "---------------------\n",
            "For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\n",
            "['Componenta', 'EU', 'applies to jurisdiction', 0.8146843910217285]\n",
            "---------------------\n",
            "In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n",
            "['EU', 'EU', 'member of', 0.6022771000862122]\n",
            "---------------------\n",
            "Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\n",
            "['EU', 'EU', 'member of', 0.4233728349208832]\n",
            "---------------------\n",
            "Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\n",
            "['EU', 'EU', 'member of', 0.5206767916679382]\n",
            "---------------------\n",
            "Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .\n",
            "['EU', 'EU', 'applies to jurisdiction', 0.5307677388191223]\n",
            "---------------------\n",
            "Clothing retail chain Sepp+l+ 's sales increased by 8 % to EUR 155.2 mn , and operating profit rose to EUR 31.1 mn from EUR 17.1 mn in 2004 .\n",
            "['EU', 'EU', 'applies to jurisdiction', 0.9802150726318359]\n",
            "---------------------\n",
            "Consolidated net sales increased 16 % to reach EUR74 .8 m , while operating profit amounted to EUR0 .9 m compared to a loss of EUR0 .7 m in the prior year period .\n",
            "['EU', 'EU', 'has part', 0.4362262785434723]\n",
            "---------------------\n",
            "Foundries division reports its sales increased by 9.7 % to EUR 63.1 mn from EUR 57.5 mn in the corresponding period in 2006 , and sales of the Machine Shop division increased by 16.4 % to EUR 41.2 mn from EUR 35.4 mn in the corresponding period in 2006 .\n",
            "['EU', 'EU', 'applies to jurisdiction', 0.8449512124061584]\n",
            "---------------------\n",
            "HELSINKI ( AFX ) - Shares closed higher , led by Nokia after it announced plans to team up with Sanyo to manufacture 3G handsets , and by Nokian Tyres after its fourth-quarter earnings report beat analysts ' expectations , dealers said .\n",
            "['Nokia', 'Sanyo', 'subsidiary', 0.8237332701683044]\n",
            "---------------------\n",
            "Its board of directors will propose a dividend of EUR0 .12 per share for 2010 , up from the EUR0 .08 per share paid in 2009 .\n",
            "['EU', 'EU', 'has part', 0.9040789008140564]\n",
            "---------------------\n",
            "MegaFon 's subscriber base increased 16.1 % in 2009 to 50.5 million users as of December 31 , while its market share by the number of customers amounted to 24 % as of late 2009 , up from 23 % as of late 2008 , according to TeliaSonera estimates .\n",
            "['MegaFon', 'TeliaSonera', 'owned by', 0.9979793429374695]\n",
            "---------------------\n",
            "Net income from life insurance doubled to EUR 6.8 mn from EUR 3.2 mn , and net income from non-life insurance rose to EUR 5.2 mn from EUR 1.5 mn in the corresponding period in 2009 .\n",
            "['EU', 'EU', 'member of', 0.3776685893535614]\n",
            "---------------------\n",
            "Net sales increased to EUR193 .3 m from EUR179 .9 m and pretax profit rose by 34.2 % to EUR43 .1 m. ( EUR1 = USD1 .4 )\n",
            "['EU', 'EU', 'has part', 0.5479825735092163]\n",
            "---------------------\n",
            "Net sales surged by 18.5 % to EUR167 .8 m. Teleste said that EUR20 .4 m , or 12.2 % , of the sales came from the acquisitions made in 2009 .\n",
            "['Teleste', 'EU', 'owned by', 0.8905014991760254]\n",
            "---------------------\n",
            "Nordea Group 's operating profit increased in 2010 by 18 percent year-on-year to 3.64 billion euros and total revenue by 3 percent to 9.33 billion euros .\n",
            "None\n",
            "---------------------\n",
            "Operating profit for the nine-month period increased from EUR13 .6 m , while net sales increased from EUR394 .7 m , as compared to the corresponding period in 2005 .\n",
            "['EU', 'EU', 'has part', 0.6739810705184937]\n",
            "---------------------\n",
            "Operating profit for the three-month period increased from EUR1 .2 m , while revenue increased from EUR20 .2 m , as compared to the corresponding period in 2005 .\n",
            "['EU', 'EU', 'member of', 0.4793180823326111]\n",
            "---------------------\n",
            "The company 's net profit rose 11.4 % on the year to 82.2 million euros in 2005 on sales of 686.5 million euros , 13.8 % up on the year , the company said earlier .\n",
            "None\n",
            "---------------------\n",
            "The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the results from its members .\n",
            "['Lithuanian', \"Lithuanian Brewers ' Association\", 'applies to jurisdiction', 0.385041207075119]\n",
            "---------------------\n",
            "Viking Line 's cargo revenue increased by 5.4 % to EUR 21.46 mn , and cargo volume increased by 2.4 % to 70,116 cargo units .\n",
            "['Viking Line', 'EU', 'member of', 0.4520467519760132]\n",
            "---------------------\n",
            "The fair value of the property portfolio doubled as a result of the Kapiteeli acquisition and totalled EUR 2,686.2 1,259.7 million .\n",
            "['EU', 'Kapiteeli', 'subsidiary', 0.9443761110305786]\n",
            "---------------------\n",
            "10 February 2011 - Finnish media company Sanoma Oyj HEL : SAA1V said yesterday its 2010 net profit almost tripled to EUR297 .3 m from EUR107 .1 m for 2009 and announced a proposal for a raised payout .\n",
            "['Finnish', 'EU', 'country', 0.877186119556427]\n",
            "---------------------\n",
            "A Helsinki : ELIiV today reported EPS of EUR1 .13 for 2009 , an increase over EPS of EUR1 .12 in 2008 .\n",
            "['ELIiV', 'EU', 'member of', 0.40831807255744934]\n",
            "---------------------\n",
            "Commission income increased by 22 % to EUR 4.4 mn , and lending volume rose by 13.5 % .\n",
            "None\n",
            "---------------------\n",
            "In January , traffic , measured in revenue passenger kilometres RPK , went up by 3.2 % and capacity , measured in available seat kilometres ASK , rose by 12.2 % .\n",
            "None\n",
            "---------------------\n",
            "In January-September 2010 , Fiskars ' net profit went up by 14 % year-on-year to EUR 65.4 million and net sales to EUR 525.3 million from EUR 487.7 million .\n",
            "['EU', 'EU', 'member of', 0.6588223576545715]\n",
            "---------------------\n",
            "Net income from life insurance rose to EUR 16.5 mn from EUR 14.0 mn , and net income from non-life insurance to EUR 22.6 mn from EUR 15.2 mn in 2009 .\n",
            "['EU', 'EU', 'applies to jurisdiction', 0.4399946630001068]\n",
            "---------------------\n",
            "Sales have risen in other export markets .\n",
            "None\n",
            "---------------------\n",
            "Sales increased due to growing market rates and increased operations .\n",
            "None\n",
            "---------------------\n",
            "The agreement strengthens our long-term partnership with Nokia Siemens Networks .\n",
            "None\n",
            "---------------------\n",
            "The company 's order book stood at 1.5 bln euro $ 2.2 bln on September 30 , 2007 , up by 24.2 pct on the year , with international orders amounting to 365 mln euro $ 534.3 mln .\n",
            "None\n",
            "---------------------\n",
            "The company said that paper demand increased in all of its main markets , including of publication papers , and that it increased average paper prices by 4 percent compared with last year .\n",
            "None\n",
            "---------------------\n",
            "The world 's second largest stainless steel maker said net profit in the three-month period until Dec. 31 surged to euro603 million US$ 781 million , or euro3 .33 US$ 4.31 per share , from euro172 million , or euro0 .94 per share , the previous year .\n",
            "['US', 'US', 'instance of', 0.5066026449203491]\n",
            "---------------------\n",
            "Shares of Standard Chartered ( STAN ) rose 1.2 % in the FTSE 100 , while Royal Bank of Scotland ( RBS ) shares rose 2 % and Barclays shares ( BARC ) ( BCS ) were up 1.7 % .\n",
            "['Standard Chartered', 'Royal Bank of Scotland', 'subsidiary', 0.6374610662460327]\n",
            "---------------------\n",
            "Shares of Nokia Corp. rose Thursday after the cell phone maker said its third-quarter earnings almost doubled and its share of the global handset market increased .\n",
            "None\n",
            "---------------------\n",
            "In its financial report , published on Friday , SEB said its net profit soared to SEK6 .745 bn in 2010 from a year-earlier SEK1 .114 bn and proposed a 50 % dividend increase to SEK1 .50 per share .\n",
            "['SEB', 'SE', 'subsidiary', 0.996081531047821]\n",
            "---------------------\n",
            "At the request of Finnish media company Alma Media 's newspapers , research manager Jari Kaivo-oja at the Finland Futures Research Centre at the Turku School of Economics has drawn up a future scenario for Finland 's national economy by using a model developed by the University of Denver .\n",
            "['Finland', 'Finnish', 'country', 0.9597074389457703]\n",
            "---------------------\n",
            "STOCK EXCHANGE ANNOUNCEMENT 20 July 2006 1 ( 1 ) BASWARE SHARE SUBSCRIPTIONS WITH WARRANTS AND INCREASE IN SHARE CAPITAL A total of 119 850 shares have been subscribed with BasWare Warrant Program .\n",
            "['BasWare', 'Program', 'instance of', 0.6167235970497131]\n",
            "---------------------\n",
            "A maximum of 666,104 new shares can further be subscribed for by exercising B options under the 2004 stock option plan .\n",
            "None\n",
            "---------------------\n",
            "Tiimari operates 194 stores in six countries -- including its core Finnish market -- and generated a turnover of 76.5 mln eur in 2005 .\n",
            "['Finnish', 'Tiimari', 'owned by', 0.810303270816803]\n",
            "---------------------\n",
            "The acquisition will considerably increase Kemira 's sales and market position in the Russian metal industry coatings market .\n",
            "['Russian', 'Kemira', 'owned by', 0.5399729609489441]\n",
            "---------------------\n",
            "In January-September 2007 , Finnlines ' net sales rose to EUR 505.4 mn from EUR 473.5 mn in the corresponding period in 2006 .\n",
            "['EU', 'EU', 'applies to jurisdiction', 0.8735178112983704]\n",
            "---------------------\n",
            "Adjusted for changes in the Group structure , the Division 's net sales increased by 1.7 % .\n",
            "None\n",
            "---------------------\n",
            "4 February 2011 - Finnish broadband data communication systems provider Teleste Oyj HEL : TLT1V saw its net profit jump to EUR2 .1 m for the last quarter of 2010 from EUR995 ,000 for the same period of 2009 .\n",
            "['Finnish', 'EU', 'has part', 0.3692098557949066]\n",
            "---------------------\n",
            "Finnish Aktia Group 's operating profit rose to EUR 17.5 mn in the first quarter of 2010 from EUR 8.2 mn in the first quarter of 2009 .\n",
            "['Finnish Aktia Group', 'EU', 'owned by', 0.41779401898384094]\n",
            "---------------------\n",
            "Finnish Bank of +land 's consolidated net operating profit increased from EUR 4.8 mn in the first quarter of 2005 to EUR 6.4 mn in the first quarter of 2006 .\n",
            "['EU', 'EU', 'member of', 0.574653148651123]\n",
            "---------------------\n",
            "Finnish financial group Aktia reports operating profit of EUR 44.4 mn in January-September 2009 , up from EUR 37.3 mn in the corresponding period in 2008 .\n",
            "['Finnish', 'Aktia', 'owned by', 0.5903118252754211]\n",
            "---------------------\n",
            "Finnish high technology provider Vaahto Group reports net sales of EUR 41.8 mn in the accounting period September 2007 - February 2008 , an increase of 11.2 % from a year earlier .\n",
            "['Finnish', 'Vaahto Group', 'owned by', 0.8216254711151123]\n",
            "---------------------\n",
            "Net sales of Finnish food industry company L+nnen Tehtaat 's continuing operations increased by 13 % in 2008 to EUR 349.1 mn from EUR 309.6 mn in 2007 .\n",
            "['Finnish', 'EU', 'has part', 0.6304869055747986]\n"
          ]
        }
      ],
      "source": [
        "# Dans un premier temps, on met les 100 premières phrases du fichier dans un tableau.\n",
        "# On en profite pour enlever les annotations.\n",
        "sentences = []\n",
        "with open('../Sentences_AllAgree.txt', encoding=\"utf8\", errors='ignore') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(50):\n",
        "      sentence, emotion = lines[i].split('@', 1)\n",
        "      sentences.append(sentence)\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(\"---------------------\")\n",
        "  print(sentence)\n",
        "  print(nre(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj88HjtcCX5_"
      },
      "source": [
        "On remarque des différences en terme de performance puisqu'il y a des termes financiers que le tokenizer ne semble pas bien interpréter. Par exemple, pour des devises avec `EUR`, celles-ci sont interprétées comme `EU` soit l'Union Européenne et pose de gros problèmes de compréhension. Les performances sont donc beaucoup moins bonnes, on peut supposer que le tokenizer n'a pas été entrainé sur des phrases si spécifiques.\n",
        "\n",
        "**Outre le tokenizer, il est également possible de affiner (finetune) les modèles NER et NRE sur un ensemble de données financières. Plus précisément, le modèle de reconnaissance d'entités devrait voir plus d'entités financières (par exemple, des noms de sociétés ou d'institutions financières) dans son apprentissage, et le modèle d'extraction de relations devrait être entraîné avec des relations plus spécifiques, qui ne sont pas présentes dans l'ensemble des 80 relations définies par Wiki80.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tp_re_ardiet_calvez.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
