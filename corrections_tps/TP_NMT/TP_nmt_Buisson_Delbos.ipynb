{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4bT-aQRmhFG"
      },
      "source": [
        "Vous trouverez dans notre archive seulement nos fichiers de configuration car notre archive globale était trop volumineuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA_bHW5Ehy_S",
        "outputId": "9e58c058-67b6-4996-c73e-ffbcf35ae69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.4)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.10.0+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (3.13)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.8.0)\n",
            "Collecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.30.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 79 kB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.43.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.2.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py) (2.0.1)\n",
            "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 pyonmttok-1.30.1 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install OpenNMT-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVi2pF-PucpY"
      },
      "source": [
        "## Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVk6PjnfjO5N"
      },
      "source": [
        "**Q1** : Que contiennent les fichiers _IWSLT10_BTEC.train.en.txt_ et _IWSLT10_BTEC.train.fr.txt_ ?\n",
        "\n",
        "Dans le fichier _IWSLT10_BTEC.train.en.txt_ on trouve des phrases en anglais. Sur le fichier _IWSLT10_BTEC.train.fr.txt_ on trouve les mêmes phrases correspondantes en français."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEXbautJk6Fw"
      },
      "source": [
        "**Q2** : Pour chacun des fichiers dans les répertoires train, dev et test, exécutez la commande suivante : `cat file_in.txt | awk -F '\\' '{print $NF}' > file_out.clean.txt`. Pour chaque fichier clean créé, exécutez la commande suivante (en changeant LANG par en ou fr selon la langue dans laquelle est le fichier) : `tokenizer.perl -l LANG -lc < ./file_in.LANG.clean.txt > file_out.LANG.tok.txt`. Quelles différences voyez-vous entre les fichiers _.clean.txt_ et _.tok.txt_ ? Quel est l’intérêt de l’opération effectuée ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tozpa-NileNt",
        "outputId": "479c3c9b-1f10-47bc-d4d7-25fa5d2ecdb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esGAiHRUlozk",
        "outputId": "b41bd3fb-eefd-454e-a4b6-5f220143f530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/tp_nmt/tp_nmt\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/tp_nmt/tp_nmt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql5BaUSklJvM"
      },
      "outputs": [],
      "source": [
        "!cat BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.txt | awk -F '\\' '{print $NF}' > BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.clean.txt\n",
        "!cat BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.txt | awk -F '\\' '{print $NF}' > BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.clean.txt\n",
        "!cat BTEC-en-fr/train/IWSLT10_BTEC.train.en.txt | awk -F '\\' '{print $NF}' > BTEC-en-fr/train/IWSLT10_BTEC.train.en.clean.txt\n",
        "!cat BTEC-en-fr/train/IWSLT10_BTEC.train.fr.txt | awk -F '\\' '{print $NF}' > BTEC-en-fr/train/IWSLT10_BTEC.train.fr.clean.txt\n",
        "!cat BTEC-en-fr/test/IWSLT09_BTEC.testset.en.txt | awk -F '\\' '{print $NF}' > BTEC-en-fr/test/IWSLT09_BTEC.testset.en.clean.txt\n",
        "!cat BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.txt | awk -F '\\' '{print $NF}' > BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.clean.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Of4QVjvoJed",
        "outputId": "b900eb13-fd8e-47c2-c749-b97ddc91e899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 1\n"
          ]
        }
      ],
      "source": [
        "!perl tokenizer.perl -l en -lc < BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.clean.txt > BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt\n",
        "!perl tokenizer.perl -l fr -lc < BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.clean.txt > BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt\n",
        "!perl tokenizer.perl -l en -lc < BTEC-en-fr/train/IWSLT10_BTEC.train.en.clean.txt > BTEC-en-fr/train/IWSLT10_BTEC.train.en.tok.txt\n",
        "!perl tokenizer.perl -l fr -lc < BTEC-en-fr/train/IWSLT10_BTEC.train.fr.clean.txt > BTEC-en-fr/train/IWSLT10_BTEC.train.fr.tok.txt\n",
        "!perl tokenizer.perl -l en -lc < BTEC-en-fr/test/IWSLT09_BTEC.testset.en.clean.txt > BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt\n",
        "!perl tokenizer.perl -l fr -lc < BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.clean.txt > BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl1o3muNs9cg"
      },
      "source": [
        "Réponse à la question 2 :\n",
        "\n",
        "Les fichiers .tok ont géré la ponctuation : des espaces ont été rajoutés avant chaque ponctuation (les virgules, les points, ...)\n",
        "Les apostrophes ont été remplacées par leur code HTML \\&apos;.\n",
        "\n",
        "Cela permettra de mieux traiter l'identification des mots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEeCzglSpPbP"
      },
      "source": [
        "**Q3** : Pourquoi avoir un corpus séparé en 3 parties (train, dev et test) ? A quoi vont servir chacun de ces fichiers ? (Note: bien expliquer la différence d'utilisation entre dev et test et la raison de cela)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJPGfTNMuFyj"
      },
      "source": [
        "Le train set contient les données sur lesquelles le réseau apprend.\n",
        "\n",
        "Le dev set contient les données permettant d'ajuster les paramètres du réseau durant l'apprentissage.\n",
        "\n",
        "Le test set contient des données que le réseau n'a jamais vu et qui permettent d'évaluer la précision de notre réseau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3ozPV2uTAT"
      },
      "source": [
        "## Création d’un modèle de TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM713eyjuj_r",
        "outputId": "56cca649-592e-4c15-995f-2f6f336b0006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-15 11:34:16,441 INFO] Counter vocab from -1 samples.\n",
            "[2022-02-15 11:34:16,442 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2022-02-15 11:34:16,459 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-15 11:34:16,865 INFO] Counters src:9978\n",
            "[2022-02-15 11:34:16,865 INFO] Counters tgt:8194\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/build_vocab.py\", line 71, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/build_vocab.py\", line 55, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/build_vocab.py\", line 44, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/utils/misc.py\", line 17, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path ./BTEC-en-fr/BTEC-en-fr/vocab.src exists, stop.\n"
          ]
        }
      ],
      "source": [
        "!onmt_build_vocab -config config-base.yaml -n_sample -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM6rTi6cui9y"
      },
      "source": [
        "**Q4** : pourquoi un système de TA a-t-il besoin d'un vocabulaire ? pourquoi est-ce que l'on utilise un vocabulaire basé sur les données d'apprentissage et non-pas un vocabulaire quelconque, plus universel ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBZLLjjWu5oW"
      },
      "source": [
        "Un système de traduction automatique a besoin d'un vocaculaire pour extraire un sous langage qui soit plus facile à manipuler. \n",
        "<span style=\"color:red\"> C'est à dire? Le vocabulaire permet d'associer à chaque mot un idéntifiant unique et un vecteur qui le represente et dont les valuers seronts appris par le réseau à travers l'entrainement. </span>\n",
        "\n",
        "\n",
        "On utilise un vocabulaire basé sur les données d'apprentissage et non pas un vocabulaire quelconque car cela permet d'avoir une correspondance parfaite entre le vocabulaire de la langue source et sa traduction. Cela permet aussi d'avoir un cadre plus restreint pour l'apprentissage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr-MrRmjvGhP"
      },
      "source": [
        "**Q5** : Quels informations nous donnent les lignes\n",
        "  ````\n",
        "  [...] Counters src:9978\n",
        "  [...] Counters tgt:8194\n",
        "  ````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7YmHp1cwox6"
      },
      "source": [
        "Cela signifie que dans le vocabulaire du langage source, on a 9978 mots différents et dans le vocabulaire du langage target, on en a 8194."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cK21DAD1B1d",
        "outputId": "94496716-99ab-4e26-ebf2-eefb940e54af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-15 11:34:19,971 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-15 11:34:20,161 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-15 11:34:20,161 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-15 11:34:20,162 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-15 11:34:20,163 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-15 11:34:20,163 INFO] Loading vocab from text file...\n",
            "[2022-02-15 11:34:20,163 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-15 11:34:20,341 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-15 11:34:20,347 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-15 11:34:20,571 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-15 11:34:20,576 INFO] Building fields with vocab in counters...\n",
            "[2022-02-15 11:34:20,586 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-15 11:34:20,599 INFO]  * src vocab size: 9980.\n",
            "[2022-02-15 11:34:20,600 INFO]  * src vocab size = 9980\n",
            "[2022-02-15 11:34:20,600 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-15 11:34:20,606 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-15 11:34:29,876 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-15 11:34:29,878 INFO] encoder: 5635120\n",
            "[2022-02-15 11:34:29,878 INFO] decoder: 7244222\n",
            "[2022-02-15 11:34:29,878 INFO] * number of parameters: 12879342\n",
            "[2022-02-15 11:34:29,880 INFO] Starting training on GPU: [0]\n",
            "[2022-02-15 11:34:29,881 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-15 11:34:29,881 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-15 11:34:29,881 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-15 11:34:30,664 INFO] Step 20/ 2000; acc:   7.71; ppl: 3048.22; xent: 8.02; lr: 1.00000; 7963/8614 tok/s;      1 sec\n",
            "[2022-02-15 11:34:31,162 INFO] Step 40/ 2000; acc:  16.61; ppl: 385.09; xent: 5.95; lr: 1.00000; 11296/11917 tok/s;      1 sec\n",
            "[2022-02-15 11:34:31,808 INFO] Step 60/ 2000; acc:  16.13; ppl: 331.50; xent: 5.80; lr: 1.00000; 12099/12205 tok/s;      2 sec\n",
            "[2022-02-15 11:34:32,344 INFO] Step 80/ 2000; acc:  20.11; ppl: 217.03; xent: 5.38; lr: 1.00000; 11494/11507 tok/s;      2 sec\n",
            "[2022-02-15 11:34:32,833 INFO] Step 100/ 2000; acc:  22.22; ppl: 170.62; xent: 5.14; lr: 1.00000; 11583/13055 tok/s;      3 sec\n",
            "[2022-02-15 11:34:33,405 INFO] Step 120/ 2000; acc:  22.18; ppl: 167.41; xent: 5.12; lr: 1.00000; 12756/12644 tok/s;      4 sec\n",
            "[2022-02-15 11:34:34,031 INFO] Step 140/ 2000; acc:  24.55; ppl: 148.49; xent: 5.00; lr: 1.00000; 11317/11595 tok/s;      4 sec\n",
            "[2022-02-15 11:34:34,485 INFO] Step 160/ 2000; acc:  30.77; ppl: 99.53; xent: 4.60; lr: 1.00000; 11673/12488 tok/s;      5 sec\n",
            "[2022-02-15 11:34:35,049 INFO] Step 180/ 2000; acc:  27.21; ppl: 110.47; xent: 4.70; lr: 1.00000; 12865/13213 tok/s;      5 sec\n",
            "[2022-02-15 11:34:35,612 INFO] Step 200/ 2000; acc:  28.95; ppl: 104.17; xent: 4.65; lr: 1.00000; 11780/12691 tok/s;      6 sec\n",
            "[2022-02-15 11:34:36,113 INFO] Step 220/ 2000; acc:  33.07; ppl: 76.36; xent: 4.34; lr: 1.00000; 11454/12264 tok/s;      6 sec\n",
            "[2022-02-15 11:34:36,631 INFO] Step 240/ 2000; acc:  33.76; ppl: 76.99; xent: 4.34; lr: 1.00000; 12271/12577 tok/s;      7 sec\n",
            "[2022-02-15 11:34:37,265 INFO] Step 260/ 2000; acc:  31.85; ppl: 84.39; xent: 4.44; lr: 1.00000; 10351/11295 tok/s;      7 sec\n",
            "[2022-02-15 11:34:37,796 INFO] Step 280/ 2000; acc:  33.55; ppl: 71.13; xent: 4.26; lr: 1.00000; 11251/12197 tok/s;      8 sec\n",
            "[2022-02-15 11:34:38,322 INFO] Step 300/ 2000; acc:  34.73; ppl: 67.23; xent: 4.21; lr: 1.00000; 11700/11974 tok/s;      8 sec\n",
            "[2022-02-15 11:34:38,907 INFO] Step 320/ 2000; acc:  34.47; ppl: 68.24; xent: 4.22; lr: 1.00000; 11852/12330 tok/s;      9 sec\n",
            "[2022-02-15 11:34:39,497 INFO] Step 340/ 2000; acc:  35.95; ppl: 61.06; xent: 4.11; lr: 1.00000; 10899/10536 tok/s;     10 sec\n",
            "[2022-02-15 11:34:39,960 INFO] Step 360/ 2000; acc:  38.37; ppl: 48.61; xent: 3.88; lr: 1.00000; 12276/12913 tok/s;     10 sec\n",
            "[2022-02-15 11:34:40,589 INFO] Step 380/ 2000; acc:  31.16; ppl: 75.39; xent: 4.32; lr: 1.00000; 12655/12834 tok/s;     11 sec\n",
            "[2022-02-15 11:34:41,115 INFO] Step 400/ 2000; acc:  39.22; ppl: 51.43; xent: 3.94; lr: 1.00000; 11717/11477 tok/s;     11 sec\n",
            "[2022-02-15 11:34:41,591 INFO] Step 420/ 2000; acc:  38.42; ppl: 48.15; xent: 3.87; lr: 1.00000; 11978/13034 tok/s;     12 sec\n",
            "[2022-02-15 11:34:42,154 INFO] Step 440/ 2000; acc:  35.76; ppl: 52.48; xent: 3.96; lr: 1.00000; 12951/12866 tok/s;     12 sec\n",
            "[2022-02-15 11:34:42,783 INFO] Step 460/ 2000; acc:  33.32; ppl: 65.82; xent: 4.19; lr: 1.00000; 11245/11830 tok/s;     13 sec\n",
            "[2022-02-15 11:34:43,245 INFO] Step 480/ 2000; acc:  38.88; ppl: 47.26; xent: 3.86; lr: 1.00000; 11368/12204 tok/s;     13 sec\n",
            "[2022-02-15 11:34:43,822 INFO] Step 500/ 2000; acc:  36.40; ppl: 49.84; xent: 3.91; lr: 1.00000; 12547/12738 tok/s;     14 sec\n",
            "[2022-02-15 11:34:44,415 INFO] Step 520/ 2000; acc:  37.03; ppl: 51.64; xent: 3.94; lr: 1.00000; 11094/11746 tok/s;     15 sec\n",
            "[2022-02-15 11:34:44,927 INFO] Step 540/ 2000; acc:  40.42; ppl: 43.50; xent: 3.77; lr: 1.00000; 11419/12080 tok/s;     15 sec\n",
            "[2022-02-15 11:34:45,463 INFO] Step 560/ 2000; acc:  39.98; ppl: 44.03; xent: 3.78; lr: 1.00000; 12032/12529 tok/s;     16 sec\n",
            "[2022-02-15 11:34:45,917 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-15 11:34:46,063 INFO] Step 580/ 2000; acc:  37.63; ppl: 47.86; xent: 3.87; lr: 1.00000; 11100/11982 tok/s;     16 sec\n",
            "[2022-02-15 11:34:46,570 INFO] Step 600/ 2000; acc:  40.72; ppl: 39.77; xent: 3.68; lr: 1.00000; 11832/12635 tok/s;     17 sec\n",
            "[2022-02-15 11:34:47,078 INFO] Step 620/ 2000; acc:  40.74; ppl: 40.46; xent: 3.70; lr: 1.00000; 11992/12525 tok/s;     17 sec\n",
            "[2022-02-15 11:34:47,209 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-15 11:34:47,392 INFO] Validation perplexity: 22.1507\n",
            "[2022-02-15 11:34:47,393 INFO] Validation accuracy: 47.427\n",
            "[2022-02-15 11:34:47,453 INFO] Saving checkpoint ./models/base/model_step_625.pt\n",
            "[2022-02-15 11:34:48,700 INFO] Step 640/ 2000; acc:  37.90; ppl: 47.57; xent: 3.86; lr: 1.00000; 4241/4360 tok/s;     19 sec\n",
            "[2022-02-15 11:34:49,308 INFO] Step 660/ 2000; acc:  39.26; ppl: 40.18; xent: 3.69; lr: 1.00000; 10136/11360 tok/s;     19 sec\n",
            "[2022-02-15 11:34:49,783 INFO] Step 680/ 2000; acc:  43.75; ppl: 31.41; xent: 3.45; lr: 1.00000; 11853/12333 tok/s;     20 sec\n",
            "[2022-02-15 11:34:50,429 INFO] Step 700/ 2000; acc:  38.34; ppl: 43.75; xent: 3.78; lr: 1.00000; 12010/11716 tok/s;     21 sec\n",
            "[2022-02-15 11:34:50,953 INFO] Step 720/ 2000; acc:  42.35; ppl: 36.14; xent: 3.59; lr: 1.00000; 11898/11629 tok/s;     21 sec\n",
            "[2022-02-15 11:34:51,527 INFO] Step 740/ 2000; acc:  42.36; ppl: 32.24; xent: 3.47; lr: 1.00000; 9951/11289 tok/s;     22 sec\n",
            "[2022-02-15 11:34:52,103 INFO] Step 760/ 2000; acc:  40.70; ppl: 39.04; xent: 3.66; lr: 1.00000; 12803/12771 tok/s;     22 sec\n",
            "[2022-02-15 11:34:52,723 INFO] Step 780/ 2000; acc:  39.63; ppl: 39.74; xent: 3.68; lr: 1.00000; 11287/12238 tok/s;     23 sec\n",
            "[2022-02-15 11:34:53,186 INFO] Step 800/ 2000; acc:  47.38; ppl: 26.68; xent: 3.28; lr: 1.00000; 11203/11980 tok/s;     23 sec\n",
            "[2022-02-15 11:34:53,743 INFO] Step 820/ 2000; acc:  41.54; ppl: 35.13; xent: 3.56; lr: 1.00000; 12761/13057 tok/s;     24 sec\n",
            "[2022-02-15 11:34:54,326 INFO] Step 840/ 2000; acc:  41.77; ppl: 37.43; xent: 3.62; lr: 1.00000; 11290/12098 tok/s;     24 sec\n",
            "[2022-02-15 11:34:54,810 INFO] Step 860/ 2000; acc:  44.98; ppl: 28.64; xent: 3.35; lr: 1.00000; 11947/12657 tok/s;     25 sec\n",
            "[2022-02-15 11:34:55,335 INFO] Step 880/ 2000; acc:  44.13; ppl: 29.84; xent: 3.40; lr: 1.00000; 12228/12548 tok/s;     25 sec\n",
            "[2022-02-15 11:34:56,034 INFO] Step 900/ 2000; acc:  42.45; ppl: 31.83; xent: 3.46; lr: 1.00000; 9508/10227 tok/s;     26 sec\n",
            "[2022-02-15 11:34:56,548 INFO] Step 920/ 2000; acc:  43.77; ppl: 29.33; xent: 3.38; lr: 1.00000; 11757/12776 tok/s;     27 sec\n",
            "[2022-02-15 11:34:57,054 INFO] Step 940/ 2000; acc:  44.10; ppl: 31.98; xent: 3.47; lr: 1.00000; 12364/12410 tok/s;     27 sec\n",
            "[2022-02-15 11:34:57,636 INFO] Step 960/ 2000; acc:  42.86; ppl: 33.30; xent: 3.51; lr: 1.00000; 12131/12346 tok/s;     28 sec\n",
            "[2022-02-15 11:34:58,260 INFO] Step 980/ 2000; acc:  45.95; ppl: 25.83; xent: 3.25; lr: 1.00000; 10143/10318 tok/s;     28 sec\n",
            "[2022-02-15 11:34:58,728 INFO] Step 1000/ 2000; acc:  47.43; ppl: 23.61; xent: 3.16; lr: 1.00000; 12086/12358 tok/s;     29 sec\n",
            "[2022-02-15 11:34:59,335 INFO] Step 1020/ 2000; acc:  39.66; ppl: 35.17; xent: 3.56; lr: 1.00000; 12827/12860 tok/s;     29 sec\n",
            "[2022-02-15 11:34:59,855 INFO] Step 1040/ 2000; acc:  46.92; ppl: 25.39; xent: 3.23; lr: 1.00000; 11978/11618 tok/s;     30 sec\n",
            "[2022-02-15 11:35:00,353 INFO] Step 1060/ 2000; acc:  46.41; ppl: 23.87; xent: 3.17; lr: 1.00000; 11539/12756 tok/s;     30 sec\n",
            "[2022-02-15 11:35:00,934 INFO] Step 1080/ 2000; acc:  43.07; ppl: 30.12; xent: 3.41; lr: 1.00000; 12747/12540 tok/s;     31 sec\n",
            "[2022-02-15 11:35:01,584 INFO] Step 1100/ 2000; acc:  41.70; ppl: 33.22; xent: 3.50; lr: 1.00000; 11026/11839 tok/s;     32 sec\n",
            "[2022-02-15 11:35:02,038 INFO] Step 1120/ 2000; acc:  47.99; ppl: 23.05; xent: 3.14; lr: 1.00000; 11598/12434 tok/s;     32 sec\n",
            "[2022-02-15 11:35:02,616 INFO] Step 1140/ 2000; acc:  44.41; ppl: 27.70; xent: 3.32; lr: 1.00000; 12459/12636 tok/s;     33 sec\n",
            "[2022-02-15 11:35:03,169 INFO] Step 1160/ 2000; acc:  45.08; ppl: 28.49; xent: 3.35; lr: 1.00000; 11775/12198 tok/s;     33 sec\n",
            "[2022-02-15 11:35:03,677 INFO] Step 1180/ 2000; acc:  46.70; ppl: 23.04; xent: 3.14; lr: 1.00000; 11272/12421 tok/s;     34 sec\n",
            "[2022-02-15 11:35:04,191 INFO] Step 1200/ 2000; acc:  46.49; ppl: 24.12; xent: 3.18; lr: 1.00000; 12403/12868 tok/s;     34 sec\n",
            "[2022-02-15 11:35:04,665 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-15 11:35:04,808 INFO] Step 1220/ 2000; acc:  45.03; ppl: 27.66; xent: 3.32; lr: 1.00000; 10797/11552 tok/s;     35 sec\n",
            "[2022-02-15 11:35:05,317 INFO] Step 1240/ 2000; acc:  46.34; ppl: 22.89; xent: 3.13; lr: 1.00000; 11890/12701 tok/s;     35 sec\n",
            "[2022-02-15 11:35:05,735 INFO] Validation perplexity: 13.6856\n",
            "[2022-02-15 11:35:05,736 INFO] Validation accuracy: 55.4242\n",
            "[2022-02-15 11:35:05,797 INFO] Saving checkpoint ./models/base/model_step_1250.pt\n",
            "[2022-02-15 11:35:07,121 INFO] Step 1260/ 2000; acc:  46.57; ppl: 24.27; xent: 3.19; lr: 1.00000; 3399/3557 tok/s;     37 sec\n",
            "[2022-02-15 11:35:07,693 INFO] Step 1280/ 2000; acc:  44.31; ppl: 28.42; xent: 3.35; lr: 1.00000; 12044/12400 tok/s;     38 sec\n",
            "[2022-02-15 11:35:08,300 INFO] Step 1300/ 2000; acc:  47.78; ppl: 21.46; xent: 3.07; lr: 1.00000; 10370/10941 tok/s;     38 sec\n",
            "[2022-02-15 11:35:08,789 INFO] Step 1320/ 2000; acc:  50.18; ppl: 19.36; xent: 2.96; lr: 1.00000; 11614/12213 tok/s;     39 sec\n",
            "[2022-02-15 11:35:09,424 INFO] Step 1340/ 2000; acc:  43.40; ppl: 27.37; xent: 3.31; lr: 1.00000; 12297/12332 tok/s;     40 sec\n",
            "[2022-02-15 11:35:09,968 INFO] Step 1360/ 2000; acc:  48.62; ppl: 22.43; xent: 3.11; lr: 1.00000; 11387/11310 tok/s;     40 sec\n",
            "[2022-02-15 11:35:10,448 INFO] Step 1380/ 2000; acc:  48.93; ppl: 20.06; xent: 3.00; lr: 1.00000; 11846/13276 tok/s;     41 sec\n",
            "[2022-02-15 11:35:11,038 INFO] Step 1400/ 2000; acc:  45.51; ppl: 24.73; xent: 3.21; lr: 1.00000; 12430/12471 tok/s;     41 sec\n",
            "[2022-02-15 11:35:11,656 INFO] Step 1420/ 2000; acc:  45.40; ppl: 24.95; xent: 3.22; lr: 1.00000; 11273/12055 tok/s;     42 sec\n",
            "[2022-02-15 11:35:12,120 INFO] Step 1440/ 2000; acc:  52.53; ppl: 17.73; xent: 2.88; lr: 1.00000; 11014/11836 tok/s;     42 sec\n",
            "[2022-02-15 11:35:12,678 INFO] Step 1460/ 2000; acc:  47.72; ppl: 20.87; xent: 3.04; lr: 1.00000; 12463/13030 tok/s;     43 sec\n",
            "[2022-02-15 11:35:13,257 INFO] Step 1480/ 2000; acc:  47.07; ppl: 22.86; xent: 3.13; lr: 1.00000; 11166/12194 tok/s;     43 sec\n",
            "[2022-02-15 11:35:13,750 INFO] Step 1500/ 2000; acc:  49.97; ppl: 18.65; xent: 2.93; lr: 1.00000; 11815/12624 tok/s;     44 sec\n",
            "[2022-02-15 11:35:14,253 INFO] Step 1520/ 2000; acc:  49.11; ppl: 20.35; xent: 3.01; lr: 1.00000; 12890/13141 tok/s;     44 sec\n",
            "[2022-02-15 11:35:14,881 INFO] Step 1540/ 2000; acc:  47.68; ppl: 21.09; xent: 3.05; lr: 1.00000; 10706/11307 tok/s;     45 sec\n",
            "[2022-02-15 11:35:15,396 INFO] Step 1560/ 2000; acc:  49.83; ppl: 18.26; xent: 2.90; lr: 1.00000; 11913/12504 tok/s;     46 sec\n",
            "[2022-02-15 11:35:15,914 INFO] Step 1580/ 2000; acc:  49.56; ppl: 19.81; xent: 2.99; lr: 1.00000; 12152/12307 tok/s;     46 sec\n",
            "[2022-02-15 11:35:16,515 INFO] Step 1600/ 2000; acc:  45.55; ppl: 23.87; xent: 3.17; lr: 1.00000; 11885/12038 tok/s;     47 sec\n",
            "[2022-02-15 11:35:17,130 INFO] Step 1620/ 2000; acc:  49.05; ppl: 19.20; xent: 2.95; lr: 1.00000; 10345/10544 tok/s;     47 sec\n",
            "[2022-02-15 11:35:17,603 INFO] Step 1640/ 2000; acc:  53.07; ppl: 15.48; xent: 2.74; lr: 1.00000; 11967/12246 tok/s;     48 sec\n",
            "[2022-02-15 11:35:18,220 INFO] Step 1660/ 2000; acc:  45.23; ppl: 24.21; xent: 3.19; lr: 1.00000; 12758/12777 tok/s;     48 sec\n",
            "[2022-02-15 11:35:18,748 INFO] Step 1680/ 2000; acc:  49.94; ppl: 18.73; xent: 2.93; lr: 1.00000; 11650/11600 tok/s;     49 sec\n",
            "[2022-02-15 11:35:19,226 INFO] Step 1700/ 2000; acc:  52.28; ppl: 15.88; xent: 2.76; lr: 1.00000; 11896/13099 tok/s;     49 sec\n",
            "[2022-02-15 11:35:19,792 INFO] Step 1720/ 2000; acc:  48.04; ppl: 20.04; xent: 3.00; lr: 1.00000; 12929/12736 tok/s;     50 sec\n",
            "[2022-02-15 11:35:20,445 INFO] Step 1740/ 2000; acc:  46.93; ppl: 21.84; xent: 3.08; lr: 1.00000; 10709/11574 tok/s;     51 sec\n",
            "[2022-02-15 11:35:20,919 INFO] Step 1760/ 2000; acc:  53.07; ppl: 15.54; xent: 2.74; lr: 1.00000; 11101/11754 tok/s;     51 sec\n",
            "[2022-02-15 11:35:21,489 INFO] Step 1780/ 2000; acc:  48.06; ppl: 18.92; xent: 2.94; lr: 1.00000; 12572/12899 tok/s;     52 sec\n",
            "[2022-02-15 11:35:22,071 INFO] Step 1800/ 2000; acc:  49.09; ppl: 18.87; xent: 2.94; lr: 1.00000; 11264/11770 tok/s;     52 sec\n",
            "[2022-02-15 11:35:22,574 INFO] Step 1820/ 2000; acc:  51.50; ppl: 16.01; xent: 2.77; lr: 1.00000; 11419/12419 tok/s;     53 sec\n",
            "[2022-02-15 11:35:23,077 INFO] Step 1840/ 2000; acc:  50.03; ppl: 17.57; xent: 2.87; lr: 1.00000; 12676/12931 tok/s;     53 sec\n",
            "[2022-02-15 11:35:23,550 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-15 11:35:23,696 INFO] Step 1860/ 2000; acc:  48.92; ppl: 18.90; xent: 2.94; lr: 1.00000; 10614/11545 tok/s;     54 sec\n",
            "[2022-02-15 11:35:24,283 INFO] Validation perplexity: 10.081\n",
            "[2022-02-15 11:35:24,284 INFO] Validation accuracy: 59.5039\n",
            "[2022-02-15 11:35:24,342 INFO] Saving checkpoint ./models/base/model_step_1875.pt\n",
            "[2022-02-15 11:35:25,434 INFO] Step 1880/ 2000; acc:  50.62; ppl: 16.49; xent: 2.80; lr: 1.00000; 3475/3785 tok/s;     56 sec\n",
            "[2022-02-15 11:35:25,976 INFO] Step 1900/ 2000; acc:  51.51; ppl: 16.48; xent: 2.80; lr: 1.00000; 11458/11731 tok/s;     56 sec\n",
            "[2022-02-15 11:35:26,560 INFO] Step 1920/ 2000; acc:  48.52; ppl: 19.75; xent: 2.98; lr: 1.00000; 11969/12313 tok/s;     57 sec\n",
            "[2022-02-15 11:35:27,148 INFO] Step 1940/ 2000; acc:  51.65; ppl: 15.79; xent: 2.76; lr: 1.00000; 10830/11089 tok/s;     57 sec\n",
            "[2022-02-15 11:35:27,637 INFO] Step 1960/ 2000; acc:  53.80; ppl: 14.09; xent: 2.65; lr: 1.00000; 11604/12505 tok/s;     58 sec\n",
            "[2022-02-15 11:35:28,295 INFO] Step 1980/ 2000; acc:  46.91; ppl: 20.94; xent: 3.04; lr: 1.00000; 11938/12040 tok/s;     58 sec\n",
            "[2022-02-15 11:35:28,827 INFO] Step 2000/ 2000; acc:  53.07; ppl: 15.56; xent: 2.74; lr: 1.00000; 11748/11453 tok/s;     59 sec\n",
            "[2022-02-15 11:35:28,887 INFO] Saving checkpoint ./models/base/model_step_2000.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-base.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDmMmM-6w5yD"
      },
      "source": [
        "**Q6** : A quoi correspond la valeur ‘acc’ affichée par le script ? Vaut-il mieux que cette valeur soit faible ou élevée à la fin de l’apprentissage ? Pourquoi ? Même question pour ‘ppl’. Le code qui permet de calculer ces variables se trouve dans le fichier [statistics.py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/utils/statistics.py) de OpenNMT-py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmT8to7C1-Mr"
      },
      "source": [
        "La valeur 'acc' correspond à la précision (accuracy). C'est le pourcentage de mots corrects sur le nombre de mots total.\n",
        "Il vaut donc mieux que cette valeur soit élevée à la fin de l'apprentissage car cela signifira que le modèle a bien appris et qu'il sera assez précis pour faire la traduction dans les données de test.\n",
        "\n",
        "La valeur 'ppl' signifie perplexity. Cette valeur se réfère à l'erreur (`exp(min(loss/nb_mots, 100))`). Il vaut donc mieux que cette valeur soit faible à la fin de l'apprentissage car cela signifira que le modèle se trompe peu dans la traduction des mots. <span style=\"color:red\"> Intuition dérrière la définition de perplexity? </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhujPfZ34Cw9"
      },
      "source": [
        "**Q7** : Ouvrez le fichier de configuration _config-base.yaml_ et observez son contenu.\n",
        "Décrivez en détail à quoi correspondent les paramètres suivants (n'hésitez pas à demander a Google!):\n",
        "  - train_step : nombre de pas d'apprentissage <span style=\"color:red\">C'est quoi un pas d'apprentissage? (vous n'avez que traduit le nom de la variable sans expliquer sa signification). train_step = nombre total d'étapes d'optimisation des poids du modèle, où chaque étape est constituée d'un passage en avant d'un lot (batch) de données d'apprentissage dans le modèle (forward pass) et d'un passage en arrière des gradients générés par la fonction de perte (backward pass), suivi par une mise à jours des poids du modèle. </span>\n",
        "  - valid_step : effectue la validation tous les 'valid_step' fois\n",
        "  - enc_layers : nombre de couches dans l'encoder\n",
        "  - dec_layers : nombre de couches dans le décodeur\n",
        "  - enc_rnn_size : nombre de neurones sur une couche du réseau de l'encodeur.\n",
        "  - dec_rnn_size : nombre de neurones sur une couche du réseau du décodeur.\n",
        "  - batch_size : taille maximum du batch pour l'entrainement. Le batch est un paquet de données qu'on envoie dans le reseau. Nombre de phrases qui sont processées à chaque step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbUE5Pi09KlC"
      },
      "source": [
        "##  Evaluation d’un modèle de TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqnN4Bwu-hWb",
        "outputId": "8aa69309-0bab-4ba3-abc0-015d3cc9868e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-15 11:35:32,478 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-15 11:35:39,285 INFO] PRED AVG SCORE: -1.2937, PRED PPL: 3.6461\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-15 11:35:41,475 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-15 11:35:45,413 INFO] PRED AVG SCORE: -1.1071, PRED PPL: 3.0257\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-15 11:35:47,609 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-15 11:35:51,624 INFO] PRED AVG SCORE: -0.9541, PRED PPL: 2.5965\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-15 11:35:53,828 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-15 11:35:57,822 INFO] PRED AVG SCORE: -0.9836, PRED PPL: 2.6741\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -model models/base/model_step_625.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/pred_625.txt\n",
        "!onmt_translate -model models/base/model_step_1250.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/pred_1250.txt\n",
        "!onmt_translate -model models/base/model_step_1875.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/pred_1875.txt\n",
        "!onmt_translate -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/pred_2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEOw5BlO-6SH"
      },
      "source": [
        "**Q8** : Entre quelle valeur et quelle valeur est compris un score BLEU ? Vaut-il mieux que la valeur obtenue soit élevée ou faible ? Pourquoi ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA_HMmJlAU5f"
      },
      "source": [
        "Un score BLEU est compris entre 0 et 1 (ou 0 et 100% s'il est rapporté en pourcentage).\n",
        "Il vaut mieux que la valeur obtenue soit élevée (proche de 1). Plus on est proche de 1, plus les phrases sont similaires.  <span style=\"color:red\"> Similaires à quoi? Est-ce qu'un score BLEU plus élévé veut toujours dire que la trad est méilleure ? -> non</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BqziB00-80p"
      },
      "source": [
        "**Q9** : Pour chacun des modèles intermédiaires, calculer un score BLEU à l'aide du script `multi-bleu.perl`. Quel modèle a le meilleur score BLEU ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGKy8f0TBWtJ",
        "outputId": "5b882378-2906-4e62-866b-e8dc616623b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 6.29, 29.6/9.2/4.5/1.3 (BP=1.000, ratio=1.262, hyp_len=4509, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 13.83, 51.0/21.0/12.3/5.4 (BP=0.847, ratio=0.858, hyp_len=3065, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 17.94, 54.6/25.8/15.9/8.0 (BP=0.871, ratio=0.878, hyp_len=3139, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 18.48, 55.4/26.5/16.3/8.1 (BP=0.880, ratio=0.887, hyp_len=3170, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/pred_625.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/pred_1250.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/pred_1875.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/pred_2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcLq4Z4KB8jA"
      },
      "source": [
        "Le meilleur score BLEU est celui du modèle model_step_2000.pt (18,48%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzoHljJvCeai"
      },
      "source": [
        "## Optimisation des paramètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhZREWnxCf22"
      },
      "source": [
        "**Q10 - nombre de training steps** : Augmenter le nombre de steps permet-il d’obtenir un meilleur score BLEU ? Pourquoi? Jusqu’à combien\n",
        "de steps ? comment utiliser l'option `early7.2.3_stopping` ([voir ici](https://opennmt.net/OpenNMT-py/options/train.html)) pour trouver le nombre optimal de steps ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Abt9tayIM7Y"
      },
      "source": [
        "Nous enregistrons ces modèles dans `models/nb_steps/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3WrD8DAHc2i",
        "outputId": "4d5277df-b2a3-4611-c951-0c7984b40b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 09:43:14,222 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-19 09:43:14,223 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-19 09:43:14,223 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-19 09:43:14,224 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-19 09:43:14,224 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-19 09:43:14,224 INFO] Loading vocab from text file...\n",
            "[2022-02-19 09:43:14,225 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-19 09:43:14,258 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-19 09:43:14,265 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-19 09:43:14,281 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-19 09:43:14,285 INFO] Building fields with vocab in counters...\n",
            "[2022-02-19 09:43:14,293 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-19 09:43:14,305 INFO]  * src vocab size: 9980.\n",
            "[2022-02-19 09:43:14,306 INFO]  * src vocab size = 9980\n",
            "[2022-02-19 09:43:14,306 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-19 09:43:14,309 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 09:43:18,340 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-19 09:43:18,341 INFO] encoder: 5635120\n",
            "[2022-02-19 09:43:18,341 INFO] decoder: 7244222\n",
            "[2022-02-19 09:43:18,341 INFO] * number of parameters: 12879342\n",
            "[2022-02-19 09:43:18,342 INFO] Starting training on GPU: [0]\n",
            "[2022-02-19 09:43:18,343 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-19 09:43:18,343 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-19 09:43:18,343 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-19 09:43:18,748 INFO] Step 20/20000; acc:   8.46; ppl: 2661.16; xent: 7.89; lr: 1.00000; 15928/17339 tok/s;      0 sec\n",
            "[2022-02-19 09:43:19,058 INFO] Step 40/20000; acc:  17.13; ppl: 429.70; xent: 6.06; lr: 1.00000; 19374/20449 tok/s;      1 sec\n",
            "[2022-02-19 09:43:19,359 INFO] Step 60/20000; acc:  16.14; ppl: 339.84; xent: 5.83; lr: 1.00000; 23420/23788 tok/s;      1 sec\n",
            "[2022-02-19 09:43:19,688 INFO] Step 80/20000; acc:  18.34; ppl: 248.05; xent: 5.51; lr: 1.00000; 20780/20725 tok/s;      1 sec\n",
            "[2022-02-19 09:43:19,964 INFO] Step 100/20000; acc:  20.79; ppl: 183.69; xent: 5.21; lr: 1.00000; 22102/23256 tok/s;      2 sec\n",
            "[2022-02-19 09:43:20,225 INFO] Step 120/20000; acc:  23.74; ppl: 153.82; xent: 5.04; lr: 1.00000; 23528/24543 tok/s;      2 sec\n",
            "[2022-02-19 09:43:20,558 INFO] Step 140/20000; acc:  23.62; ppl: 150.81; xent: 5.02; lr: 1.00000; 21922/22773 tok/s;      2 sec\n",
            "[2022-02-19 09:43:20,810 INFO] Step 160/20000; acc:  29.31; ppl: 110.87; xent: 4.71; lr: 1.00000; 22074/23772 tok/s;      2 sec\n",
            "[2022-02-19 09:43:21,103 INFO] Step 180/20000; acc:  27.00; ppl: 127.39; xent: 4.85; lr: 1.00000; 23768/24373 tok/s;      3 sec\n",
            "[2022-02-19 09:43:21,437 INFO] Step 200/20000; acc:  29.20; ppl: 104.02; xent: 4.64; lr: 1.00000; 20805/21051 tok/s;      3 sec\n",
            "[2022-02-19 09:43:21,705 INFO] Step 220/20000; acc:  32.39; ppl: 80.08; xent: 4.38; lr: 1.00000; 20602/23222 tok/s;      3 sec\n",
            "[2022-02-19 09:43:22,001 INFO] Step 240/20000; acc:  31.53; ppl: 90.48; xent: 4.51; lr: 1.00000; 21501/23068 tok/s;      4 sec\n",
            "[2022-02-19 09:43:22,311 INFO] Step 260/20000; acc:  33.82; ppl: 70.29; xent: 4.25; lr: 1.00000; 20846/21068 tok/s;      4 sec\n",
            "[2022-02-19 09:43:22,581 INFO] Step 280/20000; acc:  33.12; ppl: 77.68; xent: 4.35; lr: 1.00000; 22626/24631 tok/s;      4 sec\n",
            "[2022-02-19 09:43:22,870 INFO] Step 300/20000; acc:  34.46; ppl: 71.26; xent: 4.27; lr: 1.00000; 22122/23129 tok/s;      5 sec\n",
            "[2022-02-19 09:43:23,170 INFO] Step 320/20000; acc:  33.62; ppl: 67.71; xent: 4.22; lr: 1.00000; 22020/22937 tok/s;      5 sec\n",
            "[2022-02-19 09:43:23,542 INFO] Step 340/20000; acc:  33.30; ppl: 65.21; xent: 4.18; lr: 1.00000; 17645/18368 tok/s;      5 sec\n",
            "[2022-02-19 09:43:23,833 INFO] Step 360/20000; acc:  35.86; ppl: 63.99; xent: 4.16; lr: 1.00000; 21176/22076 tok/s;      5 sec\n",
            "[2022-02-19 09:43:24,136 INFO] Step 380/20000; acc:  34.56; ppl: 63.41; xent: 4.15; lr: 1.00000; 23647/22789 tok/s;      6 sec\n",
            "[2022-02-19 09:43:24,474 INFO] Step 400/20000; acc:  34.19; ppl: 64.96; xent: 4.17; lr: 1.00000; 20302/20743 tok/s;      6 sec\n",
            "[2022-02-19 09:43:24,774 INFO] Step 420/20000; acc:  37.52; ppl: 56.16; xent: 4.03; lr: 1.00000; 20388/21264 tok/s;      6 sec\n",
            "[2022-02-19 09:43:25,032 INFO] Step 440/20000; acc:  38.11; ppl: 49.30; xent: 3.90; lr: 1.00000; 24023/23833 tok/s;      7 sec\n",
            "[2022-02-19 09:43:25,385 INFO] Step 460/20000; acc:  36.09; ppl: 52.23; xent: 3.96; lr: 1.00000; 20691/21142 tok/s;      7 sec\n",
            "[2022-02-19 09:43:25,637 INFO] Step 480/20000; acc:  39.82; ppl: 42.13; xent: 3.74; lr: 1.00000; 21778/24295 tok/s;      7 sec\n",
            "[2022-02-19 09:43:25,944 INFO] Step 500/20000; acc:  36.82; ppl: 56.69; xent: 4.04; lr: 1.00000; 22420/22423 tok/s;      8 sec\n",
            "[2022-02-19 09:43:26,266 INFO] Step 520/20000; acc:  36.30; ppl: 56.77; xent: 4.04; lr: 1.00000; 21723/22130 tok/s;      8 sec\n",
            "[2022-02-19 09:43:26,539 INFO] Step 540/20000; acc:  41.05; ppl: 39.88; xent: 3.69; lr: 1.00000; 20578/22658 tok/s;      8 sec\n",
            "[2022-02-19 09:43:26,838 INFO] Step 560/20000; acc:  38.94; ppl: 49.04; xent: 3.89; lr: 1.00000; 21588/22862 tok/s;      8 sec\n",
            "[2022-02-19 09:43:27,082 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-19 09:43:27,168 INFO] Step 580/20000; acc:  40.60; ppl: 42.74; xent: 3.76; lr: 1.00000; 19763/20390 tok/s;      9 sec\n",
            "[2022-02-19 09:43:27,455 INFO] Step 600/20000; acc:  38.34; ppl: 46.95; xent: 3.85; lr: 1.00000; 21039/22986 tok/s;      9 sec\n",
            "[2022-02-19 09:43:27,749 INFO] Step 620/20000; acc:  41.58; ppl: 40.68; xent: 3.71; lr: 1.00000; 21563/22122 tok/s;      9 sec\n",
            "[2022-02-19 09:43:27,826 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-19 09:43:27,960 INFO] Validation perplexity: 22.7673\n",
            "[2022-02-19 09:43:27,961 INFO] Validation accuracy: 47.7979\n",
            "[2022-02-19 09:43:27,961 INFO] Model is improving ppl: inf --> 22.7673.\n",
            "[2022-02-19 09:43:27,961 INFO] Model is improving acc: -inf --> 47.7979.\n",
            "[2022-02-19 09:43:28,015 INFO] Saving checkpoint ./models/nb-steps/model_step_625.pt\n",
            "[2022-02-19 09:43:28,465 INFO] Step 640/20000; acc:  39.05; ppl: 42.73; xent: 3.75; lr: 1.00000; 9252/9730 tok/s;     10 sec\n",
            "[2022-02-19 09:43:28,830 INFO] Step 660/20000; acc:  38.91; ppl: 41.26; xent: 3.72; lr: 1.00000; 17571/19377 tok/s;     10 sec\n",
            "[2022-02-19 09:43:29,132 INFO] Step 680/20000; acc:  42.54; ppl: 36.47; xent: 3.60; lr: 1.00000; 19805/20561 tok/s;     11 sec\n",
            "[2022-02-19 09:43:29,448 INFO] Step 700/20000; acc:  39.60; ppl: 39.96; xent: 3.69; lr: 1.00000; 22107/22262 tok/s;     11 sec\n",
            "[2022-02-19 09:43:29,782 INFO] Step 720/20000; acc:  40.41; ppl: 40.92; xent: 3.71; lr: 1.00000; 20750/20522 tok/s;     11 sec\n",
            "[2022-02-19 09:43:30,066 INFO] Step 740/20000; acc:  41.75; ppl: 37.46; xent: 3.62; lr: 1.00000; 21743/22986 tok/s;     12 sec\n",
            "[2022-02-19 09:43:30,355 INFO] Step 760/20000; acc:  43.28; ppl: 34.44; xent: 3.54; lr: 1.00000; 21588/22238 tok/s;     12 sec\n",
            "[2022-02-19 09:43:30,704 INFO] Step 780/20000; acc:  41.06; ppl: 36.16; xent: 3.59; lr: 1.00000; 20716/21829 tok/s;     12 sec\n",
            "[2022-02-19 09:43:30,961 INFO] Step 800/20000; acc:  45.50; ppl: 28.61; xent: 3.35; lr: 1.00000; 21098/23429 tok/s;     13 sec\n",
            "[2022-02-19 09:43:31,268 INFO] Step 820/20000; acc:  40.63; ppl: 41.12; xent: 3.72; lr: 1.00000; 22156/22711 tok/s;     13 sec\n",
            "[2022-02-19 09:43:31,644 INFO] Step 840/20000; acc:  42.28; ppl: 34.07; xent: 3.53; lr: 1.00000; 18206/18693 tok/s;     13 sec\n",
            "[2022-02-19 09:43:31,920 INFO] Step 860/20000; acc:  45.13; ppl: 28.19; xent: 3.34; lr: 1.00000; 20336/22290 tok/s;     14 sec\n",
            "[2022-02-19 09:43:32,221 INFO] Step 880/20000; acc:  44.60; ppl: 29.64; xent: 3.39; lr: 1.00000; 21285/22713 tok/s;     14 sec\n",
            "[2022-02-19 09:43:32,539 INFO] Step 900/20000; acc:  45.85; ppl: 27.40; xent: 3.31; lr: 1.00000; 20564/20913 tok/s;     14 sec\n",
            "[2022-02-19 09:43:32,817 INFO] Step 920/20000; acc:  43.20; ppl: 31.38; xent: 3.45; lr: 1.00000; 22159/24188 tok/s;     14 sec\n",
            "[2022-02-19 09:43:33,113 INFO] Step 940/20000; acc:  42.92; ppl: 33.82; xent: 3.52; lr: 1.00000; 22080/22421 tok/s;     15 sec\n",
            "[2022-02-19 09:43:33,402 INFO] Step 960/20000; acc:  44.83; ppl: 29.11; xent: 3.37; lr: 1.00000; 23186/23803 tok/s;     15 sec\n",
            "[2022-02-19 09:43:33,777 INFO] Step 980/20000; acc:  43.37; ppl: 30.46; xent: 3.42; lr: 1.00000; 17375/17912 tok/s;     15 sec\n",
            "[2022-02-19 09:43:34,060 INFO] Step 1000/20000; acc:  44.58; ppl: 26.36; xent: 3.27; lr: 1.00000; 21134/21946 tok/s;     16 sec\n",
            "[2022-02-19 09:43:34,416 INFO] Step 1020/20000; acc:  44.11; ppl: 27.94; xent: 3.33; lr: 1.00000; 19871/19711 tok/s;     16 sec\n",
            "[2022-02-19 09:43:34,898 INFO] Step 1040/20000; acc:  42.85; ppl: 30.36; xent: 3.41; lr: 1.00000; 14455/14574 tok/s;     17 sec\n",
            "[2022-02-19 09:43:35,222 INFO] Step 1060/20000; acc:  44.65; ppl: 28.37; xent: 3.35; lr: 1.00000; 19451/19869 tok/s;     17 sec\n",
            "[2022-02-19 09:43:35,495 INFO] Step 1080/20000; acc:  46.88; ppl: 26.29; xent: 3.27; lr: 1.00000; 22953/23258 tok/s;     17 sec\n",
            "[2022-02-19 09:43:35,858 INFO] Step 1100/20000; acc:  43.95; ppl: 26.64; xent: 3.28; lr: 1.00000; 20097/21179 tok/s;     18 sec\n",
            "[2022-02-19 09:43:36,115 INFO] Step 1120/20000; acc:  47.27; ppl: 21.74; xent: 3.08; lr: 1.00000; 21313/23737 tok/s;     18 sec\n",
            "[2022-02-19 09:43:36,437 INFO] Step 1140/20000; acc:  43.67; ppl: 31.89; xent: 3.46; lr: 1.00000; 21180/21137 tok/s;     18 sec\n",
            "[2022-02-19 09:43:36,776 INFO] Step 1160/20000; acc:  45.16; ppl: 27.89; xent: 3.33; lr: 1.00000; 20604/20528 tok/s;     18 sec\n",
            "[2022-02-19 09:43:37,045 INFO] Step 1180/20000; acc:  49.66; ppl: 20.50; xent: 3.02; lr: 1.00000; 20666/22142 tok/s;     19 sec\n",
            "[2022-02-19 09:43:37,356 INFO] Step 1200/20000; acc:  45.51; ppl: 28.44; xent: 3.35; lr: 1.00000; 20638/21820 tok/s;     19 sec\n",
            "[2022-02-19 09:43:37,607 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-19 09:43:37,707 INFO] Step 1220/20000; acc:  46.25; ppl: 23.17; xent: 3.14; lr: 1.00000; 18220/19920 tok/s;     19 sec\n",
            "[2022-02-19 09:43:38,002 INFO] Step 1240/20000; acc:  46.15; ppl: 24.15; xent: 3.18; lr: 1.00000; 20698/22531 tok/s;     20 sec\n",
            "[2022-02-19 09:43:38,267 INFO] Validation perplexity: 12.8995\n",
            "[2022-02-19 09:43:38,267 INFO] Validation accuracy: 55.8646\n",
            "[2022-02-19 09:43:38,267 INFO] Model is improving ppl: 22.7673 --> 12.8995.\n",
            "[2022-02-19 09:43:38,267 INFO] Model is improving acc: 47.7979 --> 55.8646.\n",
            "[2022-02-19 09:43:38,317 INFO] Saving checkpoint ./models/nb-steps/model_step_1250.pt\n",
            "[2022-02-19 09:43:38,750 INFO] Step 1260/20000; acc:  46.07; ppl: 25.21; xent: 3.23; lr: 1.00000; 8438/8945 tok/s;     20 sec\n",
            "[2022-02-19 09:43:39,050 INFO] Step 1280/20000; acc:  46.36; ppl: 24.79; xent: 3.21; lr: 1.00000; 22405/22672 tok/s;     21 sec\n",
            "[2022-02-19 09:43:39,436 INFO] Step 1300/20000; acc:  47.05; ppl: 23.35; xent: 3.15; lr: 1.00000; 16880/17844 tok/s;     21 sec\n",
            "[2022-02-19 09:43:39,740 INFO] Step 1320/20000; acc:  49.63; ppl: 20.45; xent: 3.02; lr: 1.00000; 19829/20711 tok/s;     21 sec\n",
            "[2022-02-19 09:43:40,044 INFO] Step 1340/20000; acc:  46.03; ppl: 24.23; xent: 3.19; lr: 1.00000; 23339/23626 tok/s;     22 sec\n",
            "[2022-02-19 09:43:40,386 INFO] Step 1360/20000; acc:  45.92; ppl: 23.72; xent: 3.17; lr: 1.00000; 20069/20420 tok/s;     22 sec\n",
            "[2022-02-19 09:43:40,679 INFO] Step 1380/20000; acc:  47.75; ppl: 23.64; xent: 3.16; lr: 1.00000; 21183/22244 tok/s;     22 sec\n",
            "[2022-02-19 09:43:40,948 INFO] Step 1400/20000; acc:  49.17; ppl: 21.35; xent: 3.06; lr: 1.00000; 22974/23639 tok/s;     23 sec\n",
            "[2022-02-19 09:43:41,288 INFO] Step 1420/20000; acc:  47.51; ppl: 21.75; xent: 3.08; lr: 1.00000; 21108/21606 tok/s;     23 sec\n",
            "[2022-02-19 09:43:41,555 INFO] Step 1440/20000; acc:  51.47; ppl: 17.89; xent: 2.88; lr: 1.00000; 19977/22651 tok/s;     23 sec\n",
            "[2022-02-19 09:43:41,870 INFO] Step 1460/20000; acc:  46.35; ppl: 24.17; xent: 3.19; lr: 1.00000; 20918/22217 tok/s;     24 sec\n",
            "[2022-02-19 09:43:42,202 INFO] Step 1480/20000; acc:  48.12; ppl: 20.45; xent: 3.02; lr: 1.00000; 20483/21508 tok/s;     24 sec\n",
            "[2022-02-19 09:43:42,463 INFO] Step 1500/20000; acc:  50.52; ppl: 18.09; xent: 2.90; lr: 1.00000; 21654/23722 tok/s;     24 sec\n",
            "[2022-02-19 09:43:42,770 INFO] Step 1520/20000; acc:  48.55; ppl: 20.90; xent: 3.04; lr: 1.00000; 21085/21955 tok/s;     24 sec\n",
            "[2022-02-19 09:43:43,098 INFO] Step 1540/20000; acc:  49.36; ppl: 17.95; xent: 2.89; lr: 1.00000; 20083/20678 tok/s;     25 sec\n",
            "[2022-02-19 09:43:43,370 INFO] Step 1560/20000; acc:  48.58; ppl: 19.94; xent: 2.99; lr: 1.00000; 22877/24093 tok/s;     25 sec\n",
            "[2022-02-19 09:43:43,682 INFO] Step 1580/20000; acc:  47.85; ppl: 21.30; xent: 3.06; lr: 1.00000; 21237/21973 tok/s;     25 sec\n",
            "[2022-02-19 09:43:43,976 INFO] Step 1600/20000; acc:  48.44; ppl: 19.49; xent: 2.97; lr: 1.00000; 23126/22976 tok/s;     26 sec\n",
            "[2022-02-19 09:43:44,368 INFO] Step 1620/20000; acc:  48.29; ppl: 19.24; xent: 2.96; lr: 1.00000; 16701/17209 tok/s;     26 sec\n",
            "[2022-02-19 09:43:44,762 INFO] Step 1640/20000; acc:  48.36; ppl: 20.51; xent: 3.02; lr: 1.00000; 15398/15987 tok/s;     26 sec\n",
            "[2022-02-19 09:43:45,135 INFO] Step 1660/20000; acc:  48.50; ppl: 19.35; xent: 2.96; lr: 1.00000; 19056/18911 tok/s;     27 sec\n",
            "[2022-02-19 09:43:45,486 INFO] Step 1680/20000; acc:  47.59; ppl: 20.48; xent: 3.02; lr: 1.00000; 19561/19598 tok/s;     27 sec\n",
            "[2022-02-19 09:43:45,773 INFO] Step 1700/20000; acc:  49.83; ppl: 18.25; xent: 2.90; lr: 1.00000; 21188/22366 tok/s;     27 sec\n",
            "[2022-02-19 09:43:46,050 INFO] Step 1720/20000; acc:  51.72; ppl: 17.56; xent: 2.87; lr: 1.00000; 22330/22689 tok/s;     28 sec\n",
            "[2022-02-19 09:43:46,399 INFO] Step 1740/20000; acc:  48.00; ppl: 19.35; xent: 2.96; lr: 1.00000; 20839/21689 tok/s;     28 sec\n",
            "[2022-02-19 09:43:46,657 INFO] Step 1760/20000; acc:  51.77; ppl: 15.24; xent: 2.72; lr: 1.00000; 21232/23355 tok/s;     28 sec\n",
            "[2022-02-19 09:43:46,979 INFO] Step 1780/20000; acc:  47.38; ppl: 21.30; xent: 3.06; lr: 1.00000; 21191/21780 tok/s;     29 sec\n",
            "[2022-02-19 09:43:47,304 INFO] Step 1800/20000; acc:  48.79; ppl: 18.78; xent: 2.93; lr: 1.00000; 21308/21290 tok/s;     29 sec\n",
            "[2022-02-19 09:43:47,557 INFO] Step 1820/20000; acc:  53.21; ppl: 14.54; xent: 2.68; lr: 1.00000; 21971/23642 tok/s;     29 sec\n",
            "[2022-02-19 09:43:47,866 INFO] Step 1840/20000; acc:  48.77; ppl: 18.75; xent: 2.93; lr: 1.00000; 20474/21723 tok/s;     30 sec\n",
            "[2022-02-19 09:43:48,101 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-19 09:43:48,195 INFO] Step 1860/20000; acc:  50.35; ppl: 16.98; xent: 2.83; lr: 1.00000; 19559/21075 tok/s;     30 sec\n",
            "[2022-02-19 09:43:48,558 INFO] Validation perplexity: 9.94236\n",
            "[2022-02-19 09:43:48,559 INFO] Validation accuracy: 59.7589\n",
            "[2022-02-19 09:43:48,559 INFO] Model is improving ppl: 12.8995 --> 9.94236.\n",
            "[2022-02-19 09:43:48,559 INFO] Model is improving acc: 55.8646 --> 59.7589.\n",
            "[2022-02-19 09:43:48,609 INFO] Saving checkpoint ./models/nb-steps/model_step_1875.pt\n",
            "[2022-02-19 09:43:48,879 INFO] Step 1880/20000; acc:  50.54; ppl: 16.67; xent: 2.81; lr: 1.00000; 8942/9796 tok/s;     31 sec\n",
            "[2022-02-19 09:43:49,184 INFO] Step 1900/20000; acc:  50.59; ppl: 17.37; xent: 2.85; lr: 1.00000; 21191/22169 tok/s;     31 sec\n",
            "[2022-02-19 09:43:49,500 INFO] Step 1920/20000; acc:  49.36; ppl: 17.73; xent: 2.88; lr: 1.00000; 21296/21650 tok/s;     31 sec\n",
            "[2022-02-19 09:43:49,887 INFO] Step 1940/20000; acc:  50.62; ppl: 16.65; xent: 2.81; lr: 1.00000; 17024/17544 tok/s;     32 sec\n",
            "[2022-02-19 09:43:50,190 INFO] Step 1960/20000; acc:  51.34; ppl: 15.66; xent: 2.75; lr: 1.00000; 19958/21072 tok/s;     32 sec\n",
            "[2022-02-19 09:43:50,495 INFO] Step 1980/20000; acc:  48.94; ppl: 18.48; xent: 2.92; lr: 1.00000; 23302/23876 tok/s;     32 sec\n",
            "[2022-02-19 09:43:50,841 INFO] Step 2000/20000; acc:  49.94; ppl: 17.45; xent: 2.86; lr: 1.00000; 19902/20519 tok/s;     32 sec\n",
            "[2022-02-19 09:43:51,140 INFO] Step 2020/20000; acc:  51.79; ppl: 17.82; xent: 2.88; lr: 1.00000; 20720/21810 tok/s;     33 sec\n",
            "[2022-02-19 09:43:51,412 INFO] Step 2040/20000; acc:  52.42; ppl: 15.28; xent: 2.73; lr: 1.00000; 22876/23310 tok/s;     33 sec\n",
            "[2022-02-19 09:43:51,753 INFO] Step 2060/20000; acc:  50.45; ppl: 16.35; xent: 2.79; lr: 1.00000; 20919/21606 tok/s;     33 sec\n",
            "[2022-02-19 09:43:52,026 INFO] Step 2080/20000; acc:  54.94; ppl: 12.90; xent: 2.56; lr: 1.00000; 19513/21776 tok/s;     34 sec\n",
            "[2022-02-19 09:43:52,333 INFO] Step 2100/20000; acc:  49.33; ppl: 17.59; xent: 2.87; lr: 1.00000; 21255/22643 tok/s;     34 sec\n",
            "[2022-02-19 09:43:52,663 INFO] Step 2120/20000; acc:  51.63; ppl: 15.34; xent: 2.73; lr: 1.00000; 20614/21079 tok/s;     34 sec\n",
            "[2022-02-19 09:43:52,934 INFO] Step 2140/20000; acc:  54.56; ppl: 13.09; xent: 2.57; lr: 1.00000; 20522/22516 tok/s;     35 sec\n",
            "[2022-02-19 09:43:53,237 INFO] Step 2160/20000; acc:  52.09; ppl: 15.43; xent: 2.74; lr: 1.00000; 21228/22428 tok/s;     35 sec\n",
            "[2022-02-19 09:43:53,569 INFO] Step 2180/20000; acc:  52.73; ppl: 14.05; xent: 2.64; lr: 1.00000; 19562/20599 tok/s;     35 sec\n",
            "[2022-02-19 09:43:53,854 INFO] Step 2200/20000; acc:  52.30; ppl: 14.29; xent: 2.66; lr: 1.00000; 22200/23036 tok/s;     36 sec\n",
            "[2022-02-19 09:43:54,210 INFO] Step 2220/20000; acc:  50.42; ppl: 16.51; xent: 2.80; lr: 1.00000; 18904/19244 tok/s;     36 sec\n",
            "[2022-02-19 09:43:54,611 INFO] Step 2240/20000; acc:  50.91; ppl: 15.88; xent: 2.76; lr: 1.00000; 17255/17060 tok/s;     36 sec\n",
            "[2022-02-19 09:43:55,091 INFO] Step 2260/20000; acc:  50.84; ppl: 15.40; xent: 2.73; lr: 1.00000; 13618/14106 tok/s;     37 sec\n",
            "[2022-02-19 09:43:55,379 INFO] Step 2280/20000; acc:  52.88; ppl: 14.46; xent: 2.67; lr: 1.00000; 20874/21794 tok/s;     37 sec\n",
            "[2022-02-19 09:43:55,701 INFO] Step 2300/20000; acc:  52.30; ppl: 13.98; xent: 2.64; lr: 1.00000; 22033/21709 tok/s;     37 sec\n",
            "[2022-02-19 09:43:56,067 INFO] Step 2320/20000; acc:  50.44; ppl: 15.86; xent: 2.76; lr: 1.00000; 18880/19054 tok/s;     38 sec\n",
            "[2022-02-19 09:43:56,360 INFO] Step 2340/20000; acc:  52.29; ppl: 13.63; xent: 2.61; lr: 1.00000; 20800/22273 tok/s;     38 sec\n",
            "[2022-02-19 09:43:56,634 INFO] Step 2360/20000; acc:  52.34; ppl: 14.54; xent: 2.68; lr: 1.00000; 22644/23035 tok/s;     38 sec\n",
            "[2022-02-19 09:43:56,994 INFO] Step 2380/20000; acc:  50.05; ppl: 15.97; xent: 2.77; lr: 1.00000; 20265/20443 tok/s;     39 sec\n",
            "[2022-02-19 09:43:57,258 INFO] Step 2400/20000; acc:  56.02; ppl: 11.47; xent: 2.44; lr: 1.00000; 20426/22361 tok/s;     39 sec\n",
            "[2022-02-19 09:43:57,576 INFO] Step 2420/20000; acc:  51.31; ppl: 15.24; xent: 2.72; lr: 1.00000; 20948/22452 tok/s;     39 sec\n",
            "[2022-02-19 09:43:57,915 INFO] Step 2440/20000; acc:  51.64; ppl: 14.21; xent: 2.65; lr: 1.00000; 20172/20426 tok/s;     40 sec\n",
            "[2022-02-19 09:43:58,203 INFO] Step 2460/20000; acc:  56.65; ppl: 10.87; xent: 2.39; lr: 1.00000; 19283/21185 tok/s;     40 sec\n",
            "[2022-02-19 09:43:58,514 INFO] Step 2480/20000; acc:  51.76; ppl: 14.83; xent: 2.70; lr: 1.00000; 20565/21827 tok/s;     40 sec\n",
            "[2022-02-19 09:43:58,750 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 5\n",
            "[2022-02-19 09:43:58,852 INFO] Step 2500/20000; acc:  53.91; ppl: 12.93; xent: 2.56; lr: 1.00000; 19285/19941 tok/s;     41 sec\n",
            "[2022-02-19 09:43:58,991 INFO] Validation perplexity: 8.20646\n",
            "[2022-02-19 09:43:58,991 INFO] Validation accuracy: 62.5637\n",
            "[2022-02-19 09:43:58,991 INFO] Model is improving ppl: 9.94236 --> 8.20646.\n",
            "[2022-02-19 09:43:58,991 INFO] Model is improving acc: 59.7589 --> 62.5637.\n",
            "[2022-02-19 09:43:59,059 INFO] Saving checkpoint ./models/nb-steps/model_step_2500.pt\n",
            "[2022-02-19 09:43:59,562 INFO] Step 2520/20000; acc:  52.87; ppl: 13.50; xent: 2.60; lr: 1.00000; 8670/9485 tok/s;     41 sec\n",
            "[2022-02-19 09:43:59,881 INFO] Step 2540/20000; acc:  52.45; ppl: 14.31; xent: 2.66; lr: 1.00000; 20468/21212 tok/s;     42 sec\n",
            "[2022-02-19 09:44:00,188 INFO] Step 2560/20000; acc:  52.93; ppl: 13.18; xent: 2.58; lr: 1.00000; 22095/22641 tok/s;     42 sec\n",
            "[2022-02-19 09:44:00,569 INFO] Step 2580/20000; acc:  52.77; ppl: 13.55; xent: 2.61; lr: 1.00000; 17387/17744 tok/s;     42 sec\n",
            "[2022-02-19 09:44:00,863 INFO] Step 2600/20000; acc:  54.25; ppl: 12.53; xent: 2.53; lr: 1.00000; 20618/21700 tok/s;     43 sec\n",
            "[2022-02-19 09:44:01,195 INFO] Step 2620/20000; acc:  51.54; ppl: 14.51; xent: 2.68; lr: 1.00000; 21666/22023 tok/s;     43 sec\n",
            "[2022-02-19 09:44:01,532 INFO] Step 2640/20000; acc:  53.45; ppl: 12.96; xent: 2.56; lr: 1.00000; 20009/21209 tok/s;     43 sec\n",
            "[2022-02-19 09:44:01,814 INFO] Step 2660/20000; acc:  54.55; ppl: 13.43; xent: 2.60; lr: 1.00000; 21563/22548 tok/s;     43 sec\n",
            "[2022-02-19 09:44:02,099 INFO] Step 2680/20000; acc:  55.50; ppl: 11.78; xent: 2.47; lr: 1.00000; 21340/22459 tok/s;     44 sec\n",
            "[2022-02-19 09:44:02,432 INFO] Step 2700/20000; acc:  52.89; ppl: 13.26; xent: 2.58; lr: 1.00000; 21460/21634 tok/s;     44 sec\n",
            "[2022-02-19 09:44:02,694 INFO] Step 2720/20000; acc:  56.92; ppl: 10.27; xent: 2.33; lr: 1.00000; 20425/22873 tok/s;     44 sec\n",
            "[2022-02-19 09:44:03,009 INFO] Step 2740/20000; acc:  53.45; ppl: 13.60; xent: 2.61; lr: 1.00000; 21022/22182 tok/s;     45 sec\n",
            "[2022-02-19 09:44:03,354 INFO] Step 2760/20000; acc:  53.56; ppl: 12.09; xent: 2.49; lr: 1.00000; 19769/20460 tok/s;     45 sec\n",
            "[2022-02-19 09:44:03,620 INFO] Step 2780/20000; acc:  56.92; ppl: 10.38; xent: 2.34; lr: 1.00000; 21054/23514 tok/s;     45 sec\n",
            "[2022-02-19 09:44:03,918 INFO] Step 2800/20000; acc:  54.51; ppl: 12.40; xent: 2.52; lr: 1.00000; 21787/23031 tok/s;     46 sec\n",
            "[2022-02-19 09:44:04,275 INFO] Step 2820/20000; acc:  54.91; ppl: 11.63; xent: 2.45; lr: 1.00000; 18388/18803 tok/s;     46 sec\n",
            "[2022-02-19 09:44:04,675 INFO] Step 2840/20000; acc:  54.74; ppl: 11.15; xent: 2.41; lr: 1.00000; 15677/16345 tok/s;     46 sec\n",
            "[2022-02-19 09:44:05,123 INFO] Step 2860/20000; acc:  54.31; ppl: 12.61; xent: 2.53; lr: 1.00000; 14991/15230 tok/s;     47 sec\n",
            "[2022-02-19 09:44:05,432 INFO] Step 2880/20000; acc:  53.52; ppl: 12.41; xent: 2.52; lr: 1.00000; 22224/21555 tok/s;     47 sec\n",
            "[2022-02-19 09:44:05,817 INFO] Step 2900/20000; acc:  52.99; ppl: 12.97; xent: 2.56; lr: 1.00000; 17294/17903 tok/s;     47 sec\n",
            "[2022-02-19 09:44:06,126 INFO] Step 2920/20000; acc:  56.09; ppl: 11.42; xent: 2.44; lr: 1.00000; 19715/20538 tok/s;     48 sec\n",
            "[2022-02-19 09:44:06,439 INFO] Step 2940/20000; acc:  53.98; ppl: 12.29; xent: 2.51; lr: 1.00000; 23119/22382 tok/s;     48 sec\n",
            "[2022-02-19 09:44:06,789 INFO] Step 2960/20000; acc:  53.90; ppl: 11.56; xent: 2.45; lr: 1.00000; 19242/20260 tok/s;     48 sec\n",
            "[2022-02-19 09:44:07,089 INFO] Step 2980/20000; acc:  54.31; ppl: 11.11; xent: 2.41; lr: 1.00000; 19700/21212 tok/s;     49 sec\n",
            "[2022-02-19 09:44:07,381 INFO] Step 3000/20000; acc:  55.81; ppl: 10.85; xent: 2.38; lr: 1.00000; 20817/21457 tok/s;     49 sec\n",
            "[2022-02-19 09:44:07,724 INFO] Step 3020/20000; acc:  52.61; ppl: 13.04; xent: 2.57; lr: 1.00000; 20983/21146 tok/s;     49 sec\n",
            "[2022-02-19 09:44:07,978 INFO] Step 3040/20000; acc:  58.31; ppl:  9.44; xent: 2.24; lr: 1.00000; 21350/23487 tok/s;     50 sec\n",
            "[2022-02-19 09:44:08,317 INFO] Step 3060/20000; acc:  54.33; ppl: 12.14; xent: 2.50; lr: 1.00000; 19798/20865 tok/s;     50 sec\n",
            "[2022-02-19 09:44:08,506 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 6\n",
            "[2022-02-19 09:44:08,668 INFO] Step 3080/20000; acc:  54.05; ppl: 11.47; xent: 2.44; lr: 1.00000; 19496/20377 tok/s;     50 sec\n",
            "[2022-02-19 09:44:08,944 INFO] Step 3100/20000; acc:  57.81; ppl:  9.50; xent: 2.25; lr: 1.00000; 20282/22196 tok/s;     51 sec\n",
            "[2022-02-19 09:44:09,263 INFO] Step 3120/20000; acc:  53.38; ppl: 12.52; xent: 2.53; lr: 1.00000; 20211/21104 tok/s;     51 sec\n",
            "[2022-02-19 09:44:09,495 INFO] Validation perplexity: 7.22388\n",
            "[2022-02-19 09:44:09,496 INFO] Validation accuracy: 63.7459\n",
            "[2022-02-19 09:44:09,496 INFO] Model is improving ppl: 8.20646 --> 7.22388.\n",
            "[2022-02-19 09:44:09,496 INFO] Model is improving acc: 62.5637 --> 63.7459.\n",
            "[2022-02-19 09:44:09,546 INFO] Saving checkpoint ./models/nb-steps/model_step_3125.pt\n",
            "[2022-02-19 09:44:10,018 INFO] Step 3140/20000; acc:  55.34; ppl: 10.50; xent: 2.35; lr: 1.00000; 8669/8909 tok/s;     52 sec\n",
            "[2022-02-19 09:44:10,338 INFO] Step 3160/20000; acc:  55.75; ppl: 10.48; xent: 2.35; lr: 1.00000; 19135/21136 tok/s;     52 sec\n",
            "[2022-02-19 09:44:10,669 INFO] Step 3180/20000; acc:  56.01; ppl: 11.12; xent: 2.41; lr: 1.00000; 19748/19984 tok/s;     52 sec\n",
            "[2022-02-19 09:44:10,971 INFO] Step 3200/20000; acc:  55.03; ppl: 10.93; xent: 2.39; lr: 1.00000; 22170/22573 tok/s;     53 sec\n",
            "[2022-02-19 09:44:11,360 INFO] Step 3220/20000; acc:  54.51; ppl: 11.56; xent: 2.45; lr: 1.00000; 17253/17587 tok/s;     53 sec\n",
            "[2022-02-19 09:44:11,643 INFO] Step 3240/20000; acc:  55.39; ppl: 11.23; xent: 2.42; lr: 1.00000; 21724/22996 tok/s;     53 sec\n",
            "[2022-02-19 09:44:11,948 INFO] Step 3260/20000; acc:  54.26; ppl: 11.47; xent: 2.44; lr: 1.00000; 23783/23806 tok/s;     54 sec\n",
            "[2022-02-19 09:44:12,279 INFO] Step 3280/20000; acc:  56.16; ppl: 10.54; xent: 2.35; lr: 1.00000; 20066/21505 tok/s;     54 sec\n",
            "[2022-02-19 09:44:12,569 INFO] Step 3300/20000; acc:  56.00; ppl: 11.50; xent: 2.44; lr: 1.00000; 20356/22286 tok/s;     54 sec\n",
            "[2022-02-19 09:44:12,843 INFO] Step 3320/20000; acc:  58.48; ppl:  9.25; xent: 2.23; lr: 1.00000; 21826/22799 tok/s;     54 sec\n",
            "[2022-02-19 09:44:13,180 INFO] Step 3340/20000; acc:  55.17; ppl: 10.92; xent: 2.39; lr: 1.00000; 20990/21426 tok/s;     55 sec\n",
            "[2022-02-19 09:44:13,447 INFO] Step 3360/20000; acc:  59.00; ppl:  8.41; xent: 2.13; lr: 1.00000; 20392/22375 tok/s;     55 sec\n",
            "[2022-02-19 09:44:13,759 INFO] Step 3380/20000; acc:  56.55; ppl: 10.38; xent: 2.34; lr: 1.00000; 21425/22532 tok/s;     55 sec\n",
            "[2022-02-19 09:44:14,106 INFO] Step 3400/20000; acc:  55.19; ppl: 10.68; xent: 2.37; lr: 1.00000; 19877/20520 tok/s;     56 sec\n",
            "[2022-02-19 09:44:14,380 INFO] Step 3420/20000; acc:  58.16; ppl:  8.97; xent: 2.19; lr: 1.00000; 20625/22332 tok/s;     56 sec\n",
            "[2022-02-19 09:44:14,719 INFO] Step 3440/20000; acc:  54.40; ppl: 11.46; xent: 2.44; lr: 1.00000; 19548/20171 tok/s;     56 sec\n",
            "[2022-02-19 09:44:15,066 INFO] Step 3460/20000; acc:  57.87; ppl:  9.06; xent: 2.20; lr: 1.00000; 19031/19561 tok/s;     57 sec\n",
            "[2022-02-19 09:44:15,361 INFO] Step 3480/20000; acc:  57.47; ppl:  9.50; xent: 2.25; lr: 1.00000; 21085/21695 tok/s;     57 sec\n",
            "[2022-02-19 09:44:15,669 INFO] Step 3500/20000; acc:  56.24; ppl: 10.10; xent: 2.31; lr: 1.00000; 21212/21771 tok/s;     57 sec\n",
            "[2022-02-19 09:44:15,965 INFO] Step 3520/20000; acc:  56.74; ppl:  9.71; xent: 2.27; lr: 1.00000; 22970/22601 tok/s;     58 sec\n",
            "[2022-02-19 09:44:16,337 INFO] Step 3540/20000; acc:  54.55; ppl: 11.04; xent: 2.40; lr: 1.00000; 18270/18410 tok/s;     58 sec\n",
            "[2022-02-19 09:44:16,645 INFO] Step 3560/20000; acc:  57.43; ppl:  9.46; xent: 2.25; lr: 1.00000; 20189/20984 tok/s;     58 sec\n",
            "[2022-02-19 09:44:16,944 INFO] Step 3580/20000; acc:  53.74; ppl: 11.37; xent: 2.43; lr: 1.00000; 24546/24084 tok/s;     59 sec\n",
            "[2022-02-19 09:44:17,284 INFO] Step 3600/20000; acc:  56.47; ppl:  9.49; xent: 2.25; lr: 1.00000; 19600/21429 tok/s;     59 sec\n",
            "[2022-02-19 09:44:17,574 INFO] Step 3620/20000; acc:  56.75; ppl:  9.36; xent: 2.24; lr: 1.00000; 20225/22072 tok/s;     59 sec\n",
            "[2022-02-19 09:44:17,851 INFO] Step 3640/20000; acc:  56.95; ppl:  9.35; xent: 2.24; lr: 1.00000; 21847/22161 tok/s;     60 sec\n",
            "[2022-02-19 09:44:18,186 INFO] Step 3660/20000; acc:  54.88; ppl: 10.12; xent: 2.31; lr: 1.00000; 21403/21122 tok/s;     60 sec\n",
            "[2022-02-19 09:44:18,453 INFO] Step 3680/20000; acc:  61.85; ppl:  7.35; xent: 1.99; lr: 1.00000; 20147/21782 tok/s;     60 sec\n",
            "[2022-02-19 09:44:18,770 INFO] Step 3700/20000; acc:  55.45; ppl: 11.14; xent: 2.41; lr: 1.00000; 20948/22692 tok/s;     60 sec\n",
            "[2022-02-19 09:44:18,961 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 7\n",
            "[2022-02-19 09:44:19,133 INFO] Step 3720/20000; acc:  55.30; ppl:  9.90; xent: 2.29; lr: 1.00000; 18815/19836 tok/s;     61 sec\n",
            "[2022-02-19 09:44:19,438 INFO] Step 3740/20000; acc:  59.89; ppl:  8.13; xent: 2.10; lr: 1.00000; 18393/20412 tok/s;     61 sec\n",
            "[2022-02-19 09:44:19,861 INFO] Validation perplexity: 6.69092\n",
            "[2022-02-19 09:44:19,861 INFO] Validation accuracy: 64.8354\n",
            "[2022-02-19 09:44:19,861 INFO] Model is improving ppl: 7.22388 --> 6.69092.\n",
            "[2022-02-19 09:44:19,862 INFO] Model is improving acc: 63.7459 --> 64.8354.\n",
            "[2022-02-19 09:44:19,928 INFO] Saving checkpoint ./models/nb-steps/model_step_3750.pt\n",
            "[2022-02-19 09:44:20,270 INFO] Step 3760/20000; acc:  56.00; ppl: 10.24; xent: 2.33; lr: 1.00000; 7710/8109 tok/s;     62 sec\n",
            "[2022-02-19 09:44:20,600 INFO] Step 3780/20000; acc:  56.73; ppl:  9.45; xent: 2.25; lr: 1.00000; 19996/20211 tok/s;     62 sec\n",
            "[2022-02-19 09:44:20,894 INFO] Step 3800/20000; acc:  57.37; ppl:  8.55; xent: 2.15; lr: 1.00000; 21178/22484 tok/s;     63 sec\n",
            "[2022-02-19 09:44:21,216 INFO] Step 3820/20000; acc:  58.33; ppl:  9.14; xent: 2.21; lr: 1.00000; 20428/21028 tok/s;     63 sec\n",
            "[2022-02-19 09:44:21,521 INFO] Step 3840/20000; acc:  56.20; ppl:  9.71; xent: 2.27; lr: 1.00000; 22263/22551 tok/s;     63 sec\n",
            "[2022-02-19 09:44:21,924 INFO] Step 3860/20000; acc:  56.06; ppl:  9.87; xent: 2.29; lr: 1.00000; 16604/17174 tok/s;     64 sec\n",
            "[2022-02-19 09:44:22,227 INFO] Step 3880/20000; acc:  56.80; ppl: 10.15; xent: 2.32; lr: 1.00000; 20315/21385 tok/s;     64 sec\n",
            "[2022-02-19 09:44:22,537 INFO] Step 3900/20000; acc:  56.37; ppl:  9.82; xent: 2.28; lr: 1.00000; 23338/23413 tok/s;     64 sec\n",
            "[2022-02-19 09:44:22,873 INFO] Step 3920/20000; acc:  57.00; ppl:  8.97; xent: 2.19; lr: 1.00000; 19581/20356 tok/s;     65 sec\n",
            "[2022-02-19 09:44:23,160 INFO] Step 3940/20000; acc:  59.25; ppl:  8.60; xent: 2.15; lr: 1.00000; 20080/22400 tok/s;     65 sec\n",
            "[2022-02-19 09:44:23,440 INFO] Step 3960/20000; acc:  59.75; ppl:  7.85; xent: 2.06; lr: 1.00000; 20723/22586 tok/s;     65 sec\n",
            "[2022-02-19 09:44:23,790 INFO] Step 3980/20000; acc:  56.64; ppl:  9.19; xent: 2.22; lr: 1.00000; 20136/20984 tok/s;     65 sec\n",
            "[2022-02-19 09:44:24,069 INFO] Step 4000/20000; acc:  61.62; ppl:  6.94; xent: 1.94; lr: 1.00000; 19607/21638 tok/s;     66 sec\n",
            "[2022-02-19 09:44:24,390 INFO] Step 4020/20000; acc:  56.85; ppl:  9.56; xent: 2.26; lr: 1.00000; 20985/21549 tok/s;     66 sec\n",
            "[2022-02-19 09:44:24,743 INFO] Step 4040/20000; acc:  57.51; ppl:  8.87; xent: 2.18; lr: 1.00000; 19705/20487 tok/s;     66 sec\n",
            "[2022-02-19 09:44:25,005 INFO] Step 4060/20000; acc:  60.47; ppl:  7.57; xent: 2.02; lr: 1.00000; 21934/22919 tok/s;     67 sec\n",
            "[2022-02-19 09:44:25,326 INFO] Step 4080/20000; acc:  57.08; ppl:  9.51; xent: 2.25; lr: 1.00000; 20945/21547 tok/s;     67 sec\n",
            "[2022-02-19 09:44:25,662 INFO] Step 4100/20000; acc:  58.72; ppl:  8.00; xent: 2.08; lr: 1.00000; 19770/20023 tok/s;     67 sec\n",
            "[2022-02-19 09:44:26,094 INFO] Step 4120/20000; acc:  58.40; ppl:  8.53; xent: 2.14; lr: 1.00000; 14481/14893 tok/s;     68 sec\n",
            "[2022-02-19 09:44:26,455 INFO] Step 4140/20000; acc:  58.16; ppl:  8.87; xent: 2.18; lr: 1.00000; 18314/18783 tok/s;     68 sec\n",
            "[2022-02-19 09:44:26,775 INFO] Step 4160/20000; acc:  57.44; ppl:  8.71; xent: 2.16; lr: 1.00000; 21283/21033 tok/s;     68 sec\n",
            "[2022-02-19 09:44:27,157 INFO] Step 4180/20000; acc:  57.50; ppl:  8.89; xent: 2.19; lr: 1.00000; 17472/17831 tok/s;     69 sec\n",
            "[2022-02-19 09:44:27,439 INFO] Step 4200/20000; acc:  58.89; ppl:  8.34; xent: 2.12; lr: 1.00000; 21547/22528 tok/s;     69 sec\n",
            "[2022-02-19 09:44:27,761 INFO] Step 4220/20000; acc:  55.82; ppl:  9.74; xent: 2.28; lr: 1.00000; 22537/22282 tok/s;     69 sec\n",
            "[2022-02-19 09:44:28,088 INFO] Step 4240/20000; acc:  58.34; ppl:  8.14; xent: 2.10; lr: 1.00000; 20386/21953 tok/s;     70 sec\n",
            "[2022-02-19 09:44:28,371 INFO] Step 4260/20000; acc:  59.48; ppl:  8.11; xent: 2.09; lr: 1.00000; 20829/22497 tok/s;     70 sec\n",
            "[2022-02-19 09:44:28,656 INFO] Step 4280/20000; acc:  59.47; ppl:  7.71; xent: 2.04; lr: 1.00000; 21066/21908 tok/s;     70 sec\n",
            "[2022-02-19 09:44:29,003 INFO] Step 4300/20000; acc:  57.38; ppl:  8.49; xent: 2.14; lr: 1.00000; 20874/20719 tok/s;     71 sec\n",
            "[2022-02-19 09:44:29,271 INFO] Step 4320/20000; acc:  62.80; ppl:  6.48; xent: 1.87; lr: 1.00000; 20024/21601 tok/s;     71 sec\n",
            "[2022-02-19 09:44:29,587 INFO] Step 4340/20000; acc:  57.22; ppl:  9.36; xent: 2.24; lr: 1.00000; 20726/22318 tok/s;     71 sec\n",
            "[2022-02-19 09:44:29,784 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 8\n",
            "[2022-02-19 09:44:29,974 INFO] Step 4360/20000; acc:  57.66; ppl:  8.48; xent: 2.14; lr: 1.00000; 17728/18634 tok/s;     72 sec\n",
            "[2022-02-19 09:44:30,328 INFO] Validation perplexity: 6.19067\n",
            "[2022-02-19 09:44:30,328 INFO] Validation accuracy: 66.458\n",
            "[2022-02-19 09:44:30,328 INFO] Model is improving ppl: 6.69092 --> 6.19067.\n",
            "[2022-02-19 09:44:30,329 INFO] Model is improving acc: 64.8354 --> 66.458.\n",
            "[2022-02-19 09:44:30,384 INFO] Saving checkpoint ./models/nb-steps/model_step_4375.pt\n",
            "[2022-02-19 09:44:30,644 INFO] Step 4380/20000; acc:  61.48; ppl:  7.11; xent: 1.96; lr: 1.00000; 8411/9405 tok/s;     72 sec\n",
            "[2022-02-19 09:44:30,959 INFO] Step 4400/20000; acc:  57.19; ppl:  9.08; xent: 2.21; lr: 1.00000; 20722/21477 tok/s;     73 sec\n",
            "[2022-02-19 09:44:31,287 INFO] Step 4420/20000; acc:  59.02; ppl:  7.81; xent: 2.05; lr: 1.00000; 20302/20732 tok/s;     73 sec\n",
            "[2022-02-19 09:44:31,571 INFO] Step 4440/20000; acc:  59.32; ppl:  7.69; xent: 2.04; lr: 1.00000; 22204/22752 tok/s;     73 sec\n",
            "[2022-02-19 09:44:31,898 INFO] Step 4460/20000; acc:  58.86; ppl:  8.38; xent: 2.13; lr: 1.00000; 20159/21155 tok/s;     74 sec\n",
            "[2022-02-19 09:44:32,193 INFO] Step 4480/20000; acc:  57.01; ppl:  8.67; xent: 2.16; lr: 1.00000; 23156/23791 tok/s;     74 sec\n",
            "[2022-02-19 09:44:32,580 INFO] Step 4500/20000; acc:  58.08; ppl:  8.51; xent: 2.14; lr: 1.00000; 17287/18000 tok/s;     74 sec\n",
            "[2022-02-19 09:44:32,874 INFO] Step 4520/20000; acc:  59.10; ppl:  8.65; xent: 2.16; lr: 1.00000; 20949/22324 tok/s;     75 sec\n",
            "[2022-02-19 09:44:33,179 INFO] Step 4540/20000; acc:  57.81; ppl:  8.57; xent: 2.15; lr: 1.00000; 23780/23475 tok/s;     75 sec\n",
            "[2022-02-19 09:44:33,503 INFO] Step 4560/20000; acc:  59.57; ppl:  7.66; xent: 2.04; lr: 1.00000; 20069/21140 tok/s;     75 sec\n",
            "[2022-02-19 09:44:33,787 INFO] Step 4580/20000; acc:  62.12; ppl:  6.92; xent: 1.93; lr: 1.00000; 20024/22154 tok/s;     75 sec\n",
            "[2022-02-19 09:44:34,051 INFO] Step 4600/20000; acc:  62.02; ppl:  6.67; xent: 1.90; lr: 1.00000; 21941/23801 tok/s;     76 sec\n",
            "[2022-02-19 09:44:34,395 INFO] Step 4620/20000; acc:  58.74; ppl:  8.09; xent: 2.09; lr: 1.00000; 20451/20876 tok/s;     76 sec\n",
            "[2022-02-19 09:44:34,656 INFO] Step 4640/20000; acc:  63.88; ppl:  6.14; xent: 1.81; lr: 1.00000; 20671/22473 tok/s;     76 sec\n",
            "[2022-02-19 09:44:34,987 INFO] Step 4660/20000; acc:  58.29; ppl:  8.14; xent: 2.10; lr: 1.00000; 20258/21460 tok/s;     77 sec\n",
            "[2022-02-19 09:44:35,328 INFO] Step 4680/20000; acc:  59.28; ppl:  7.70; xent: 2.04; lr: 1.00000; 20415/20837 tok/s;     77 sec\n",
            "[2022-02-19 09:44:35,603 INFO] Step 4700/20000; acc:  62.07; ppl:  6.49; xent: 1.87; lr: 1.00000; 21091/22707 tok/s;     77 sec\n",
            "[2022-02-19 09:44:35,921 INFO] Step 4720/20000; acc:  57.03; ppl:  9.21; xent: 2.22; lr: 1.00000; 21513/21727 tok/s;     78 sec\n",
            "[2022-02-19 09:44:36,242 INFO] Step 4740/20000; acc:  60.21; ppl:  7.26; xent: 1.98; lr: 1.00000; 21019/21031 tok/s;     78 sec\n",
            "[2022-02-19 09:44:36,529 INFO] Step 4760/20000; acc:  59.23; ppl:  7.48; xent: 2.01; lr: 1.00000; 21789/22538 tok/s;     78 sec\n",
            "[2022-02-19 09:44:36,818 INFO] Step 4780/20000; acc:  59.53; ppl:  7.99; xent: 2.08; lr: 1.00000; 22761/23223 tok/s;     78 sec\n",
            "[2022-02-19 09:44:37,136 INFO] Step 4800/20000; acc:  60.07; ppl:  7.19; xent: 1.97; lr: 1.00000; 21424/21242 tok/s;     79 sec\n",
            "[2022-02-19 09:44:37,535 INFO] Step 4820/20000; acc:  58.79; ppl:  7.80; xent: 2.05; lr: 1.00000; 16815/17291 tok/s;     79 sec\n",
            "[2022-02-19 09:44:37,833 INFO] Step 4840/20000; acc:  60.15; ppl:  7.22; xent: 1.98; lr: 1.00000; 20288/21619 tok/s;     79 sec\n",
            "[2022-02-19 09:44:38,151 INFO] Step 4860/20000; acc:  56.32; ppl:  8.83; xent: 2.18; lr: 1.00000; 22991/22647 tok/s;     80 sec\n",
            "[2022-02-19 09:44:38,476 INFO] Step 4880/20000; acc:  59.05; ppl:  7.59; xent: 2.03; lr: 1.00000; 20481/21227 tok/s;     80 sec\n",
            "[2022-02-19 09:44:38,749 INFO] Step 4900/20000; acc:  61.22; ppl:  7.22; xent: 1.98; lr: 1.00000; 21204/23002 tok/s;     80 sec\n",
            "[2022-02-19 09:44:39,040 INFO] Step 4920/20000; acc:  62.31; ppl:  6.25; xent: 1.83; lr: 1.00000; 20098/22077 tok/s;     81 sec\n",
            "[2022-02-19 09:44:39,415 INFO] Step 4940/20000; acc:  58.72; ppl:  7.55; xent: 2.02; lr: 1.00000; 18931/18960 tok/s;     81 sec\n",
            "[2022-02-19 09:44:39,792 INFO] Step 4960/20000; acc:  64.39; ppl:  5.80; xent: 1.76; lr: 1.00000; 14291/15686 tok/s;     81 sec\n",
            "[2022-02-19 09:44:40,158 INFO] Step 4980/20000; acc:  59.60; ppl:  7.85; xent: 2.06; lr: 1.00000; 18107/18988 tok/s;     82 sec\n",
            "[2022-02-19 09:44:40,331 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 9\n",
            "[2022-02-19 09:44:40,500 INFO] Step 5000/20000; acc:  58.39; ppl:  7.58; xent: 2.03; lr: 1.00000; 20087/21141 tok/s;     82 sec\n",
            "[2022-02-19 09:44:40,631 INFO] Validation perplexity: 5.81202\n",
            "[2022-02-19 09:44:40,632 INFO] Validation accuracy: 67.5707\n",
            "[2022-02-19 09:44:40,632 INFO] Model is improving ppl: 6.19067 --> 5.81202.\n",
            "[2022-02-19 09:44:40,632 INFO] Model is improving acc: 66.458 --> 67.5707.\n",
            "[2022-02-19 09:44:40,682 INFO] Saving checkpoint ./models/nb-steps/model_step_5000.pt\n",
            "[2022-02-19 09:44:41,229 INFO] Step 5020/20000; acc:  62.53; ppl:  6.29; xent: 1.84; lr: 1.00000; 7802/8438 tok/s;     83 sec\n",
            "[2022-02-19 09:44:41,551 INFO] Step 5040/20000; acc:  58.44; ppl:  8.08; xent: 2.09; lr: 1.00000; 20534/21656 tok/s;     83 sec\n",
            "[2022-02-19 09:44:41,876 INFO] Step 5060/20000; acc:  59.41; ppl:  7.26; xent: 1.98; lr: 1.00000; 20609/21029 tok/s;     84 sec\n",
            "[2022-02-19 09:44:42,173 INFO] Step 5080/20000; acc:  61.33; ppl:  6.70; xent: 1.90; lr: 1.00000; 21315/21757 tok/s;     84 sec\n",
            "[2022-02-19 09:44:42,470 INFO] Step 5100/20000; acc:  59.11; ppl:  7.73; xent: 2.05; lr: 1.00000; 22280/23033 tok/s;     84 sec\n",
            "[2022-02-19 09:44:42,759 INFO] Step 5120/20000; acc:  59.31; ppl:  7.66; xent: 2.04; lr: 1.00000; 23717/24427 tok/s;     84 sec\n",
            "[2022-02-19 09:44:43,158 INFO] Step 5140/20000; acc:  60.05; ppl:  7.15; xent: 1.97; lr: 1.00000; 16409/17527 tok/s;     85 sec\n",
            "[2022-02-19 09:44:43,436 INFO] Step 5160/20000; acc:  61.09; ppl:  7.20; xent: 1.97; lr: 1.00000; 21706/22677 tok/s;     85 sec\n",
            "[2022-02-19 09:44:43,740 INFO] Step 5180/20000; acc:  58.72; ppl:  7.49; xent: 2.01; lr: 1.00000; 23351/23824 tok/s;     85 sec\n",
            "[2022-02-19 09:44:44,059 INFO] Step 5200/20000; acc:  60.78; ppl:  6.85; xent: 1.92; lr: 1.00000; 20557/21276 tok/s;     86 sec\n",
            "[2022-02-19 09:44:44,341 INFO] Step 5220/20000; acc:  62.03; ppl:  6.75; xent: 1.91; lr: 1.00000; 20447/22656 tok/s;     86 sec\n",
            "[2022-02-19 09:44:44,615 INFO] Step 5240/20000; acc:  64.03; ppl:  5.70; xent: 1.74; lr: 1.00000; 21313/22974 tok/s;     86 sec\n",
            "[2022-02-19 09:44:44,956 INFO] Step 5260/20000; acc:  58.48; ppl:  7.50; xent: 2.02; lr: 1.00000; 20734/21330 tok/s;     87 sec\n",
            "[2022-02-19 09:44:45,220 INFO] Step 5280/20000; acc:  65.28; ppl:  5.28; xent: 1.66; lr: 1.00000; 20502/22717 tok/s;     87 sec\n",
            "[2022-02-19 09:44:45,530 INFO] Step 5300/20000; acc:  59.94; ppl:  7.53; xent: 2.02; lr: 1.00000; 21768/22934 tok/s;     87 sec\n",
            "[2022-02-19 09:44:45,863 INFO] Step 5320/20000; acc:  59.94; ppl:  7.07; xent: 1.96; lr: 1.00000; 21050/20978 tok/s;     88 sec\n",
            "[2022-02-19 09:44:46,134 INFO] Step 5340/20000; acc:  64.73; ppl:  5.39; xent: 1.69; lr: 1.00000; 21220/22924 tok/s;     88 sec\n",
            "[2022-02-19 09:44:46,430 INFO] Step 5360/20000; acc:  58.33; ppl:  8.06; xent: 2.09; lr: 1.00000; 23000/23190 tok/s;     88 sec\n",
            "[2022-02-19 09:44:46,757 INFO] Step 5380/20000; acc:  61.59; ppl:  6.30; xent: 1.84; lr: 1.00000; 20586/20289 tok/s;     88 sec\n",
            "[2022-02-19 09:44:47,041 INFO] Step 5400/20000; acc:  60.09; ppl:  7.02; xent: 1.95; lr: 1.00000; 22359/23169 tok/s;     89 sec\n",
            "[2022-02-19 09:44:47,360 INFO] Step 5420/20000; acc:  60.31; ppl:  7.24; xent: 1.98; lr: 1.00000; 20810/21284 tok/s;     89 sec\n",
            "[2022-02-19 09:44:47,645 INFO] Step 5440/20000; acc:  60.09; ppl:  6.94; xent: 1.94; lr: 1.00000; 24324/23628 tok/s;     89 sec\n",
            "[2022-02-19 09:44:48,017 INFO] Step 5460/20000; acc:  59.40; ppl:  6.94; xent: 1.94; lr: 1.00000; 17508/18782 tok/s;     90 sec\n",
            "[2022-02-19 09:44:48,306 INFO] Step 5480/20000; acc:  61.93; ppl:  6.22; xent: 1.83; lr: 1.00000; 20487/21492 tok/s;     90 sec\n",
            "[2022-02-19 09:44:48,609 INFO] Step 5500/20000; acc:  59.58; ppl:  7.35; xent: 1.99; lr: 1.00000; 23394/23834 tok/s;     90 sec\n",
            "[2022-02-19 09:44:48,927 INFO] Step 5520/20000; acc:  60.90; ppl:  6.51; xent: 1.87; lr: 1.00000; 20852/21323 tok/s;     91 sec\n",
            "[2022-02-19 09:44:49,209 INFO] Step 5540/20000; acc:  62.32; ppl:  6.44; xent: 1.86; lr: 1.00000; 20718/22501 tok/s;     91 sec\n",
            "[2022-02-19 09:44:49,555 INFO] Step 5560/20000; acc:  63.93; ppl:  5.54; xent: 1.71; lr: 1.00000; 17026/18432 tok/s;     91 sec\n",
            "[2022-02-19 09:44:49,736 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 10\n",
            "[2022-02-19 09:44:50,021 INFO] Step 5580/20000; acc:  58.69; ppl:  7.49; xent: 2.01; lr: 1.00000; 15234/15812 tok/s;     92 sec\n",
            "[2022-02-19 09:44:50,290 INFO] Step 5600/20000; acc:  65.31; ppl:  4.97; xent: 1.60; lr: 1.00000; 20094/21628 tok/s;     92 sec\n",
            "[2022-02-19 09:44:50,601 INFO] Step 5620/20000; acc:  59.67; ppl:  7.44; xent: 2.01; lr: 1.00000; 21573/22603 tok/s;     92 sec\n",
            "[2022-02-19 09:44:50,806 INFO] Validation perplexity: 5.66691\n",
            "[2022-02-19 09:44:50,806 INFO] Validation accuracy: 68.6602\n",
            "[2022-02-19 09:44:50,807 INFO] Model is improving ppl: 5.81202 --> 5.66691.\n",
            "[2022-02-19 09:44:50,807 INFO] Model is improving acc: 67.5707 --> 68.6602.\n",
            "[2022-02-19 09:44:50,857 INFO] Saving checkpoint ./models/nb-steps/model_step_5625.pt\n",
            "[2022-02-19 09:44:51,373 INFO] Step 5640/20000; acc:  59.72; ppl:  6.87; xent: 1.93; lr: 1.00000; 8929/9369 tok/s;     93 sec\n",
            "[2022-02-19 09:44:51,674 INFO] Step 5660/20000; acc:  64.25; ppl:  5.51; xent: 1.71; lr: 1.00000; 18904/20368 tok/s;     93 sec\n",
            "[2022-02-19 09:44:52,002 INFO] Step 5680/20000; acc:  60.61; ppl:  7.05; xent: 1.95; lr: 1.00000; 20102/20587 tok/s;     94 sec\n",
            "[2022-02-19 09:44:52,343 INFO] Step 5700/20000; acc:  61.56; ppl:  6.30; xent: 1.84; lr: 1.00000; 19475/19886 tok/s;     94 sec\n",
            "[2022-02-19 09:44:52,638 INFO] Step 5720/20000; acc:  61.67; ppl:  6.66; xent: 1.90; lr: 1.00000; 21716/22275 tok/s;     94 sec\n",
            "[2022-02-19 09:44:52,936 INFO] Step 5740/20000; acc:  59.80; ppl:  7.17; xent: 1.97; lr: 1.00000; 22597/23378 tok/s;     95 sec\n",
            "[2022-02-19 09:44:53,240 INFO] Step 5760/20000; acc:  60.83; ppl:  6.65; xent: 1.89; lr: 1.00000; 22769/23101 tok/s;     95 sec\n",
            "[2022-02-19 09:44:53,611 INFO] Step 5780/20000; acc:  62.11; ppl:  6.09; xent: 1.81; lr: 1.00000; 17292/19029 tok/s;     95 sec\n",
            "[2022-02-19 09:44:53,897 INFO] Step 5800/20000; acc:  62.38; ppl:  6.67; xent: 1.90; lr: 1.00000; 20690/21712 tok/s;     96 sec\n",
            "[2022-02-19 09:44:54,186 INFO] Step 5820/20000; acc:  61.01; ppl:  6.45; xent: 1.86; lr: 1.00000; 24007/24827 tok/s;     96 sec\n",
            "[2022-02-19 09:44:54,520 INFO] Step 5840/20000; acc:  62.02; ppl:  6.29; xent: 1.84; lr: 1.00000; 19677/20376 tok/s;     96 sec\n",
            "[2022-02-19 09:44:54,802 INFO] Step 5860/20000; acc:  63.75; ppl:  5.95; xent: 1.78; lr: 1.00000; 20621/22536 tok/s;     96 sec\n",
            "[2022-02-19 09:44:55,079 INFO] Step 5880/20000; acc:  66.49; ppl:  4.85; xent: 1.58; lr: 1.00000; 21164/22638 tok/s;     97 sec\n",
            "[2022-02-19 09:44:55,433 INFO] Step 5900/20000; acc:  59.72; ppl:  7.01; xent: 1.95; lr: 1.00000; 20169/20766 tok/s;     97 sec\n",
            "[2022-02-19 09:44:55,685 INFO] Step 5920/20000; acc:  65.90; ppl:  4.75; xent: 1.56; lr: 1.00000; 21758/23810 tok/s;     97 sec\n",
            "[2022-02-19 09:44:56,006 INFO] Step 5940/20000; acc:  59.71; ppl:  7.27; xent: 1.98; lr: 1.00000; 21516/21865 tok/s;     98 sec\n",
            "[2022-02-19 09:44:56,351 INFO] Step 5960/20000; acc:  61.32; ppl:  6.40; xent: 1.86; lr: 1.00000; 20376/20350 tok/s;     98 sec\n",
            "[2022-02-19 09:44:56,607 INFO] Step 5980/20000; acc:  65.89; ppl:  4.80; xent: 1.57; lr: 1.00000; 22277/23742 tok/s;     98 sec\n",
            "[2022-02-19 09:44:56,908 INFO] Step 6000/20000; acc:  60.20; ppl:  6.80; xent: 1.92; lr: 1.00000; 22162/22428 tok/s;     99 sec\n",
            "[2022-02-19 09:44:57,245 INFO] Step 6020/20000; acc:  63.00; ppl:  5.68; xent: 1.74; lr: 1.00000; 19794/19919 tok/s;     99 sec\n",
            "[2022-02-19 09:44:57,540 INFO] Step 6040/20000; acc:  61.01; ppl:  6.58; xent: 1.88; lr: 1.00000; 21987/22205 tok/s;     99 sec\n",
            "[2022-02-19 09:44:57,857 INFO] Step 6060/20000; acc:  61.79; ppl:  6.43; xent: 1.86; lr: 1.00000; 21255/21760 tok/s;    100 sec\n",
            "[2022-02-19 09:44:58,143 INFO] Step 6080/20000; acc:  60.51; ppl:  6.73; xent: 1.91; lr: 1.00000; 24499/24356 tok/s;    100 sec\n",
            "[2022-02-19 09:44:58,532 INFO] Step 6100/20000; acc:  61.36; ppl:  6.01; xent: 1.79; lr: 1.00000; 16518/18275 tok/s;    100 sec\n",
            "[2022-02-19 09:44:58,802 INFO] Step 6120/20000; acc:  63.75; ppl:  5.54; xent: 1.71; lr: 1.00000; 21878/23238 tok/s;    100 sec\n",
            "[2022-02-19 09:44:59,107 INFO] Step 6140/20000; acc:  59.28; ppl:  6.72; xent: 1.90; lr: 1.00000; 23149/23189 tok/s;    101 sec\n",
            "[2022-02-19 09:44:59,484 INFO] Step 6160/20000; acc:  61.76; ppl:  6.12; xent: 1.81; lr: 1.00000; 17630/17415 tok/s;    101 sec\n",
            "[2022-02-19 09:44:59,859 INFO] Step 6180/20000; acc:  63.63; ppl:  5.77; xent: 1.75; lr: 1.00000; 15439/16550 tok/s;    102 sec\n",
            "[2022-02-19 09:45:00,196 INFO] Step 6200/20000; acc:  64.50; ppl:  5.21; xent: 1.65; lr: 1.00000; 17127/19534 tok/s;    102 sec\n",
            "[2022-02-19 09:45:00,327 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 11\n",
            "[2022-02-19 09:45:00,555 INFO] Step 6220/20000; acc:  60.26; ppl:  6.69; xent: 1.90; lr: 1.00000; 19775/20636 tok/s;    102 sec\n",
            "[2022-02-19 09:45:00,823 INFO] Step 6240/20000; acc:  66.33; ppl:  4.74; xent: 1.56; lr: 1.00000; 20250/22362 tok/s;    102 sec\n",
            "[2022-02-19 09:45:01,139 INFO] Validation perplexity: 5.55058\n",
            "[2022-02-19 09:45:01,140 INFO] Validation accuracy: 68.4516\n",
            "[2022-02-19 09:45:01,140 INFO] Stalled patience: 1/2\n",
            "[2022-02-19 09:45:01,193 INFO] Saving checkpoint ./models/nb-steps/model_step_6250.pt\n",
            "[2022-02-19 09:45:01,539 INFO] Step 6260/20000; acc:  59.44; ppl:  7.03; xent: 1.95; lr: 1.00000; 9340/9715 tok/s;    103 sec\n",
            "[2022-02-19 09:45:01,896 INFO] Step 6280/20000; acc:  61.44; ppl:  6.08; xent: 1.81; lr: 1.00000; 19567/19508 tok/s;    104 sec\n",
            "[2022-02-19 09:45:02,169 INFO] Step 6300/20000; acc:  64.87; ppl:  4.99; xent: 1.61; lr: 1.00000; 21045/22697 tok/s;    104 sec\n",
            "[2022-02-19 09:45:02,496 INFO] Step 6320/20000; acc:  61.33; ppl:  6.63; xent: 1.89; lr: 1.00000; 20385/21141 tok/s;    104 sec\n",
            "[2022-02-19 09:45:02,837 INFO] Step 6340/20000; acc:  62.36; ppl:  5.59; xent: 1.72; lr: 1.00000; 19647/20137 tok/s;    104 sec\n",
            "[2022-02-19 09:45:03,141 INFO] Step 6360/20000; acc:  62.33; ppl:  6.03; xent: 1.80; lr: 1.00000; 21047/21700 tok/s;    105 sec\n",
            "[2022-02-19 09:45:03,452 INFO] Step 6380/20000; acc:  60.74; ppl:  6.83; xent: 1.92; lr: 1.00000; 21626/22275 tok/s;    105 sec\n",
            "[2022-02-19 09:45:03,745 INFO] Step 6400/20000; acc:  62.10; ppl:  6.16; xent: 1.82; lr: 1.00000; 23643/24036 tok/s;    105 sec\n",
            "[2022-02-19 09:45:04,106 INFO] Step 6420/20000; acc:  62.67; ppl:  5.58; xent: 1.72; lr: 1.00000; 17566/18383 tok/s;    106 sec\n",
            "[2022-02-19 09:45:04,379 INFO] Step 6440/20000; acc:  65.12; ppl:  5.52; xent: 1.71; lr: 1.00000; 20948/23225 tok/s;    106 sec\n",
            "[2022-02-19 09:45:04,689 INFO] Step 6460/20000; acc:  62.65; ppl:  5.69; xent: 1.74; lr: 1.00000; 21900/22957 tok/s;    106 sec\n",
            "[2022-02-19 09:45:05,014 INFO] Step 6480/20000; acc:  61.70; ppl:  5.90; xent: 1.78; lr: 1.00000; 20150/21757 tok/s;    107 sec\n",
            "[2022-02-19 09:45:05,326 INFO] Step 6500/20000; acc:  63.64; ppl:  5.45; xent: 1.69; lr: 1.00000; 18777/20220 tok/s;    107 sec\n",
            "[2022-02-19 09:45:05,602 INFO] Step 6520/20000; acc:  66.27; ppl:  4.73; xent: 1.55; lr: 1.00000; 21583/22713 tok/s;    107 sec\n",
            "[2022-02-19 09:45:05,942 INFO] Step 6540/20000; acc:  61.62; ppl:  6.26; xent: 1.83; lr: 1.00000; 21112/21870 tok/s;    108 sec\n",
            "[2022-02-19 09:45:06,209 INFO] Step 6560/20000; acc:  67.52; ppl:  4.24; xent: 1.44; lr: 1.00000; 20856/21857 tok/s;    108 sec\n",
            "[2022-02-19 09:45:06,535 INFO] Step 6580/20000; acc:  60.71; ppl:  6.84; xent: 1.92; lr: 1.00000; 21495/21621 tok/s;    108 sec\n",
            "[2022-02-19 09:45:06,887 INFO] Step 6600/20000; acc:  62.42; ppl:  5.72; xent: 1.74; lr: 1.00000; 20020/19971 tok/s;    109 sec\n",
            "[2022-02-19 09:45:07,161 INFO] Step 6620/20000; acc:  65.53; ppl:  4.76; xent: 1.56; lr: 1.00000; 20943/22526 tok/s;    109 sec\n",
            "[2022-02-19 09:45:07,469 INFO] Step 6640/20000; acc:  61.13; ppl:  6.65; xent: 1.89; lr: 1.00000; 21853/22005 tok/s;    109 sec\n",
            "[2022-02-19 09:45:07,809 INFO] Step 6660/20000; acc:  63.81; ppl:  5.17; xent: 1.64; lr: 1.00000; 19669/19522 tok/s;    109 sec\n",
            "[2022-02-19 09:45:08,089 INFO] Step 6680/20000; acc:  63.41; ppl:  5.60; xent: 1.72; lr: 1.00000; 22625/23366 tok/s;    110 sec\n",
            "[2022-02-19 09:45:08,408 INFO] Step 6700/20000; acc:  63.16; ppl:  5.73; xent: 1.75; lr: 1.00000; 20733/21563 tok/s;    110 sec\n",
            "[2022-02-19 09:45:08,698 INFO] Step 6720/20000; acc:  60.83; ppl:  6.46; xent: 1.87; lr: 1.00000; 23906/23479 tok/s;    110 sec\n",
            "[2022-02-19 09:45:09,070 INFO] Step 6740/20000; acc:  62.33; ppl:  5.80; xent: 1.76; lr: 1.00000; 17354/19357 tok/s;    111 sec\n",
            "[2022-02-19 09:45:09,348 INFO] Step 6760/20000; acc:  64.38; ppl:  5.33; xent: 1.67; lr: 1.00000; 21310/22591 tok/s;    111 sec\n",
            "[2022-02-19 09:45:09,774 INFO] Step 6780/20000; acc:  62.25; ppl:  5.92; xent: 1.78; lr: 1.00000; 16519/16758 tok/s;    111 sec\n",
            "[2022-02-19 09:45:10,173 INFO] Step 6800/20000; acc:  64.53; ppl:  5.27; xent: 1.66; lr: 1.00000; 16590/16479 tok/s;    112 sec\n",
            "[2022-02-19 09:45:10,460 INFO] Step 6820/20000; acc:  66.41; ppl:  5.01; xent: 1.61; lr: 1.00000; 19863/21492 tok/s;    112 sec\n",
            "[2022-02-19 09:45:10,756 INFO] Step 6840/20000; acc:  65.58; ppl:  4.68; xent: 1.54; lr: 1.00000; 19596/21805 tok/s;    112 sec\n",
            "[2022-02-19 09:45:10,890 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 12\n",
            "[2022-02-19 09:45:11,121 INFO] Step 6860/20000; acc:  61.01; ppl:  6.17; xent: 1.82; lr: 1.00000; 19569/20348 tok/s;    113 sec\n",
            "[2022-02-19 09:45:11,466 INFO] Validation perplexity: 5.49383\n",
            "[2022-02-19 09:45:11,466 INFO] Validation accuracy: 68.7761\n",
            "[2022-02-19 09:45:11,467 INFO] Model is improving ppl: 5.66691 --> 5.49383.\n",
            "[2022-02-19 09:45:11,467 INFO] Model is improving acc: 68.6602 --> 68.7761.\n",
            "[2022-02-19 09:45:11,516 INFO] Saving checkpoint ./models/nb-steps/model_step_6875.pt\n",
            "[2022-02-19 09:45:11,771 INFO] Step 6880/20000; acc:  68.16; ppl:  4.31; xent: 1.46; lr: 1.00000; 8361/9286 tok/s;    113 sec\n",
            "[2022-02-19 09:45:12,110 INFO] Step 6900/20000; acc:  60.24; ppl:  6.86; xent: 1.93; lr: 1.00000; 20153/20546 tok/s;    114 sec\n",
            "[2022-02-19 09:45:12,443 INFO] Step 6920/20000; acc:  62.00; ppl:  5.70; xent: 1.74; lr: 1.00000; 21059/21020 tok/s;    114 sec\n",
            "[2022-02-19 09:45:12,725 INFO] Step 6940/20000; acc:  65.84; ppl:  4.68; xent: 1.54; lr: 1.00000; 20556/22049 tok/s;    114 sec\n",
            "[2022-02-19 09:45:13,049 INFO] Step 6960/20000; acc:  62.05; ppl:  6.10; xent: 1.81; lr: 1.00000; 20551/21471 tok/s;    115 sec\n",
            "[2022-02-19 09:45:13,378 INFO] Step 6980/20000; acc:  62.81; ppl:  5.32; xent: 1.67; lr: 1.00000; 20389/21262 tok/s;    115 sec\n",
            "[2022-02-19 09:45:13,679 INFO] Step 7000/20000; acc:  63.19; ppl:  5.74; xent: 1.75; lr: 1.00000; 21239/22189 tok/s;    115 sec\n",
            "[2022-02-19 09:45:13,972 INFO] Step 7020/20000; acc:  62.37; ppl:  6.11; xent: 1.81; lr: 1.00000; 23012/23681 tok/s;    116 sec\n",
            "[2022-02-19 09:45:14,260 INFO] Step 7040/20000; acc:  62.72; ppl:  5.62; xent: 1.73; lr: 1.00000; 24168/24335 tok/s;    116 sec\n",
            "[2022-02-19 09:45:14,618 INFO] Step 7060/20000; acc:  64.12; ppl:  5.15; xent: 1.64; lr: 1.00000; 17523/18472 tok/s;    116 sec\n",
            "[2022-02-19 09:45:14,905 INFO] Step 7080/20000; acc:  66.43; ppl:  4.75; xent: 1.56; lr: 1.00000; 19626/21707 tok/s;    117 sec\n",
            "[2022-02-19 09:45:15,205 INFO] Step 7100/20000; acc:  64.27; ppl:  5.14; xent: 1.64; lr: 1.00000; 22529/23585 tok/s;    117 sec\n",
            "[2022-02-19 09:45:15,537 INFO] Step 7120/20000; acc:  63.53; ppl:  5.28; xent: 1.66; lr: 1.00000; 19773/20700 tok/s;    117 sec\n",
            "[2022-02-19 09:45:15,831 INFO] Step 7140/20000; acc:  65.25; ppl:  5.19; xent: 1.65; lr: 1.00000; 19993/21786 tok/s;    117 sec\n",
            "[2022-02-19 09:45:16,131 INFO] Step 7160/20000; acc:  68.23; ppl:  4.18; xent: 1.43; lr: 1.00000; 19575/20960 tok/s;    118 sec\n",
            "[2022-02-19 09:45:16,477 INFO] Step 7180/20000; acc:  63.59; ppl:  5.39; xent: 1.68; lr: 1.00000; 20891/21122 tok/s;    118 sec\n",
            "[2022-02-19 09:45:16,742 INFO] Step 7200/20000; acc:  67.90; ppl:  4.22; xent: 1.44; lr: 1.00000; 21332/22389 tok/s;    118 sec\n",
            "[2022-02-19 09:45:17,066 INFO] Step 7220/20000; acc:  59.77; ppl:  6.90; xent: 1.93; lr: 1.00000; 21880/22379 tok/s;    119 sec\n",
            "[2022-02-19 09:45:17,398 INFO] Step 7240/20000; acc:  63.56; ppl:  5.38; xent: 1.68; lr: 1.00000; 21487/21150 tok/s;    119 sec\n",
            "[2022-02-19 09:45:17,664 INFO] Step 7260/20000; acc:  67.10; ppl:  4.27; xent: 1.45; lr: 1.00000; 21517/22819 tok/s;    119 sec\n",
            "[2022-02-19 09:45:17,967 INFO] Step 7280/20000; acc:  62.51; ppl:  6.08; xent: 1.81; lr: 1.00000; 22038/22163 tok/s;    120 sec\n",
            "[2022-02-19 09:45:18,287 INFO] Step 7300/20000; acc:  64.53; ppl:  4.84; xent: 1.58; lr: 1.00000; 20863/21128 tok/s;    120 sec\n",
            "[2022-02-19 09:45:18,591 INFO] Step 7320/20000; acc:  63.46; ppl:  5.40; xent: 1.69; lr: 1.00000; 20992/21726 tok/s;    120 sec\n",
            "[2022-02-19 09:45:18,901 INFO] Step 7340/20000; acc:  63.89; ppl:  5.39; xent: 1.68; lr: 1.00000; 21375/22443 tok/s;    121 sec\n",
            "[2022-02-19 09:45:19,193 INFO] Step 7360/20000; acc:  62.19; ppl:  5.91; xent: 1.78; lr: 1.00000; 23870/23572 tok/s;    121 sec\n",
            "[2022-02-19 09:45:19,602 INFO] Step 7380/20000; acc:  63.68; ppl:  5.25; xent: 1.66; lr: 1.00000; 15743/16542 tok/s;    121 sec\n",
            "[2022-02-19 09:45:19,968 INFO] Step 7400/20000; acc:  65.92; ppl:  5.03; xent: 1.62; lr: 1.00000; 15738/17220 tok/s;    122 sec\n",
            "[2022-02-19 09:45:20,323 INFO] Step 7420/20000; acc:  62.94; ppl:  5.47; xent: 1.70; lr: 1.00000; 19411/20269 tok/s;    122 sec\n",
            "[2022-02-19 09:45:20,643 INFO] Step 7440/20000; acc:  64.22; ppl:  5.13; xent: 1.63; lr: 1.00000; 20537/20878 tok/s;    122 sec\n",
            "[2022-02-19 09:45:20,933 INFO] Step 7460/20000; acc:  66.39; ppl:  4.71; xent: 1.55; lr: 1.00000; 19972/21660 tok/s;    123 sec\n",
            "[2022-02-19 09:45:21,204 INFO] Step 7480/20000; acc:  66.45; ppl:  4.58; xent: 1.52; lr: 1.00000; 21570/23269 tok/s;    123 sec\n",
            "[2022-02-19 09:45:21,336 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 13\n",
            "[2022-02-19 09:45:21,566 INFO] Step 7500/20000; acc:  61.95; ppl:  5.73; xent: 1.75; lr: 1.00000; 19662/20779 tok/s;    123 sec\n",
            "[2022-02-19 09:45:21,699 INFO] Validation perplexity: 5.45321\n",
            "[2022-02-19 09:45:21,699 INFO] Validation accuracy: 69.7033\n",
            "[2022-02-19 09:45:21,700 INFO] Model is improving ppl: 5.49383 --> 5.45321.\n",
            "[2022-02-19 09:45:21,700 INFO] Model is improving acc: 68.7761 --> 69.7033.\n",
            "[2022-02-19 09:45:21,750 INFO] Saving checkpoint ./models/nb-steps/model_step_7500.pt\n",
            "[2022-02-19 09:45:22,215 INFO] Step 7520/20000; acc:  67.58; ppl:  4.17; xent: 1.43; lr: 1.00000; 8438/9075 tok/s;    124 sec\n",
            "[2022-02-19 09:45:22,542 INFO] Step 7540/20000; acc:  61.08; ppl:  6.18; xent: 1.82; lr: 1.00000; 21110/21774 tok/s;    124 sec\n",
            "[2022-02-19 09:45:22,884 INFO] Step 7560/20000; acc:  63.32; ppl:  5.26; xent: 1.66; lr: 1.00000; 20634/20760 tok/s;    125 sec\n",
            "[2022-02-19 09:45:23,156 INFO] Step 7580/20000; acc:  67.69; ppl:  4.27; xent: 1.45; lr: 1.00000; 21532/22441 tok/s;    125 sec\n",
            "[2022-02-19 09:45:23,454 INFO] Step 7600/20000; acc:  62.51; ppl:  5.97; xent: 1.79; lr: 1.00000; 22533/23374 tok/s;    125 sec\n",
            "[2022-02-19 09:45:23,772 INFO] Step 7620/20000; acc:  64.73; ppl:  5.02; xent: 1.61; lr: 1.00000; 21168/22172 tok/s;    125 sec\n",
            "[2022-02-19 09:45:24,078 INFO] Step 7640/20000; acc:  65.51; ppl:  4.94; xent: 1.60; lr: 1.00000; 20479/21658 tok/s;    126 sec\n",
            "[2022-02-19 09:45:24,366 INFO] Step 7660/20000; acc:  63.84; ppl:  5.50; xent: 1.70; lr: 1.00000; 22889/23852 tok/s;    126 sec\n",
            "[2022-02-19 09:45:24,664 INFO] Step 7680/20000; acc:  63.85; ppl:  5.10; xent: 1.63; lr: 1.00000; 22824/23213 tok/s;    126 sec\n",
            "[2022-02-19 09:45:25,027 INFO] Step 7700/20000; acc:  64.75; ppl:  4.94; xent: 1.60; lr: 1.00000; 17409/18154 tok/s;    127 sec\n",
            "[2022-02-19 09:45:25,330 INFO] Step 7720/20000; acc:  65.99; ppl:  4.80; xent: 1.57; lr: 1.00000; 18966/20996 tok/s;    127 sec\n",
            "[2022-02-19 09:45:25,617 INFO] Step 7740/20000; acc:  64.88; ppl:  4.83; xent: 1.57; lr: 1.00000; 23689/24634 tok/s;    127 sec\n",
            "[2022-02-19 09:45:25,942 INFO] Step 7760/20000; acc:  64.04; ppl:  5.13; xent: 1.63; lr: 1.00000; 20284/21506 tok/s;    128 sec\n",
            "[2022-02-19 09:45:26,224 INFO] Step 7780/20000; acc:  65.75; ppl:  4.77; xent: 1.56; lr: 1.00000; 20820/22775 tok/s;    128 sec\n",
            "[2022-02-19 09:45:26,496 INFO] Step 7800/20000; acc:  67.51; ppl:  4.16; xent: 1.43; lr: 1.00000; 21780/23065 tok/s;    128 sec\n",
            "[2022-02-19 09:45:26,832 INFO] Step 7820/20000; acc:  63.95; ppl:  5.23; xent: 1.65; lr: 1.00000; 21632/21760 tok/s;    128 sec\n",
            "[2022-02-19 09:45:27,088 INFO] Step 7840/20000; acc:  69.92; ppl:  3.74; xent: 1.32; lr: 1.00000; 21830/23024 tok/s;    129 sec\n",
            "[2022-02-19 09:45:27,398 INFO] Step 7860/20000; acc:  60.90; ppl:  6.39; xent: 1.85; lr: 1.00000; 22773/23053 tok/s;    129 sec\n",
            "[2022-02-19 09:45:27,735 INFO] Step 7880/20000; acc:  63.58; ppl:  5.25; xent: 1.66; lr: 1.00000; 21237/20660 tok/s;    129 sec\n",
            "[2022-02-19 09:45:28,004 INFO] Step 7900/20000; acc:  66.46; ppl:  4.36; xent: 1.47; lr: 1.00000; 21515/23325 tok/s;    130 sec\n",
            "[2022-02-19 09:45:28,298 INFO] Step 7920/20000; acc:  63.10; ppl:  5.81; xent: 1.76; lr: 1.00000; 22991/22865 tok/s;    130 sec\n",
            "[2022-02-19 09:45:28,618 INFO] Step 7940/20000; acc:  65.83; ppl:  4.53; xent: 1.51; lr: 1.00000; 21122/20771 tok/s;    130 sec\n",
            "[2022-02-19 09:45:28,917 INFO] Step 7960/20000; acc:  64.07; ppl:  4.96; xent: 1.60; lr: 1.00000; 20773/22213 tok/s;    131 sec\n",
            "[2022-02-19 09:45:29,247 INFO] Step 7980/20000; acc:  64.37; ppl:  4.90; xent: 1.59; lr: 1.00000; 19652/20475 tok/s;    131 sec\n",
            "[2022-02-19 09:45:29,623 INFO] Step 8000/20000; acc:  63.46; ppl:  5.42; xent: 1.69; lr: 1.00000; 18079/18428 tok/s;    131 sec\n",
            "[2022-02-19 09:45:30,088 INFO] Step 8020/20000; acc:  65.03; ppl:  4.76; xent: 1.56; lr: 1.00000; 13775/14517 tok/s;    132 sec\n",
            "[2022-02-19 09:45:30,381 INFO] Step 8040/20000; acc:  67.03; ppl:  4.57; xent: 1.52; lr: 1.00000; 19841/21360 tok/s;    132 sec\n",
            "[2022-02-19 09:45:30,679 INFO] Step 8060/20000; acc:  64.56; ppl:  4.87; xent: 1.58; lr: 1.00000; 23130/24063 tok/s;    132 sec\n",
            "[2022-02-19 09:45:30,735 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 14\n",
            "[2022-02-19 09:45:30,999 INFO] Step 8080/20000; acc:  63.09; ppl:  5.17; xent: 1.64; lr: 1.00000; 20630/21957 tok/s;    133 sec\n",
            "[2022-02-19 09:45:31,288 INFO] Step 8100/20000; acc:  67.31; ppl:  4.63; xent: 1.53; lr: 1.00000; 20094/21319 tok/s;    133 sec\n",
            "[2022-02-19 09:45:31,560 INFO] Step 8120/20000; acc:  66.93; ppl:  4.27; xent: 1.45; lr: 1.00000; 21667/23236 tok/s;    133 sec\n",
            "[2022-02-19 09:45:31,785 INFO] Validation perplexity: 5.45697\n",
            "[2022-02-19 09:45:31,785 INFO] Validation accuracy: 69.1006\n",
            "[2022-02-19 09:45:31,785 INFO] Decreasing patience: 1/2\n",
            "[2022-02-19 09:45:31,834 INFO] Saving checkpoint ./models/nb-steps/model_step_8125.pt\n",
            "[2022-02-19 09:45:32,287 INFO] Step 8140/20000; acc:  63.59; ppl:  5.13; xent: 1.64; lr: 1.00000; 9823/10319 tok/s;    134 sec\n",
            "[2022-02-19 09:45:32,548 INFO] Step 8160/20000; acc:  69.52; ppl:  3.68; xent: 1.30; lr: 1.00000; 21088/22673 tok/s;    134 sec\n",
            "[2022-02-19 09:45:32,873 INFO] Step 8180/20000; acc:  62.05; ppl:  5.72; xent: 1.74; lr: 1.00000; 21172/21074 tok/s;    135 sec\n",
            "[2022-02-19 09:45:33,206 INFO] Step 8200/20000; acc:  62.86; ppl:  5.36; xent: 1.68; lr: 1.00000; 21236/21638 tok/s;    135 sec\n",
            "[2022-02-19 09:45:33,485 INFO] Step 8220/20000; acc:  67.93; ppl:  4.14; xent: 1.42; lr: 1.00000; 21181/22073 tok/s;    135 sec\n",
            "[2022-02-19 09:45:33,779 INFO] Step 8240/20000; acc:  63.11; ppl:  5.77; xent: 1.75; lr: 1.00000; 23241/23929 tok/s;    135 sec\n",
            "[2022-02-19 09:45:34,106 INFO] Step 8260/20000; acc:  66.48; ppl:  4.49; xent: 1.50; lr: 1.00000; 20638/21252 tok/s;    136 sec\n",
            "[2022-02-19 09:45:34,390 INFO] Step 8280/20000; acc:  65.81; ppl:  4.60; xent: 1.53; lr: 1.00000; 21501/23793 tok/s;    136 sec\n",
            "[2022-02-19 09:45:34,680 INFO] Step 8300/20000; acc:  65.27; ppl:  5.04; xent: 1.62; lr: 1.00000; 22221/22869 tok/s;    136 sec\n",
            "[2022-02-19 09:45:34,977 INFO] Step 8320/20000; acc:  65.96; ppl:  4.58; xent: 1.52; lr: 1.00000; 22354/23166 tok/s;    137 sec\n",
            "[2022-02-19 09:45:35,338 INFO] Step 8340/20000; acc:  64.48; ppl:  4.76; xent: 1.56; lr: 1.00000; 17588/18828 tok/s;    137 sec\n",
            "[2022-02-19 09:45:35,630 INFO] Step 8360/20000; acc:  67.18; ppl:  4.37; xent: 1.47; lr: 1.00000; 19696/21395 tok/s;    137 sec\n",
            "[2022-02-19 09:45:35,922 INFO] Step 8380/20000; acc:  65.99; ppl:  4.52; xent: 1.51; lr: 1.00000; 23551/24330 tok/s;    138 sec\n",
            "[2022-02-19 09:45:36,253 INFO] Step 8400/20000; acc:  65.20; ppl:  4.88; xent: 1.58; lr: 1.00000; 20099/21140 tok/s;    138 sec\n",
            "[2022-02-19 09:45:36,533 INFO] Step 8420/20000; acc:  65.96; ppl:  4.92; xent: 1.59; lr: 1.00000; 21559/22820 tok/s;    138 sec\n",
            "[2022-02-19 09:45:36,804 INFO] Step 8440/20000; acc:  68.61; ppl:  3.92; xent: 1.37; lr: 1.00000; 22058/23162 tok/s;    138 sec\n",
            "[2022-02-19 09:45:37,156 INFO] Step 8460/20000; acc:  63.90; ppl:  5.16; xent: 1.64; lr: 1.00000; 20593/20692 tok/s;    139 sec\n",
            "[2022-02-19 09:45:37,419 INFO] Step 8480/20000; acc:  71.28; ppl:  3.36; xent: 1.21; lr: 1.00000; 21115/22068 tok/s;    139 sec\n",
            "[2022-02-19 09:45:37,714 INFO] Step 8500/20000; acc:  61.85; ppl:  5.63; xent: 1.73; lr: 1.00000; 23563/23720 tok/s;    139 sec\n",
            "[2022-02-19 09:45:38,057 INFO] Step 8520/20000; acc:  64.85; ppl:  4.89; xent: 1.59; lr: 1.00000; 20840/20679 tok/s;    140 sec\n",
            "[2022-02-19 09:45:38,327 INFO] Step 8540/20000; acc:  67.80; ppl:  4.04; xent: 1.40; lr: 1.00000; 21882/22808 tok/s;    140 sec\n",
            "[2022-02-19 09:45:38,629 INFO] Step 8560/20000; acc:  63.11; ppl:  5.61; xent: 1.72; lr: 1.00000; 22820/22919 tok/s;    140 sec\n",
            "[2022-02-19 09:45:38,955 INFO] Step 8580/20000; acc:  65.83; ppl:  4.55; xent: 1.52; lr: 1.00000; 20870/21259 tok/s;    141 sec\n",
            "[2022-02-19 09:45:39,248 INFO] Step 8600/20000; acc:  67.34; ppl:  4.31; xent: 1.46; lr: 1.00000; 20945/23235 tok/s;    141 sec\n",
            "[2022-02-19 09:45:39,591 INFO] Step 8620/20000; acc:  65.77; ppl:  4.64; xent: 1.54; lr: 1.00000; 18826/19333 tok/s;    141 sec\n",
            "[2022-02-19 09:45:40,006 INFO] Step 8640/20000; acc:  65.57; ppl:  4.79; xent: 1.57; lr: 1.00000; 16304/16408 tok/s;    142 sec\n",
            "[2022-02-19 09:45:40,399 INFO] Step 8660/20000; acc:  65.65; ppl:  4.56; xent: 1.52; lr: 1.00000; 16360/16591 tok/s;    142 sec\n",
            "[2022-02-19 09:45:40,675 INFO] Step 8680/20000; acc:  68.20; ppl:  4.34; xent: 1.47; lr: 1.00000; 20882/22329 tok/s;    142 sec\n",
            "[2022-02-19 09:45:40,978 INFO] Step 8700/20000; acc:  65.20; ppl:  4.71; xent: 1.55; lr: 1.00000; 22420/24515 tok/s;    143 sec\n",
            "[2022-02-19 09:45:41,037 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 15\n",
            "[2022-02-19 09:45:41,322 INFO] Step 8720/20000; acc:  63.72; ppl:  4.99; xent: 1.61; lr: 1.00000; 19145/20503 tok/s;    143 sec\n",
            "[2022-02-19 09:45:41,594 INFO] Step 8740/20000; acc:  67.62; ppl:  4.27; xent: 1.45; lr: 1.00000; 21217/23067 tok/s;    143 sec\n",
            "[2022-02-19 09:45:41,860 INFO] Validation perplexity: 5.44089\n",
            "[2022-02-19 09:45:41,860 INFO] Validation accuracy: 69.5874\n",
            "[2022-02-19 09:45:41,861 INFO] Stalled patience: 1/2\n",
            "[2022-02-19 09:45:41,913 INFO] Saving checkpoint ./models/nb-steps/model_step_8750.pt\n",
            "[2022-02-19 09:45:42,232 INFO] Step 8760/20000; acc:  67.17; ppl:  4.14; xent: 1.42; lr: 1.00000; 9308/9909 tok/s;    144 sec\n",
            "[2022-02-19 09:45:42,601 INFO] Step 8780/20000; acc:  63.94; ppl:  5.00; xent: 1.61; lr: 1.00000; 19599/19699 tok/s;    144 sec\n",
            "[2022-02-19 09:45:42,865 INFO] Step 8800/20000; acc:  70.55; ppl:  3.43; xent: 1.23; lr: 1.00000; 21193/22445 tok/s;    145 sec\n",
            "[2022-02-19 09:45:43,184 INFO] Step 8820/20000; acc:  62.69; ppl:  5.56; xent: 1.71; lr: 1.00000; 21764/22046 tok/s;    145 sec\n",
            "[2022-02-19 09:45:43,541 INFO] Step 8840/20000; acc:  63.68; ppl:  5.04; xent: 1.62; lr: 1.00000; 19895/20380 tok/s;    145 sec\n",
            "[2022-02-19 09:45:43,831 INFO] Step 8860/20000; acc:  68.44; ppl:  3.98; xent: 1.38; lr: 1.00000; 20190/21217 tok/s;    145 sec\n",
            "[2022-02-19 09:45:44,133 INFO] Step 8880/20000; acc:  64.37; ppl:  5.49; xent: 1.70; lr: 1.00000; 22823/23442 tok/s;    146 sec\n",
            "[2022-02-19 09:45:44,469 INFO] Step 8900/20000; acc:  66.03; ppl:  4.41; xent: 1.48; lr: 1.00000; 20016/20170 tok/s;    146 sec\n",
            "[2022-02-19 09:45:44,731 INFO] Step 8920/20000; acc:  66.87; ppl:  4.21; xent: 1.44; lr: 1.00000; 22831/25008 tok/s;    146 sec\n",
            "[2022-02-19 09:45:45,020 INFO] Step 8940/20000; acc:  67.12; ppl:  4.42; xent: 1.49; lr: 1.00000; 21739/23272 tok/s;    147 sec\n",
            "[2022-02-19 09:45:45,333 INFO] Step 8960/20000; acc:  66.91; ppl:  4.20; xent: 1.44; lr: 1.00000; 20740/21931 tok/s;    147 sec\n",
            "[2022-02-19 09:45:45,694 INFO] Step 8980/20000; acc:  65.36; ppl:  4.47; xent: 1.50; lr: 1.00000; 17695/19452 tok/s;    147 sec\n",
            "[2022-02-19 09:45:45,980 INFO] Step 9000/20000; acc:  68.96; ppl:  4.04; xent: 1.40; lr: 1.00000; 20458/21534 tok/s;    148 sec\n",
            "[2022-02-19 09:45:46,281 INFO] Step 9020/20000; acc:  66.42; ppl:  4.25; xent: 1.45; lr: 1.00000; 23133/23769 tok/s;    148 sec\n",
            "[2022-02-19 09:45:46,608 INFO] Step 9040/20000; acc:  65.45; ppl:  4.64; xent: 1.53; lr: 1.00000; 20521/21225 tok/s;    148 sec\n",
            "[2022-02-19 09:45:46,895 INFO] Step 9060/20000; acc:  67.29; ppl:  4.47; xent: 1.50; lr: 1.00000; 21264/22439 tok/s;    149 sec\n",
            "[2022-02-19 09:45:47,162 INFO] Step 9080/20000; acc:  68.86; ppl:  3.79; xent: 1.33; lr: 1.00000; 22800/23430 tok/s;    149 sec\n",
            "[2022-02-19 09:45:47,508 INFO] Step 9100/20000; acc:  64.40; ppl:  4.81; xent: 1.57; lr: 1.00000; 21016/21123 tok/s;    149 sec\n",
            "[2022-02-19 09:45:47,771 INFO] Step 9120/20000; acc:  70.84; ppl:  3.42; xent: 1.23; lr: 1.00000; 21123/22483 tok/s;    149 sec\n",
            "[2022-02-19 09:45:48,069 INFO] Step 9140/20000; acc:  63.36; ppl:  5.46; xent: 1.70; lr: 1.00000; 23478/23199 tok/s;    150 sec\n",
            "[2022-02-19 09:45:48,406 INFO] Step 9160/20000; acc:  65.39; ppl:  4.71; xent: 1.55; lr: 1.00000; 21183/20656 tok/s;    150 sec\n",
            "[2022-02-19 09:45:48,676 INFO] Step 9180/20000; acc:  68.59; ppl:  3.86; xent: 1.35; lr: 1.00000; 21360/23286 tok/s;    150 sec\n",
            "[2022-02-19 09:45:48,974 INFO] Step 9200/20000; acc:  65.30; ppl:  5.08; xent: 1.63; lr: 1.00000; 22585/23232 tok/s;    151 sec\n",
            "[2022-02-19 09:45:49,313 INFO] Step 9220/20000; acc:  65.61; ppl:  4.49; xent: 1.50; lr: 1.00000; 20000/19763 tok/s;    151 sec\n",
            "[2022-02-19 09:45:49,703 INFO] Step 9240/20000; acc:  66.57; ppl:  4.35; xent: 1.47; lr: 1.00000; 15838/17645 tok/s;    151 sec\n",
            "[2022-02-19 09:45:50,135 INFO] Step 9260/20000; acc:  66.49; ppl:  4.48; xent: 1.50; lr: 1.00000; 14943/15409 tok/s;    152 sec\n",
            "[2022-02-19 09:45:50,440 INFO] Step 9280/20000; acc:  65.84; ppl:  4.55; xent: 1.51; lr: 1.00000; 22146/22655 tok/s;    152 sec\n",
            "[2022-02-19 09:45:50,797 INFO] Step 9300/20000; acc:  67.65; ppl:  4.05; xent: 1.40; lr: 1.00000; 17916/18099 tok/s;    152 sec\n",
            "[2022-02-19 09:45:51,075 INFO] Step 9320/20000; acc:  69.09; ppl:  3.93; xent: 1.37; lr: 1.00000; 20421/22027 tok/s;    153 sec\n",
            "[2022-02-19 09:45:51,385 INFO] Step 9340/20000; acc:  65.08; ppl:  4.48; xent: 1.50; lr: 1.00000; 21974/23578 tok/s;    153 sec\n",
            "[2022-02-19 09:45:51,444 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 16\n",
            "[2022-02-19 09:45:51,735 INFO] Step 9360/20000; acc:  64.00; ppl:  4.84; xent: 1.58; lr: 1.00000; 18867/20275 tok/s;    153 sec\n",
            "[2022-02-19 09:45:52,058 INFO] Validation perplexity: 5.52935\n",
            "[2022-02-19 09:45:52,058 INFO] Validation accuracy: 69.9815\n",
            "[2022-02-19 09:45:52,059 INFO] Stalled patience: 0/2\n",
            "[2022-02-19 09:45:52,059 INFO] Training finished after stalled validations. Early Stop!\n",
            "[2022-02-19 09:45:52,059 INFO] Best model found at step 7500\n",
            "[2022-02-19 09:45:52,111 INFO] Saving checkpoint ./models/nb-steps/model_step_9375.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-nb-steps.yaml -early_stopping 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhLmKKCMUvC7"
      },
      "source": [
        "L'option early_stopping permet d'arrêter l'apprentissage lorsqu'il n'y a plus d'amélioration de la précision et de l'erreur dans les prochaines validation. Par défaut l'option early_stopping concerne l'accuracy et la perplexity (ppl), mais l'option early_stopping_criteria peut nous permettre de changer ces paramètres. \n",
        "\n",
        "Nous avons choisi de garder ces paramètres et nous avons choisi un early_stopping de 2 (si l'accuracy est moins bonne que la meilleur accuracy deux fois de suite, l'entrainement s'arrête). Cette option nous donne aussi le meilleur modèle, ce qui nous permet donc de trouver le nombre optimal de steps (cependant, cela nécessite de bien choisir le paramètre early_stopping).\n",
        "\n",
        "Ici, le meilleur modèle est celui à 7500 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO-pbJeLK6hW",
        "outputId": "7a00950b-b93a-42d7-9a50-864b21ebe39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 09:51:48,621 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 09:51:52,328 INFO] PRED AVG SCORE: -0.5936, PRED PPL: 1.8104\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 09:51:54,015 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 09:51:57,727 INFO] PRED AVG SCORE: -0.5815, PRED PPL: 1.7887\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 09:51:59,441 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 09:52:03,183 INFO] PRED AVG SCORE: -0.5398, PRED PPL: 1.7157\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 09:52:04,864 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 09:52:08,407 INFO] PRED AVG SCORE: -0.5152, PRED PPL: 1.6739\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 09:52:10,114 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 09:52:13,695 INFO] PRED AVG SCORE: -0.4995, PRED PPL: 1.6479\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -model models/nb-steps/model_step_6250.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-steps/pred_6250.txt\n",
        "!onmt_translate -model models/nb-steps/model_step_6875.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-steps/pred_6875.txt\n",
        "!onmt_translate -model models/nb-steps/model_step_7500.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-steps/pred_7500.txt\n",
        "!onmt_translate -model models/nb-steps/model_step_8125.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-steps/pred_8125.txt\n",
        "!onmt_translate -model models/nb-steps/model_step_8750.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-steps/pred_8750.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr8MAexULAAc",
        "outputId": "8e035c66-75df-46f0-9e20-e3aca5c722ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 32.29, 64.0/39.2/27.4/18.6 (BP=0.960, ratio=0.961, hyp_len=3434, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 31.41, 64.1/39.0/26.9/17.4 (BP=0.956, ratio=0.957, hyp_len=3421, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 33.67, 65.4/40.6/29.3/20.4 (BP=0.948, ratio=0.949, hyp_len=3393, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 33.87, 65.9/41.5/30.2/21.2 (BP=0.931, ratio=0.933, hyp_len=3336, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
            "BLEU = 34.62, 65.8/41.7/30.2/21.0 (BP=0.953, ratio=0.954, hyp_len=3409, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-steps/pred_6250.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-steps/pred_6875.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-steps/pred_7500.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-steps/pred_8125.txt\n",
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-steps/pred_8750.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU3nPcLSXhoL"
      },
      "source": [
        "Nous pouvons observer que le fait d'augmenter le nombre de steps, permet d'augmenter le score BLEU dans un premier temps. Cela s'explique car on apprend plus longtemps sur nos données. Cependant, au bout d'un certain nombre de steps, le score BLEU se stabilise et oscille car il y a de l'overfitting. \n",
        "\n",
        "Mais augmenter le nombre de steps ne suffit pas, le score BLEU n'est toujours pas excellent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psUoSU_BCvRq"
      },
      "source": [
        "**Q11 - nombre de couches** : Augmentez le nombre de couches de l’encodeur et du décodeur (n’allez pas au delà de 3 couches). Cela permet-il toujours d’obtenir des meilleurs résultats ? Pourquoi ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTKmySPyakog",
        "outputId": "5d342418-193b-4b64-b5a5-72e83573d646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 10:24:25,820 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-19 10:24:25,821 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-19 10:24:25,821 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-19 10:24:25,821 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-19 10:24:25,822 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-19 10:24:25,822 INFO] Loading vocab from text file...\n",
            "[2022-02-19 10:24:25,822 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-19 10:24:25,844 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-19 10:24:25,848 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-19 10:24:25,863 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-19 10:24:25,867 INFO] Building fields with vocab in counters...\n",
            "[2022-02-19 10:24:25,875 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-19 10:24:25,887 INFO]  * src vocab size: 9980.\n",
            "[2022-02-19 10:24:25,887 INFO]  * src vocab size = 9980\n",
            "[2022-02-19 10:24:25,888 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-19 10:24:25,890 INFO] Building model...\n",
            "[2022-02-19 10:24:29,944 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, num_layers=3, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-19 10:24:29,945 INFO] encoder: 6425648\n",
            "[2022-02-19 10:24:29,945 INFO] decoder: 7244222\n",
            "[2022-02-19 10:24:29,946 INFO] * number of parameters: 13669870\n",
            "[2022-02-19 10:24:29,947 INFO] Starting training on GPU: [0]\n",
            "[2022-02-19 10:24:29,947 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-19 10:24:29,948 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-19 10:24:29,948 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-19 10:24:30,418 INFO] Step 20/ 2000; acc:  10.21; ppl: 1930.07; xent: 7.57; lr: 1.00000; 12467/13249 tok/s;      0 sec\n",
            "[2022-02-19 10:24:30,869 INFO] Step 40/ 2000; acc:  12.96; ppl: 562.48; xent: 6.33; lr: 1.00000; 17566/17986 tok/s;      1 sec\n",
            "[2022-02-19 10:24:31,223 INFO] Step 60/ 2000; acc:  18.43; ppl: 277.22; xent: 5.62; lr: 1.00000; 16613/17340 tok/s;      1 sec\n",
            "[2022-02-19 10:24:31,613 INFO] Step 80/ 2000; acc:  21.23; ppl: 188.23; xent: 5.24; lr: 1.00000; 15404/16050 tok/s;      2 sec\n",
            "[2022-02-19 10:24:32,018 INFO] Step 100/ 2000; acc:  21.31; ppl: 187.50; xent: 5.23; lr: 1.00000; 17091/18229 tok/s;      2 sec\n",
            "[2022-02-19 10:24:32,424 INFO] Step 120/ 2000; acc:  22.85; ppl: 174.49; xent: 5.16; lr: 1.00000; 16839/17029 tok/s;      2 sec\n",
            "[2022-02-19 10:24:32,810 INFO] Step 140/ 2000; acc:  29.19; ppl: 106.19; xent: 4.67; lr: 1.00000; 15221/15519 tok/s;      3 sec\n",
            "[2022-02-19 10:24:33,208 INFO] Step 160/ 2000; acc:  26.07; ppl: 136.86; xent: 4.92; lr: 1.00000; 17680/18573 tok/s;      3 sec\n",
            "[2022-02-19 10:24:33,588 INFO] Step 180/ 2000; acc:  28.21; ppl: 104.44; xent: 4.65; lr: 1.00000; 17157/18480 tok/s;      4 sec\n",
            "[2022-02-19 10:24:33,967 INFO] Step 200/ 2000; acc:  31.81; ppl: 83.87; xent: 4.43; lr: 1.00000; 15120/15692 tok/s;      4 sec\n",
            "[2022-02-19 10:24:34,345 INFO] Step 220/ 2000; acc:  31.15; ppl: 84.91; xent: 4.44; lr: 1.00000; 17230/17762 tok/s;      4 sec\n",
            "[2022-02-19 10:24:34,773 INFO] Step 240/ 2000; acc:  29.05; ppl: 95.41; xent: 4.56; lr: 1.00000; 16230/16985 tok/s;      5 sec\n",
            "[2022-02-19 10:24:35,160 INFO] Step 260/ 2000; acc:  34.90; ppl: 64.25; xent: 4.16; lr: 1.00000; 14234/15845 tok/s;      5 sec\n",
            "[2022-02-19 10:24:35,517 INFO] Step 280/ 2000; acc:  36.02; ppl: 61.89; xent: 4.13; lr: 1.00000; 17036/17338 tok/s;      6 sec\n",
            "[2022-02-19 10:24:35,946 INFO] Step 300/ 2000; acc:  29.23; ppl: 89.29; xent: 4.49; lr: 1.00000; 17849/18422 tok/s;      6 sec\n",
            "[2022-02-19 10:24:36,288 INFO] Step 320/ 2000; acc:  37.23; ppl: 57.33; xent: 4.05; lr: 1.00000; 16040/17788 tok/s;      6 sec\n",
            "[2022-02-19 10:24:36,713 INFO] Step 340/ 2000; acc:  38.73; ppl: 49.69; xent: 3.91; lr: 1.00000; 14181/14133 tok/s;      7 sec\n",
            "[2022-02-19 10:24:37,171 INFO] Step 360/ 2000; acc:  30.37; ppl: 80.82; xent: 4.39; lr: 1.00000; 17388/17373 tok/s;      7 sec\n",
            "[2022-02-19 10:24:37,534 INFO] Step 380/ 2000; acc:  37.13; ppl: 52.99; xent: 3.97; lr: 1.00000; 16583/17206 tok/s;      8 sec\n",
            "[2022-02-19 10:24:37,909 INFO] Step 400/ 2000; acc:  39.89; ppl: 45.62; xent: 3.82; lr: 1.00000; 16116/15913 tok/s;      8 sec\n",
            "[2022-02-19 10:24:38,322 INFO] Step 420/ 2000; acc:  34.87; ppl: 60.66; xent: 4.11; lr: 1.00000; 16859/17583 tok/s;      8 sec\n",
            "[2022-02-19 10:24:38,724 INFO] Step 440/ 2000; acc:  34.84; ppl: 58.90; xent: 4.08; lr: 1.00000; 17178/17824 tok/s;      9 sec\n",
            "[2022-02-19 10:24:39,110 INFO] Step 460/ 2000; acc:  39.27; ppl: 45.45; xent: 3.82; lr: 1.00000; 15137/15384 tok/s;      9 sec\n",
            "[2022-02-19 10:24:39,537 INFO] Step 480/ 2000; acc:  36.21; ppl: 53.16; xent: 3.97; lr: 1.00000; 16315/16802 tok/s;     10 sec\n",
            "[2022-02-19 10:24:39,918 INFO] Step 500/ 2000; acc:  37.28; ppl: 49.15; xent: 3.89; lr: 1.00000; 17084/17948 tok/s;     10 sec\n",
            "[2022-02-19 10:24:40,312 INFO] Step 520/ 2000; acc:  40.35; ppl: 41.98; xent: 3.74; lr: 1.00000; 14416/15602 tok/s;     10 sec\n",
            "[2022-02-19 10:24:40,677 INFO] Step 540/ 2000; acc:  40.22; ppl: 39.52; xent: 3.68; lr: 1.00000; 17962/18469 tok/s;     11 sec\n",
            "[2022-02-19 10:24:41,085 INFO] Step 560/ 2000; acc:  36.90; ppl: 52.91; xent: 3.97; lr: 1.00000; 17362/18008 tok/s;     11 sec\n",
            "[2022-02-19 10:24:41,387 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-19 10:24:41,496 INFO] Step 580/ 2000; acc:  42.52; ppl: 37.56; xent: 3.63; lr: 1.00000; 13614/15054 tok/s;     12 sec\n",
            "[2022-02-19 10:24:41,843 INFO] Step 600/ 2000; acc:  42.49; ppl: 36.07; xent: 3.59; lr: 1.00000; 17568/17637 tok/s;     12 sec\n",
            "[2022-02-19 10:24:42,283 INFO] Step 620/ 2000; acc:  37.06; ppl: 48.92; xent: 3.89; lr: 1.00000; 17348/17815 tok/s;     12 sec\n",
            "[2022-02-19 10:24:42,360 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-19 10:24:42,548 INFO] Validation perplexity: 20.7795\n",
            "[2022-02-19 10:24:42,548 INFO] Validation accuracy: 49.1887\n",
            "[2022-02-19 10:24:42,601 INFO] Saving checkpoint ./models/nb-layer/model_step_625.pt\n",
            "[2022-02-19 10:24:43,086 INFO] Step 640/ 2000; acc:  43.59; ppl: 34.07; xent: 3.53; lr: 1.00000; 6755/7658 tok/s;     13 sec\n",
            "[2022-02-19 10:24:43,507 INFO] Step 660/ 2000; acc:  44.55; ppl: 30.48; xent: 3.42; lr: 1.00000; 13993/14445 tok/s;     14 sec\n",
            "[2022-02-19 10:24:43,979 INFO] Step 680/ 2000; acc:  35.68; ppl: 49.93; xent: 3.91; lr: 1.00000; 16544/17039 tok/s;     14 sec\n",
            "[2022-02-19 10:24:44,326 INFO] Step 700/ 2000; acc:  43.47; ppl: 32.61; xent: 3.48; lr: 1.00000; 16845/17623 tok/s;     14 sec\n",
            "[2022-02-19 10:24:44,714 INFO] Step 720/ 2000; acc:  43.79; ppl: 31.82; xent: 3.46; lr: 1.00000; 15555/16498 tok/s;     15 sec\n",
            "[2022-02-19 10:24:45,142 INFO] Step 740/ 2000; acc:  39.72; ppl: 39.05; xent: 3.66; lr: 1.00000; 16445/17303 tok/s;     15 sec\n",
            "[2022-02-19 10:24:45,555 INFO] Step 760/ 2000; acc:  40.21; ppl: 39.34; xent: 3.67; lr: 1.00000; 16830/16914 tok/s;     16 sec\n",
            "[2022-02-19 10:24:45,927 INFO] Step 780/ 2000; acc:  45.78; ppl: 27.75; xent: 3.32; lr: 1.00000; 15622/16117 tok/s;     16 sec\n",
            "[2022-02-19 10:24:46,345 INFO] Step 800/ 2000; acc:  41.55; ppl: 38.30; xent: 3.65; lr: 1.00000; 16520/17051 tok/s;     16 sec\n",
            "[2022-02-19 10:24:46,725 INFO] Step 820/ 2000; acc:  43.01; ppl: 32.84; xent: 3.49; lr: 1.00000; 16890/17545 tok/s;     17 sec\n",
            "[2022-02-19 10:24:47,118 INFO] Step 840/ 2000; acc:  45.09; ppl: 26.67; xent: 3.28; lr: 1.00000; 14160/16279 tok/s;     17 sec\n",
            "[2022-02-19 10:24:47,484 INFO] Step 860/ 2000; acc:  44.93; ppl: 28.43; xent: 3.35; lr: 1.00000; 17986/18561 tok/s;     18 sec\n",
            "[2022-02-19 10:24:47,908 INFO] Step 880/ 2000; acc:  42.36; ppl: 32.58; xent: 3.48; lr: 1.00000; 16551/17256 tok/s;     18 sec\n",
            "[2022-02-19 10:24:48,296 INFO] Step 900/ 2000; acc:  45.86; ppl: 25.92; xent: 3.25; lr: 1.00000; 14355/15594 tok/s;     18 sec\n",
            "[2022-02-19 10:24:48,695 INFO] Step 920/ 2000; acc:  46.90; ppl: 25.26; xent: 3.23; lr: 1.00000; 15414/15677 tok/s;     19 sec\n",
            "[2022-02-19 10:24:49,203 INFO] Step 940/ 2000; acc:  40.12; ppl: 37.73; xent: 3.63; lr: 1.00000; 15239/15956 tok/s;     19 sec\n",
            "[2022-02-19 10:24:49,631 INFO] Step 960/ 2000; acc:  47.79; ppl: 26.25; xent: 3.27; lr: 1.00000; 13173/13764 tok/s;     20 sec\n",
            "[2022-02-19 10:24:50,138 INFO] Step 980/ 2000; acc:  47.67; ppl: 24.13; xent: 3.18; lr: 1.00000; 11809/11778 tok/s;     20 sec\n",
            "[2022-02-19 10:24:50,578 INFO] Step 1000/ 2000; acc:  40.70; ppl: 33.74; xent: 3.52; lr: 1.00000; 17707/17961 tok/s;     21 sec\n",
            "[2022-02-19 10:24:50,947 INFO] Step 1020/ 2000; acc:  47.35; ppl: 23.47; xent: 3.16; lr: 1.00000; 16161/16674 tok/s;     21 sec\n",
            "[2022-02-19 10:24:51,353 INFO] Step 1040/ 2000; acc:  47.28; ppl: 23.42; xent: 3.15; lr: 1.00000; 14949/15265 tok/s;     21 sec\n",
            "[2022-02-19 10:24:51,790 INFO] Step 1060/ 2000; acc:  42.72; ppl: 31.49; xent: 3.45; lr: 1.00000; 16331/16583 tok/s;     22 sec\n",
            "[2022-02-19 10:24:52,186 INFO] Step 1080/ 2000; acc:  43.24; ppl: 29.62; xent: 3.39; lr: 1.00000; 17739/18009 tok/s;     22 sec\n",
            "[2022-02-19 10:24:52,568 INFO] Step 1100/ 2000; acc:  48.26; ppl: 22.27; xent: 3.10; lr: 1.00000; 15300/15653 tok/s;     23 sec\n",
            "[2022-02-19 10:24:52,973 INFO] Step 1120/ 2000; acc:  44.63; ppl: 27.56; xent: 3.32; lr: 1.00000; 16987/17753 tok/s;     23 sec\n",
            "[2022-02-19 10:24:53,343 INFO] Step 1140/ 2000; acc:  45.02; ppl: 25.70; xent: 3.25; lr: 1.00000; 17457/18561 tok/s;     23 sec\n",
            "[2022-02-19 10:24:53,740 INFO] Step 1160/ 2000; acc:  47.83; ppl: 23.01; xent: 3.14; lr: 1.00000; 14354/15426 tok/s;     24 sec\n",
            "[2022-02-19 10:24:54,125 INFO] Step 1180/ 2000; acc:  46.98; ppl: 22.94; xent: 3.13; lr: 1.00000; 16875/17424 tok/s;     24 sec\n",
            "[2022-02-19 10:24:54,538 INFO] Step 1200/ 2000; acc:  43.98; ppl: 28.81; xent: 3.36; lr: 1.00000; 17018/17844 tok/s;     25 sec\n",
            "[2022-02-19 10:24:54,828 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-19 10:24:54,933 INFO] Step 1220/ 2000; acc:  48.29; ppl: 22.13; xent: 3.10; lr: 1.00000; 13967/15677 tok/s;     25 sec\n",
            "[2022-02-19 10:24:55,289 INFO] Step 1240/ 2000; acc:  48.38; ppl: 21.74; xent: 3.08; lr: 1.00000; 17340/17258 tok/s;     25 sec\n",
            "[2022-02-19 10:24:55,701 INFO] Validation perplexity: 13.3807\n",
            "[2022-02-19 10:24:55,701 INFO] Validation accuracy: 55.9573\n",
            "[2022-02-19 10:24:55,751 INFO] Saving checkpoint ./models/nb-layer/model_step_1250.pt\n",
            "[2022-02-19 10:24:56,197 INFO] Step 1260/ 2000; acc:  43.44; ppl: 29.18; xent: 3.37; lr: 1.00000; 8401/8604 tok/s;     26 sec\n",
            "[2022-02-19 10:24:56,560 INFO] Step 1280/ 2000; acc:  48.65; ppl: 20.03; xent: 3.00; lr: 1.00000; 15067/16983 tok/s;     27 sec\n",
            "[2022-02-19 10:24:57,003 INFO] Step 1300/ 2000; acc:  50.30; ppl: 18.74; xent: 2.93; lr: 1.00000; 13460/13982 tok/s;     27 sec\n",
            "[2022-02-19 10:24:57,469 INFO] Step 1320/ 2000; acc:  42.40; ppl: 30.44; xent: 3.42; lr: 1.00000; 16958/17386 tok/s;     28 sec\n",
            "[2022-02-19 10:24:57,823 INFO] Step 1340/ 2000; acc:  50.57; ppl: 18.25; xent: 2.90; lr: 1.00000; 16704/17275 tok/s;     28 sec\n",
            "[2022-02-19 10:24:58,203 INFO] Step 1360/ 2000; acc:  50.68; ppl: 19.56; xent: 2.97; lr: 1.00000; 15858/16147 tok/s;     28 sec\n",
            "[2022-02-19 10:24:58,624 INFO] Step 1380/ 2000; acc:  45.60; ppl: 25.18; xent: 3.23; lr: 1.00000; 16646/17896 tok/s;     29 sec\n",
            "[2022-02-19 10:24:59,033 INFO] Step 1400/ 2000; acc:  46.55; ppl: 24.67; xent: 3.21; lr: 1.00000; 16944/17226 tok/s;     29 sec\n",
            "[2022-02-19 10:24:59,435 INFO] Step 1420/ 2000; acc:  52.03; ppl: 17.20; xent: 2.85; lr: 1.00000; 14195/15192 tok/s;     29 sec\n",
            "[2022-02-19 10:24:59,827 INFO] Step 1440/ 2000; acc:  47.28; ppl: 23.44; xent: 3.15; lr: 1.00000; 17260/17839 tok/s;     30 sec\n",
            "[2022-02-19 10:25:00,210 INFO] Step 1460/ 2000; acc:  48.91; ppl: 19.64; xent: 2.98; lr: 1.00000; 16422/17321 tok/s;     30 sec\n",
            "[2022-02-19 10:25:00,594 INFO] Step 1480/ 2000; acc:  51.16; ppl: 17.49; xent: 2.86; lr: 1.00000; 14444/16040 tok/s;     31 sec\n",
            "[2022-02-19 10:25:00,959 INFO] Step 1500/ 2000; acc:  49.78; ppl: 18.72; xent: 2.93; lr: 1.00000; 18128/18808 tok/s;     31 sec\n",
            "[2022-02-19 10:25:01,388 INFO] Step 1520/ 2000; acc:  47.17; ppl: 22.37; xent: 3.11; lr: 1.00000; 16467/17253 tok/s;     31 sec\n",
            "[2022-02-19 10:25:01,772 INFO] Step 1540/ 2000; acc:  51.77; ppl: 17.01; xent: 2.83; lr: 1.00000; 14781/15215 tok/s;     32 sec\n",
            "[2022-02-19 10:25:02,132 INFO] Step 1560/ 2000; acc:  50.35; ppl: 17.25; xent: 2.85; lr: 1.00000; 17209/17915 tok/s;     32 sec\n",
            "[2022-02-19 10:25:02,570 INFO] Step 1580/ 2000; acc:  44.97; ppl: 24.26; xent: 3.19; lr: 1.00000; 17758/18566 tok/s;     33 sec\n",
            "[2022-02-19 10:25:02,927 INFO] Step 1600/ 2000; acc:  52.55; ppl: 16.37; xent: 2.80; lr: 1.00000; 16077/16517 tok/s;     33 sec\n",
            "[2022-02-19 10:25:03,340 INFO] Step 1620/ 2000; acc:  51.89; ppl: 16.49; xent: 2.80; lr: 1.00000; 14480/14095 tok/s;     33 sec\n",
            "[2022-02-19 10:25:03,793 INFO] Step 1640/ 2000; acc:  44.18; ppl: 25.44; xent: 3.24; lr: 1.00000; 17443/17825 tok/s;     34 sec\n",
            "[2022-02-19 10:25:04,212 INFO] Step 1660/ 2000; acc:  50.67; ppl: 16.44; xent: 2.80; lr: 1.00000; 14232/14877 tok/s;     34 sec\n",
            "[2022-02-19 10:25:04,702 INFO] Step 1680/ 2000; acc:  52.02; ppl: 15.75; xent: 2.76; lr: 1.00000; 12307/12311 tok/s;     35 sec\n",
            "[2022-02-19 10:25:05,186 INFO] Step 1700/ 2000; acc:  47.14; ppl: 20.79; xent: 3.03; lr: 1.00000; 14361/15130 tok/s;     35 sec\n",
            "[2022-02-19 10:25:05,586 INFO] Step 1720/ 2000; acc:  48.17; ppl: 20.28; xent: 3.01; lr: 1.00000; 17264/17702 tok/s;     36 sec\n",
            "[2022-02-19 10:25:05,988 INFO] Step 1740/ 2000; acc:  52.17; ppl: 16.45; xent: 2.80; lr: 1.00000; 14423/14960 tok/s;     36 sec\n",
            "[2022-02-19 10:25:06,417 INFO] Step 1760/ 2000; acc:  47.97; ppl: 20.47; xent: 3.02; lr: 1.00000; 16089/16764 tok/s;     36 sec\n",
            "[2022-02-19 10:25:06,791 INFO] Step 1780/ 2000; acc:  51.08; ppl: 16.97; xent: 2.83; lr: 1.00000; 17281/18295 tok/s;     37 sec\n",
            "[2022-02-19 10:25:07,191 INFO] Step 1800/ 2000; acc:  51.99; ppl: 15.55; xent: 2.74; lr: 1.00000; 14126/15177 tok/s;     37 sec\n",
            "[2022-02-19 10:25:07,570 INFO] Step 1820/ 2000; acc:  50.81; ppl: 16.45; xent: 2.80; lr: 1.00000; 17134/17681 tok/s;     38 sec\n",
            "[2022-02-19 10:25:07,993 INFO] Step 1840/ 2000; acc:  47.42; ppl: 21.04; xent: 3.05; lr: 1.00000; 16432/17173 tok/s;     38 sec\n",
            "[2022-02-19 10:25:08,285 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-19 10:25:08,405 INFO] Step 1860/ 2000; acc:  53.02; ppl: 15.39; xent: 2.73; lr: 1.00000; 13424/14926 tok/s;     38 sec\n",
            "[2022-02-19 10:25:08,855 INFO] Validation perplexity: 9.74561\n",
            "[2022-02-19 10:25:08,855 INFO] Validation accuracy: 60.0603\n",
            "[2022-02-19 10:25:08,908 INFO] Saving checkpoint ./models/nb-layer/model_step_1875.pt\n",
            "[2022-02-19 10:25:09,244 INFO] Step 1880/ 2000; acc:  53.78; ppl: 13.93; xent: 2.63; lr: 1.00000; 7349/7549 tok/s;     39 sec\n",
            "[2022-02-19 10:25:09,733 INFO] Step 1900/ 2000; acc:  45.65; ppl: 22.56; xent: 3.12; lr: 1.00000; 15795/16282 tok/s;     40 sec\n",
            "[2022-02-19 10:25:10,103 INFO] Step 1920/ 2000; acc:  51.93; ppl: 15.63; xent: 2.75; lr: 1.00000; 14971/16248 tok/s;     40 sec\n",
            "[2022-02-19 10:25:10,563 INFO] Step 1940/ 2000; acc:  52.78; ppl: 14.11; xent: 2.65; lr: 1.00000; 13029/13509 tok/s;     41 sec\n",
            "[2022-02-19 10:25:11,036 INFO] Step 1960/ 2000; acc:  46.30; ppl: 21.86; xent: 3.08; lr: 1.00000; 16764/17210 tok/s;     41 sec\n",
            "[2022-02-19 10:25:11,392 INFO] Step 1980/ 2000; acc:  52.65; ppl: 14.42; xent: 2.67; lr: 1.00000; 16750/17430 tok/s;     41 sec\n",
            "[2022-02-19 10:25:11,769 INFO] Step 2000/ 2000; acc:  54.97; ppl: 13.61; xent: 2.61; lr: 1.00000; 16043/16388 tok/s;     42 sec\n",
            "[2022-02-19 10:25:11,825 INFO] Saving checkpoint ./models/nb-layer/model_step_2000.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-nb-layer.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUHYfnD4arem",
        "outputId": "f8e903b6-65fd-426a-c53e-1839c52d34c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 10:25:14,049 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 10:25:17,945 INFO] PRED AVG SCORE: -0.9744, PRED PPL: 2.6495\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -model models/nb-layer/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-layer/pred_2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDPFnPFAcDui",
        "outputId": "0fd9a2a6-30b1-4751-ec2b-accb0cab7b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 19.15, 53.5/25.3/15.4/7.9 (BP=0.950, ratio=0.951, hyp_len=3399, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-layer/pred_2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO4x8vJTad6U"
      },
      "source": [
        "Augmenter le nombre de couches ne permet pas toujours d'obtenir un meilleur score BLEU car on a seulement un élément en entrée et un en sortie. Donc la fonction est linéaire et ne nécessite donc pas beaucoup de couches. \n",
        "\n",
        "En effet, nous avons testé avec 2 couches pour l'encodeur et pour le décodeur, 3 couches pour l'encodeur et le décodeur et 3 couches pour l'encodeur et 2 pour le décodeur. Pour ces trois exemples, le score BLEU était moins bon qu'avec 1 couche pour l'encodeur et le décodeur.\n",
        "\n",
        "Le seul exemple ayant augmenté le score BLEU est avec 3 couches pour l'encodeur et 1 pour le décodeur. On est passé de 18,48% à 19,15%, ce qui n'est pas une augmentation très importante.\n",
        "\n",
        "<span style=\"color:red\">Il aurait fallu utiliser l'option early_stopping pour bien repondre à cette question (ainsi que la prochaine). En effet, si l'on utilise pas cette option, on risque de comparer des modèles qui sont sous-entrainé (c'est votre cas) ou sur-entrainés (overfitted),  sans donc pouvoir en tirer des conclusions. En effet, le nombre d'étapes d'entrainement nécessaires à l'optimization du modèle varie selon la quantité de paramétres (poids) à optimizer, ce qui est le cas lorsqu'on varie le nombre de couches ou leur taille.  </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6pcCP16Cy-3"
      },
      "source": [
        "**Q12 - nombre d'unités** : Faites varier le nombre d’unités pour l’encoder et le décodeur (n’allez pas au delà de 512.\n",
        "Obtenez-vous de meilleurs résultats ? Pourquoi ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdqV8XuvhByP",
        "outputId": "9f09678e-b9e1-4109-b078-a2ba0072918e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 10:34:10,456 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-19 10:34:10,458 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-19 10:34:10,458 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-19 10:34:10,458 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-19 10:34:10,459 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-19 10:34:10,459 INFO] Loading vocab from text file...\n",
            "[2022-02-19 10:34:10,459 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-19 10:34:10,480 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-19 10:34:10,485 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-19 10:34:10,500 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-19 10:34:10,504 INFO] Building fields with vocab in counters...\n",
            "[2022-02-19 10:34:10,512 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-19 10:34:10,525 INFO]  * src vocab size: 9980.\n",
            "[2022-02-19 10:34:10,525 INFO]  * src vocab size = 9980\n",
            "[2022-02-19 10:34:10,525 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-19 10:34:10,529 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 10:34:14,794 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 256, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1012, 512)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-19 10:34:14,795 INFO] encoder: 6542384\n",
            "[2022-02-19 10:34:14,795 INFO] decoder: 11429822\n",
            "[2022-02-19 10:34:14,795 INFO] * number of parameters: 17972206\n",
            "[2022-02-19 10:34:14,797 INFO] Starting training on GPU: [0]\n",
            "[2022-02-19 10:34:14,797 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-19 10:34:14,797 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-19 10:34:14,798 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-19 10:34:15,246 INFO] Step 20/ 2000; acc:   9.34; ppl: 10259.71; xent: 9.24; lr: 1.00000; 12275/13493 tok/s;      0 sec\n",
            "[2022-02-19 10:34:15,683 INFO] Step 40/ 2000; acc:  12.66; ppl: 663.84; xent: 6.50; lr: 1.00000; 18014/18641 tok/s;      1 sec\n",
            "[2022-02-19 10:34:16,013 INFO] Step 60/ 2000; acc:  17.64; ppl: 307.39; xent: 5.73; lr: 1.00000; 18217/18874 tok/s;      1 sec\n",
            "[2022-02-19 10:34:16,364 INFO] Step 80/ 2000; acc:  21.76; ppl: 186.85; xent: 5.23; lr: 1.00000; 16723/16669 tok/s;      2 sec\n",
            "[2022-02-19 10:34:16,738 INFO] Step 100/ 2000; acc:  19.58; ppl: 224.19; xent: 5.41; lr: 1.00000; 19412/20375 tok/s;      2 sec\n",
            "[2022-02-19 10:34:17,067 INFO] Step 120/ 2000; acc:  23.25; ppl: 168.11; xent: 5.12; lr: 1.00000; 19589/19989 tok/s;      2 sec\n",
            "[2022-02-19 10:34:17,423 INFO] Step 140/ 2000; acc:  25.34; ppl: 130.15; xent: 4.87; lr: 1.00000; 17206/17924 tok/s;      3 sec\n",
            "[2022-02-19 10:34:17,785 INFO] Step 160/ 2000; acc:  26.35; ppl: 121.88; xent: 4.80; lr: 1.00000; 20118/20813 tok/s;      3 sec\n",
            "[2022-02-19 10:34:18,110 INFO] Step 180/ 2000; acc:  29.25; ppl: 112.38; xent: 4.72; lr: 1.00000; 18626/20055 tok/s;      3 sec\n",
            "[2022-02-19 10:34:18,451 INFO] Step 200/ 2000; acc:  31.17; ppl: 88.74; xent: 4.49; lr: 1.00000; 17330/18652 tok/s;      4 sec\n",
            "[2022-02-19 10:34:18,800 INFO] Step 220/ 2000; acc:  30.34; ppl: 86.89; xent: 4.46; lr: 1.00000; 19244/20372 tok/s;      4 sec\n",
            "[2022-02-19 10:34:19,161 INFO] Step 240/ 2000; acc:  30.82; ppl: 87.81; xent: 4.48; lr: 1.00000; 17628/18922 tok/s;      4 sec\n",
            "[2022-02-19 10:34:19,509 INFO] Step 260/ 2000; acc:  35.85; ppl: 65.94; xent: 4.19; lr: 1.00000; 17142/17768 tok/s;      5 sec\n",
            "[2022-02-19 10:34:19,842 INFO] Step 280/ 2000; acc:  32.64; ppl: 69.62; xent: 4.24; lr: 1.00000; 18745/20128 tok/s;      5 sec\n",
            "[2022-02-19 10:34:20,216 INFO] Step 300/ 2000; acc:  33.24; ppl: 76.90; xent: 4.34; lr: 1.00000; 19150/19511 tok/s;      5 sec\n",
            "[2022-02-19 10:34:20,523 INFO] Step 320/ 2000; acc:  35.81; ppl: 64.07; xent: 4.16; lr: 1.00000; 19592/20524 tok/s;      6 sec\n",
            "[2022-02-19 10:34:20,899 INFO] Step 340/ 2000; acc:  40.65; ppl: 44.09; xent: 3.79; lr: 1.00000; 15155/15219 tok/s;      6 sec\n",
            "[2022-02-19 10:34:21,296 INFO] Step 360/ 2000; acc:  29.90; ppl: 82.79; xent: 4.42; lr: 1.00000; 20084/20358 tok/s;      6 sec\n",
            "[2022-02-19 10:34:21,612 INFO] Step 380/ 2000; acc:  36.80; ppl: 55.33; xent: 4.01; lr: 1.00000; 19255/19610 tok/s;      7 sec\n",
            "[2022-02-19 10:34:21,962 INFO] Step 400/ 2000; acc:  40.85; ppl: 43.23; xent: 3.77; lr: 1.00000; 16868/16856 tok/s;      7 sec\n",
            "[2022-02-19 10:34:22,325 INFO] Step 420/ 2000; acc:  33.91; ppl: 58.56; xent: 4.07; lr: 1.00000; 20316/20650 tok/s;      8 sec\n",
            "[2022-02-19 10:34:22,655 INFO] Step 440/ 2000; acc:  36.44; ppl: 61.24; xent: 4.11; lr: 1.00000; 19529/19802 tok/s;      8 sec\n",
            "[2022-02-19 10:34:23,023 INFO] Step 460/ 2000; acc:  37.72; ppl: 45.39; xent: 3.82; lr: 1.00000; 16441/17763 tok/s;      8 sec\n",
            "[2022-02-19 10:34:23,390 INFO] Step 480/ 2000; acc:  34.79; ppl: 56.25; xent: 4.03; lr: 1.00000; 19689/20783 tok/s;      9 sec\n",
            "[2022-02-19 10:34:23,729 INFO] Step 500/ 2000; acc:  38.27; ppl: 50.11; xent: 3.91; lr: 1.00000; 17679/18613 tok/s;      9 sec\n",
            "[2022-02-19 10:34:24,085 INFO] Step 520/ 2000; acc:  41.01; ppl: 38.30; xent: 3.65; lr: 1.00000; 16578/17205 tok/s;      9 sec\n",
            "[2022-02-19 10:34:24,428 INFO] Step 540/ 2000; acc:  38.04; ppl: 46.98; xent: 3.85; lr: 1.00000; 19928/20698 tok/s;     10 sec\n",
            "[2022-02-19 10:34:24,783 INFO] Step 560/ 2000; acc:  38.22; ppl: 49.58; xent: 3.90; lr: 1.00000; 18409/19786 tok/s;     10 sec\n",
            "[2022-02-19 10:34:25,043 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-19 10:34:25,131 INFO] Step 580/ 2000; acc:  42.96; ppl: 34.86; xent: 3.55; lr: 1.00000; 17185/17927 tok/s;     10 sec\n",
            "[2022-02-19 10:34:25,472 INFO] Step 600/ 2000; acc:  39.43; ppl: 43.71; xent: 3.78; lr: 1.00000; 18264/19700 tok/s;     11 sec\n",
            "[2022-02-19 10:34:25,848 INFO] Step 620/ 2000; acc:  39.19; ppl: 45.57; xent: 3.82; lr: 1.00000; 18959/18807 tok/s;     11 sec\n",
            "[2022-02-19 10:34:25,927 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-19 10:34:26,081 INFO] Validation perplexity: 21.3202\n",
            "[2022-02-19 10:34:26,081 INFO] Validation accuracy: 49.4669\n",
            "[2022-02-19 10:34:26,134 INFO] Saving checkpoint ./models/nb-unite/model_step_625.pt\n",
            "[2022-02-19 10:34:26,658 INFO] Step 640/ 2000; acc:  41.12; ppl: 36.30; xent: 3.59; lr: 1.00000; 7371/7930 tok/s;     12 sec\n",
            "[2022-02-19 10:34:27,057 INFO] Step 660/ 2000; acc:  44.44; ppl: 31.54; xent: 3.45; lr: 1.00000; 13864/14729 tok/s;     12 sec\n",
            "[2022-02-19 10:34:27,468 INFO] Step 680/ 2000; acc:  37.56; ppl: 44.71; xent: 3.80; lr: 1.00000; 18841/19603 tok/s;     13 sec\n",
            "[2022-02-19 10:34:27,778 INFO] Step 700/ 2000; acc:  42.22; ppl: 34.11; xent: 3.53; lr: 1.00000; 19304/20216 tok/s;     13 sec\n",
            "[2022-02-19 10:34:28,113 INFO] Step 720/ 2000; acc:  45.67; ppl: 29.48; xent: 3.38; lr: 1.00000; 17691/17407 tok/s;     13 sec\n",
            "[2022-02-19 10:34:28,486 INFO] Step 740/ 2000; acc:  40.21; ppl: 37.13; xent: 3.61; lr: 1.00000; 19809/20874 tok/s;     14 sec\n",
            "[2022-02-19 10:34:28,824 INFO] Step 760/ 2000; acc:  40.53; ppl: 39.93; xent: 3.69; lr: 1.00000; 19258/19940 tok/s;     14 sec\n",
            "[2022-02-19 10:34:29,199 INFO] Step 780/ 2000; acc:  44.61; ppl: 30.97; xent: 3.43; lr: 1.00000; 16184/16895 tok/s;     14 sec\n",
            "[2022-02-19 10:34:29,550 INFO] Step 800/ 2000; acc:  41.32; ppl: 33.79; xent: 3.52; lr: 1.00000; 20306/20944 tok/s;     15 sec\n",
            "[2022-02-19 10:34:29,875 INFO] Step 820/ 2000; acc:  44.52; ppl: 32.05; xent: 3.47; lr: 1.00000; 18096/20250 tok/s;     15 sec\n",
            "[2022-02-19 10:34:30,231 INFO] Step 840/ 2000; acc:  45.95; ppl: 25.90; xent: 3.25; lr: 1.00000; 16486/17531 tok/s;     15 sec\n",
            "[2022-02-19 10:34:30,567 INFO] Step 860/ 2000; acc:  43.50; ppl: 28.26; xent: 3.34; lr: 1.00000; 20094/21233 tok/s;     16 sec\n",
            "[2022-02-19 10:34:30,933 INFO] Step 880/ 2000; acc:  43.41; ppl: 32.01; xent: 3.47; lr: 1.00000; 17631/18626 tok/s;     16 sec\n",
            "[2022-02-19 10:34:31,281 INFO] Step 900/ 2000; acc:  46.58; ppl: 26.41; xent: 3.27; lr: 1.00000; 17408/17621 tok/s;     16 sec\n",
            "[2022-02-19 10:34:31,607 INFO] Step 920/ 2000; acc:  46.53; ppl: 24.17; xent: 3.19; lr: 1.00000; 19308/20948 tok/s;     17 sec\n",
            "[2022-02-19 10:34:31,982 INFO] Step 940/ 2000; acc:  41.70; ppl: 35.81; xent: 3.58; lr: 1.00000; 19436/19811 tok/s;     17 sec\n",
            "[2022-02-19 10:34:32,293 INFO] Step 960/ 2000; acc:  46.32; ppl: 27.43; xent: 3.31; lr: 1.00000; 19630/20056 tok/s;     17 sec\n",
            "[2022-02-19 10:34:32,659 INFO] Step 980/ 2000; acc:  49.33; ppl: 20.39; xent: 3.01; lr: 1.00000; 15358/15714 tok/s;     18 sec\n",
            "[2022-02-19 10:34:33,067 INFO] Step 1000/ 2000; acc:  40.82; ppl: 33.79; xent: 3.52; lr: 1.00000; 19135/19439 tok/s;     18 sec\n",
            "[2022-02-19 10:34:33,382 INFO] Step 1020/ 2000; acc:  47.13; ppl: 23.41; xent: 3.15; lr: 1.00000; 19148/19527 tok/s;     19 sec\n",
            "[2022-02-19 10:34:33,766 INFO] Step 1040/ 2000; acc:  48.71; ppl: 21.56; xent: 3.07; lr: 1.00000; 15477/15253 tok/s;     19 sec\n",
            "[2022-02-19 10:34:34,199 INFO] Step 1060/ 2000; acc:  43.02; ppl: 28.13; xent: 3.34; lr: 1.00000; 17299/17657 tok/s;     19 sec\n",
            "[2022-02-19 10:34:34,599 INFO] Step 1080/ 2000; acc:  43.92; ppl: 29.23; xent: 3.38; lr: 1.00000; 16355/16909 tok/s;     20 sec\n",
            "[2022-02-19 10:34:35,008 INFO] Step 1100/ 2000; acc:  47.67; ppl: 21.57; xent: 3.07; lr: 1.00000; 14946/15737 tok/s;     20 sec\n",
            "[2022-02-19 10:34:35,378 INFO] Step 1120/ 2000; acc:  43.15; ppl: 28.86; xent: 3.36; lr: 1.00000; 19449/20400 tok/s;     21 sec\n",
            "[2022-02-19 10:34:35,712 INFO] Step 1140/ 2000; acc:  46.61; ppl: 25.07; xent: 3.22; lr: 1.00000; 17688/19135 tok/s;     21 sec\n",
            "[2022-02-19 10:34:36,053 INFO] Step 1160/ 2000; acc:  48.85; ppl: 19.91; xent: 2.99; lr: 1.00000; 17248/17714 tok/s;     21 sec\n",
            "[2022-02-19 10:34:36,422 INFO] Step 1180/ 2000; acc:  47.43; ppl: 23.19; xent: 3.14; lr: 1.00000; 18237/19247 tok/s;     22 sec\n",
            "[2022-02-19 10:34:36,775 INFO] Step 1200/ 2000; acc:  45.62; ppl: 26.02; xent: 3.26; lr: 1.00000; 18447/19587 tok/s;     22 sec\n",
            "[2022-02-19 10:34:37,035 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-19 10:34:37,135 INFO] Step 1220/ 2000; acc:  48.31; ppl: 19.86; xent: 2.99; lr: 1.00000; 16377/17728 tok/s;     22 sec\n",
            "[2022-02-19 10:34:37,461 INFO] Step 1240/ 2000; acc:  47.90; ppl: 22.22; xent: 3.10; lr: 1.00000; 19250/20353 tok/s;     23 sec\n",
            "[2022-02-19 10:34:37,787 INFO] Validation perplexity: 12.5667\n",
            "[2022-02-19 10:34:37,788 INFO] Validation accuracy: 56.7223\n",
            "[2022-02-19 10:34:37,836 INFO] Saving checkpoint ./models/nb-unite/model_step_1250.pt\n",
            "[2022-02-19 10:34:38,309 INFO] Step 1260/ 2000; acc:  45.79; ppl: 25.59; xent: 3.24; lr: 1.00000; 8371/8518 tok/s;     24 sec\n",
            "[2022-02-19 10:34:38,647 INFO] Step 1280/ 2000; acc:  48.29; ppl: 20.59; xent: 3.02; lr: 1.00000; 17934/18698 tok/s;     24 sec\n",
            "[2022-02-19 10:34:39,034 INFO] Step 1300/ 2000; acc:  51.05; ppl: 17.50; xent: 2.86; lr: 1.00000; 14534/15088 tok/s;     24 sec\n",
            "[2022-02-19 10:34:39,446 INFO] Step 1320/ 2000; acc:  45.00; ppl: 24.40; xent: 3.19; lr: 1.00000; 18993/19691 tok/s;     25 sec\n",
            "[2022-02-19 10:34:39,771 INFO] Step 1340/ 2000; acc:  49.47; ppl: 19.09; xent: 2.95; lr: 1.00000; 18628/19358 tok/s;     25 sec\n",
            "[2022-02-19 10:34:40,116 INFO] Step 1360/ 2000; acc:  51.48; ppl: 17.89; xent: 2.88; lr: 1.00000; 17076/17099 tok/s;     25 sec\n",
            "[2022-02-19 10:34:40,488 INFO] Step 1380/ 2000; acc:  46.35; ppl: 21.99; xent: 3.09; lr: 1.00000; 19753/20864 tok/s;     26 sec\n",
            "[2022-02-19 10:34:40,820 INFO] Step 1400/ 2000; acc:  46.86; ppl: 23.92; xent: 3.17; lr: 1.00000; 19677/19961 tok/s;     26 sec\n",
            "[2022-02-19 10:34:41,178 INFO] Step 1420/ 2000; acc:  51.10; ppl: 17.81; xent: 2.88; lr: 1.00000; 16640/18368 tok/s;     26 sec\n",
            "[2022-02-19 10:34:41,534 INFO] Step 1440/ 2000; acc:  47.93; ppl: 20.81; xent: 3.04; lr: 1.00000; 19568/20326 tok/s;     27 sec\n",
            "[2022-02-19 10:34:41,860 INFO] Step 1460/ 2000; acc:  49.68; ppl: 19.40; xent: 2.97; lr: 1.00000; 17732/19610 tok/s;     27 sec\n",
            "[2022-02-19 10:34:42,205 INFO] Step 1480/ 2000; acc:  52.28; ppl: 15.27; xent: 2.73; lr: 1.00000; 16789/17889 tok/s;     27 sec\n",
            "[2022-02-19 10:34:42,554 INFO] Step 1500/ 2000; acc:  50.58; ppl: 18.10; xent: 2.90; lr: 1.00000; 19605/20382 tok/s;     28 sec\n",
            "[2022-02-19 10:34:42,929 INFO] Step 1520/ 2000; acc:  48.87; ppl: 19.49; xent: 2.97; lr: 1.00000; 17336/18269 tok/s;     28 sec\n",
            "[2022-02-19 10:34:43,283 INFO] Step 1540/ 2000; acc:  52.52; ppl: 16.26; xent: 2.79; lr: 1.00000; 17286/17574 tok/s;     28 sec\n",
            "[2022-02-19 10:34:43,608 INFO] Step 1560/ 2000; acc:  51.52; ppl: 15.82; xent: 2.76; lr: 1.00000; 19690/20453 tok/s;     29 sec\n",
            "[2022-02-19 10:34:44,115 INFO] Step 1580/ 2000; acc:  47.12; ppl: 21.85; xent: 3.08; lr: 1.00000; 14506/14790 tok/s;     29 sec\n",
            "[2022-02-19 10:34:44,507 INFO] Step 1600/ 2000; acc:  50.15; ppl: 16.96; xent: 2.83; lr: 1.00000; 15660/16264 tok/s;     30 sec\n",
            "[2022-02-19 10:34:44,913 INFO] Step 1620/ 2000; acc:  53.61; ppl: 14.00; xent: 2.64; lr: 1.00000; 13888/14141 tok/s;     30 sec\n",
            "[2022-02-19 10:34:45,318 INFO] Step 1640/ 2000; acc:  45.39; ppl: 23.53; xent: 3.16; lr: 1.00000; 19562/19940 tok/s;     31 sec\n",
            "[2022-02-19 10:34:45,631 INFO] Step 1660/ 2000; acc:  52.85; ppl: 14.49; xent: 2.67; lr: 1.00000; 19300/19642 tok/s;     31 sec\n",
            "[2022-02-19 10:34:45,976 INFO] Step 1680/ 2000; acc:  54.61; ppl: 13.54; xent: 2.61; lr: 1.00000; 17030/16908 tok/s;     31 sec\n",
            "[2022-02-19 10:34:46,348 INFO] Step 1700/ 2000; acc:  48.11; ppl: 19.11; xent: 2.95; lr: 1.00000; 19889/20342 tok/s;     32 sec\n",
            "[2022-02-19 10:34:46,700 INFO] Step 1720/ 2000; acc:  50.53; ppl: 17.96; xent: 2.89; lr: 1.00000; 18281/18585 tok/s;     32 sec\n",
            "[2022-02-19 10:34:47,057 INFO] Step 1740/ 2000; acc:  52.51; ppl: 14.44; xent: 2.67; lr: 1.00000; 16815/18698 tok/s;     32 sec\n",
            "[2022-02-19 10:34:47,427 INFO] Step 1760/ 2000; acc:  48.50; ppl: 18.30; xent: 2.91; lr: 1.00000; 19407/20376 tok/s;     33 sec\n",
            "[2022-02-19 10:34:47,763 INFO] Step 1780/ 2000; acc:  50.93; ppl: 17.48; xent: 2.86; lr: 1.00000; 17677/18910 tok/s;     33 sec\n",
            "[2022-02-19 10:34:48,096 INFO] Step 1800/ 2000; acc:  54.27; ppl: 13.19; xent: 2.58; lr: 1.00000; 17614/18047 tok/s;     33 sec\n",
            "[2022-02-19 10:34:48,452 INFO] Step 1820/ 2000; acc:  51.26; ppl: 15.61; xent: 2.75; lr: 1.00000; 18884/20026 tok/s;     34 sec\n",
            "[2022-02-19 10:34:48,803 INFO] Step 1840/ 2000; acc:  51.07; ppl: 16.93; xent: 2.83; lr: 1.00000; 18310/19437 tok/s;     34 sec\n",
            "[2022-02-19 10:34:49,064 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-19 10:34:49,164 INFO] Step 1860/ 2000; acc:  53.61; ppl: 13.84; xent: 2.63; lr: 1.00000; 16406/17171 tok/s;     34 sec\n",
            "[2022-02-19 10:34:49,567 INFO] Validation perplexity: 9.41885\n",
            "[2022-02-19 10:34:49,567 INFO] Validation accuracy: 59.9212\n",
            "[2022-02-19 10:34:49,618 INFO] Saving checkpoint ./models/nb-unite/model_step_1875.pt\n",
            "[2022-02-19 10:34:49,987 INFO] Step 1880/ 2000; acc:  52.99; ppl: 14.13; xent: 2.65; lr: 1.00000; 7650/8220 tok/s;     35 sec\n",
            "[2022-02-19 10:34:50,361 INFO] Step 1900/ 2000; acc:  49.60; ppl: 17.93; xent: 2.89; lr: 1.00000; 19272/19776 tok/s;     36 sec\n",
            "[2022-02-19 10:34:50,703 INFO] Step 1920/ 2000; acc:  52.83; ppl: 14.52; xent: 2.68; lr: 1.00000; 17818/18509 tok/s;     36 sec\n",
            "[2022-02-19 10:34:51,087 INFO] Step 1940/ 2000; acc:  55.69; ppl: 12.32; xent: 2.51; lr: 1.00000; 14709/15357 tok/s;     36 sec\n",
            "[2022-02-19 10:34:51,511 INFO] Step 1960/ 2000; acc:  49.16; ppl: 17.29; xent: 2.85; lr: 1.00000; 18542/19372 tok/s;     37 sec\n",
            "[2022-02-19 10:34:51,821 INFO] Step 1980/ 2000; acc:  53.72; ppl: 13.81; xent: 2.63; lr: 1.00000; 19699/19960 tok/s;     37 sec\n",
            "[2022-02-19 10:34:52,159 INFO] Step 2000/ 2000; acc:  56.44; ppl: 12.04; xent: 2.49; lr: 1.00000; 17477/17797 tok/s;     37 sec\n",
            "[2022-02-19 10:34:52,220 INFO] Saving checkpoint ./models/nb-unite/model_step_2000.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-nb-unite.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5kWpL8xhMv1",
        "outputId": "987d4a75-ced8-4160-c3e0-192246a7bf25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 10:48:19,479 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 10:48:26,006 INFO] PRED AVG SCORE: -0.9050, PRED PPL: 2.4718\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -model models/nb-unite/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/nb-unite/pred_2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwb1wr0vhQoz",
        "outputId": "934d27c2-d7f4-4312-97c7-8c5de459f978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 19.69, 52.3/25.3/15.6/7.9 (BP=0.978, ratio=0.978, hyp_len=3497, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/nb-unite/pred_2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnaxff3iek-"
      },
      "source": [
        "Augmenter le nombre de neurones des couches de l'encodeur et du décodeur permet d'obtenir de meilleurs score BLEU. En effet, cela permet d'augmenter le nombre de paramètres avec lesquels jouer, à mettre à jour pour améliorer le modèle.\n",
        "\n",
        "En effet, avec 256 unités, on avait un score BLEU de 18,48 %.\n",
        "Avec 128, on obtient un score BLEU de 13,59 %.\n",
        "\n",
        "En passant à 512, on obtient un score BLEU de 19,69%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z3WanuuC5VT"
      },
      "source": [
        "**Q14** : Expliquez ce qu’est le beam search. Quelle est la taille par défaut du beam ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtnYmMbvjSk2"
      },
      "source": [
        "Le beam search (recherche en faisceau) est un algorithme de recherche heuristique qui n'explore qu'un nombre limité de fils de noeuds dans un graphe. La recherche en faisceau utilise l'algorithme de parcours en largeur. La largeur du faisceau détermine combien de fils sont explorés.\n",
        "\n",
        "<span style=\"color:red\"> C'est quoi les noeuds et les fils dans notre cas ? --> les tokens de chaque étape de décodage de la phrase cible, c.à.d. de la traduction </span>\n",
        "\n",
        "\n",
        "La taille par défaut du beam (faisceau) est 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVKrqhCBC9If"
      },
      "source": [
        "**Q15** : Changez la taille du beam en la faisant varier de 1 à 10. Comment évolue le score BLEU ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2TmCIMilFc0",
        "outputId": "77cb7d0a-aede-40cb-b532-fceeb6de18e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 10:45:23,891 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-19 10:45:23,892 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-19 10:45:23,893 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-19 10:45:23,893 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-19 10:45:23,894 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-19 10:45:23,894 INFO] Loading vocab from text file...\n",
            "[2022-02-19 10:45:23,894 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-19 10:45:23,916 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-19 10:45:23,920 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-19 10:45:23,936 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-19 10:45:23,940 INFO] Building fields with vocab in counters...\n",
            "[2022-02-19 10:45:23,949 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-19 10:45:23,960 INFO]  * src vocab size: 9980.\n",
            "[2022-02-19 10:45:23,961 INFO]  * src vocab size = 9980\n",
            "[2022-02-19 10:45:23,961 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-19 10:45:23,964 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 10:45:28,118 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 256, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1012, 512)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-19 10:45:28,119 INFO] encoder: 6542384\n",
            "[2022-02-19 10:45:28,119 INFO] decoder: 11429822\n",
            "[2022-02-19 10:45:28,119 INFO] * number of parameters: 17972206\n",
            "[2022-02-19 10:45:28,121 INFO] Starting training on GPU: [0]\n",
            "[2022-02-19 10:45:28,121 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-19 10:45:28,121 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-19 10:45:28,121 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-19 10:45:28,612 INFO] Step 20/ 2000; acc:   9.20; ppl: 2545.20; xent: 7.84; lr: 1.00000; 13451/13495 tok/s;      0 sec\n",
            "[2022-02-19 10:45:28,920 INFO] Step 40/ 2000; acc:  16.47; ppl: 482.78; xent: 6.18; lr: 1.00000; 19785/20486 tok/s;      1 sec\n",
            "[2022-02-19 10:45:29,335 INFO] Step 60/ 2000; acc:  14.63; ppl: 384.21; xent: 5.95; lr: 1.00000; 16568/17891 tok/s;      1 sec\n",
            "[2022-02-19 10:45:29,709 INFO] Step 80/ 2000; acc:  18.76; ppl: 255.95; xent: 5.54; lr: 1.00000; 17270/18427 tok/s;      2 sec\n",
            "[2022-02-19 10:45:30,030 INFO] Step 100/ 2000; acc:  20.85; ppl: 197.58; xent: 5.29; lr: 1.00000; 20177/20803 tok/s;      2 sec\n",
            "[2022-02-19 10:45:30,403 INFO] Step 120/ 2000; acc:  21.93; ppl: 175.27; xent: 5.17; lr: 1.00000; 19098/19117 tok/s;      2 sec\n",
            "[2022-02-19 10:45:30,757 INFO] Step 140/ 2000; acc:  25.65; ppl: 137.77; xent: 4.93; lr: 1.00000; 17074/18329 tok/s;      3 sec\n",
            "[2022-02-19 10:45:31,064 INFO] Step 160/ 2000; acc:  27.00; ppl: 123.49; xent: 4.82; lr: 1.00000; 21133/21806 tok/s;      3 sec\n",
            "[2022-02-19 10:45:31,438 INFO] Step 180/ 2000; acc:  26.25; ppl: 125.89; xent: 4.84; lr: 1.00000; 19253/19923 tok/s;      3 sec\n",
            "[2022-02-19 10:45:31,763 INFO] Step 200/ 2000; acc:  35.44; ppl: 74.14; xent: 4.31; lr: 1.00000; 16301/17444 tok/s;      4 sec\n",
            "[2022-02-19 10:45:32,100 INFO] Step 220/ 2000; acc:  32.41; ppl: 81.35; xent: 4.40; lr: 1.00000; 19604/19904 tok/s;      4 sec\n",
            "[2022-02-19 10:45:32,475 INFO] Step 240/ 2000; acc:  28.85; ppl: 101.13; xent: 4.62; lr: 1.00000; 18915/19958 tok/s;      4 sec\n",
            "[2022-02-19 10:45:32,803 INFO] Step 260/ 2000; acc:  36.10; ppl: 59.95; xent: 4.09; lr: 1.00000; 15889/18113 tok/s;      5 sec\n",
            "[2022-02-19 10:45:33,135 INFO] Step 280/ 2000; acc:  34.41; ppl: 69.56; xent: 4.24; lr: 1.00000; 20155/20720 tok/s;      5 sec\n",
            "[2022-02-19 10:45:33,504 INFO] Step 300/ 2000; acc:  31.61; ppl: 83.69; xent: 4.43; lr: 1.00000; 18723/19138 tok/s;      5 sec\n",
            "[2022-02-19 10:45:33,813 INFO] Step 320/ 2000; acc:  37.79; ppl: 53.43; xent: 3.98; lr: 1.00000; 18177/20224 tok/s;      6 sec\n",
            "[2022-02-19 10:45:34,227 INFO] Step 340/ 2000; acc:  34.40; ppl: 65.04; xent: 4.18; lr: 1.00000; 15918/16539 tok/s;      6 sec\n",
            "[2022-02-19 10:45:34,549 INFO] Step 360/ 2000; acc:  37.34; ppl: 55.44; xent: 4.02; lr: 1.00000; 19471/19853 tok/s;      6 sec\n",
            "[2022-02-19 10:45:34,890 INFO] Step 380/ 2000; acc:  34.44; ppl: 62.36; xent: 4.13; lr: 1.00000; 20593/20423 tok/s;      7 sec\n",
            "[2022-02-19 10:45:35,266 INFO] Step 400/ 2000; acc:  34.77; ppl: 57.46; xent: 4.05; lr: 1.00000; 17379/18008 tok/s;      7 sec\n",
            "[2022-02-19 10:45:35,587 INFO] Step 420/ 2000; acc:  37.82; ppl: 52.51; xent: 3.96; lr: 1.00000; 20458/20050 tok/s;      7 sec\n",
            "[2022-02-19 10:45:35,946 INFO] Step 440/ 2000; acc:  34.05; ppl: 63.04; xent: 4.14; lr: 1.00000; 19844/20170 tok/s;      8 sec\n",
            "[2022-02-19 10:45:36,314 INFO] Step 460/ 2000; acc:  37.69; ppl: 51.66; xent: 3.94; lr: 1.00000; 16421/17169 tok/s;      8 sec\n",
            "[2022-02-19 10:45:36,638 INFO] Step 480/ 2000; acc:  38.66; ppl: 45.57; xent: 3.82; lr: 1.00000; 19918/20338 tok/s;      9 sec\n",
            "[2022-02-19 10:45:37,023 INFO] Step 500/ 2000; acc:  35.85; ppl: 54.92; xent: 4.01; lr: 1.00000; 18486/18754 tok/s;      9 sec\n",
            "[2022-02-19 10:45:37,363 INFO] Step 520/ 2000; acc:  41.29; ppl: 38.06; xent: 3.64; lr: 1.00000; 15517/17774 tok/s;      9 sec\n",
            "[2022-02-19 10:45:37,697 INFO] Step 540/ 2000; acc:  39.57; ppl: 43.56; xent: 3.77; lr: 1.00000; 19971/20283 tok/s;     10 sec\n",
            "[2022-02-19 10:45:38,065 INFO] Step 560/ 2000; acc:  37.55; ppl: 51.40; xent: 3.94; lr: 1.00000; 19750/20591 tok/s;     10 sec\n",
            "[2022-02-19 10:45:38,300 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-19 10:45:38,401 INFO] Step 580/ 2000; acc:  44.78; ppl: 32.97; xent: 3.50; lr: 1.00000; 15621/17204 tok/s;     10 sec\n",
            "[2022-02-19 10:45:38,748 INFO] Step 600/ 2000; acc:  40.12; ppl: 40.01; xent: 3.69; lr: 1.00000; 19276/20044 tok/s;     11 sec\n",
            "[2022-02-19 10:45:39,133 INFO] Step 620/ 2000; acc:  38.47; ppl: 48.68; xent: 3.89; lr: 1.00000; 17828/17935 tok/s;     11 sec\n",
            "[2022-02-19 10:45:39,220 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-19 10:45:39,393 INFO] Validation perplexity: 20.4912\n",
            "[2022-02-19 10:45:39,394 INFO] Validation accuracy: 49.9536\n",
            "[2022-02-19 10:45:39,449 INFO] Saving checkpoint ./models/nb-unite/model_step_625.pt\n",
            "[2022-02-19 10:45:39,920 INFO] Step 640/ 2000; acc:  43.45; ppl: 32.32; xent: 3.48; lr: 1.00000; 7091/7977 tok/s;     12 sec\n",
            "[2022-02-19 10:45:40,359 INFO] Step 660/ 2000; acc:  41.23; ppl: 38.28; xent: 3.64; lr: 1.00000; 14906/15048 tok/s;     12 sec\n",
            "[2022-02-19 10:45:40,721 INFO] Step 680/ 2000; acc:  44.04; ppl: 30.89; xent: 3.43; lr: 1.00000; 16780/17341 tok/s;     13 sec\n",
            "[2022-02-19 10:45:41,172 INFO] Step 700/ 2000; acc:  39.16; ppl: 40.70; xent: 3.71; lr: 1.00000; 15159/16078 tok/s;     13 sec\n",
            "[2022-02-19 10:45:41,559 INFO] Step 720/ 2000; acc:  41.25; ppl: 36.01; xent: 3.58; lr: 1.00000; 16835/17830 tok/s;     13 sec\n",
            "[2022-02-19 10:45:41,903 INFO] Step 740/ 2000; acc:  42.38; ppl: 35.35; xent: 3.57; lr: 1.00000; 19124/19811 tok/s;     14 sec\n",
            "[2022-02-19 10:45:42,270 INFO] Step 760/ 2000; acc:  40.72; ppl: 39.68; xent: 3.68; lr: 1.00000; 19700/19645 tok/s;     14 sec\n",
            "[2022-02-19 10:45:42,668 INFO] Step 780/ 2000; acc:  43.25; ppl: 31.10; xent: 3.44; lr: 1.00000; 14904/16174 tok/s;     15 sec\n",
            "[2022-02-19 10:45:42,983 INFO] Step 800/ 2000; acc:  45.20; ppl: 29.10; xent: 3.37; lr: 1.00000; 20310/20609 tok/s;     15 sec\n",
            "[2022-02-19 10:45:43,356 INFO] Step 820/ 2000; acc:  41.19; ppl: 37.85; xent: 3.63; lr: 1.00000; 18836/19820 tok/s;     15 sec\n",
            "[2022-02-19 10:45:43,756 INFO] Step 840/ 2000; acc:  46.87; ppl: 24.81; xent: 3.21; lr: 1.00000; 13084/14546 tok/s;     16 sec\n",
            "[2022-02-19 10:45:44,119 INFO] Step 860/ 2000; acc:  44.92; ppl: 27.08; xent: 3.30; lr: 1.00000; 18408/18275 tok/s;     16 sec\n",
            "[2022-02-19 10:45:44,503 INFO] Step 880/ 2000; acc:  41.08; ppl: 35.24; xent: 3.56; lr: 1.00000; 18675/19663 tok/s;     16 sec\n",
            "[2022-02-19 10:45:44,831 INFO] Step 900/ 2000; acc:  48.20; ppl: 23.03; xent: 3.14; lr: 1.00000; 16029/17839 tok/s;     17 sec\n",
            "[2022-02-19 10:45:45,163 INFO] Step 920/ 2000; acc:  44.13; ppl: 29.36; xent: 3.38; lr: 1.00000; 20438/20788 tok/s;     17 sec\n",
            "[2022-02-19 10:45:45,524 INFO] Step 940/ 2000; acc:  42.27; ppl: 35.23; xent: 3.56; lr: 1.00000; 19505/19945 tok/s;     17 sec\n",
            "[2022-02-19 10:45:45,829 INFO] Step 960/ 2000; acc:  47.23; ppl: 22.59; xent: 3.12; lr: 1.00000; 18618/20693 tok/s;     18 sec\n",
            "[2022-02-19 10:45:46,236 INFO] Step 980/ 2000; acc:  45.30; ppl: 26.14; xent: 3.26; lr: 1.00000; 16076/16380 tok/s;     18 sec\n",
            "[2022-02-19 10:45:46,560 INFO] Step 1000/ 2000; acc:  47.69; ppl: 24.15; xent: 3.18; lr: 1.00000; 19147/19481 tok/s;     18 sec\n",
            "[2022-02-19 10:45:46,918 INFO] Step 1020/ 2000; acc:  42.14; ppl: 29.68; xent: 3.39; lr: 1.00000; 19129/19263 tok/s;     19 sec\n",
            "[2022-02-19 10:45:47,288 INFO] Step 1040/ 2000; acc:  44.68; ppl: 25.90; xent: 3.25; lr: 1.00000; 17830/18355 tok/s;     19 sec\n",
            "[2022-02-19 10:45:47,606 INFO] Step 1060/ 2000; acc:  45.87; ppl: 25.31; xent: 3.23; lr: 1.00000; 20844/20769 tok/s;     19 sec\n",
            "[2022-02-19 10:45:47,978 INFO] Step 1080/ 2000; acc:  43.14; ppl: 30.57; xent: 3.42; lr: 1.00000; 19593/19914 tok/s;     20 sec\n",
            "[2022-02-19 10:45:48,334 INFO] Step 1100/ 2000; acc:  45.39; ppl: 25.68; xent: 3.25; lr: 1.00000; 16949/17779 tok/s;     20 sec\n",
            "[2022-02-19 10:45:48,668 INFO] Step 1120/ 2000; acc:  47.34; ppl: 22.26; xent: 3.10; lr: 1.00000; 19231/19791 tok/s;     21 sec\n",
            "[2022-02-19 10:45:49,073 INFO] Step 1140/ 2000; acc:  44.00; ppl: 27.97; xent: 3.33; lr: 1.00000; 17377/17961 tok/s;     21 sec\n",
            "[2022-02-19 10:45:49,471 INFO] Step 1160/ 2000; acc:  48.48; ppl: 21.40; xent: 3.06; lr: 1.00000; 13253/14978 tok/s;     21 sec\n",
            "[2022-02-19 10:45:49,873 INFO] Step 1180/ 2000; acc:  46.08; ppl: 23.84; xent: 3.17; lr: 1.00000; 16284/17225 tok/s;     22 sec\n",
            "[2022-02-19 10:45:50,311 INFO] Step 1200/ 2000; acc:  44.54; ppl: 27.97; xent: 3.33; lr: 1.00000; 16574/16807 tok/s;     22 sec\n",
            "[2022-02-19 10:45:50,548 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-19 10:45:50,657 INFO] Step 1220/ 2000; acc:  51.81; ppl: 17.10; xent: 2.84; lr: 1.00000; 14957/16615 tok/s;     23 sec\n",
            "[2022-02-19 10:45:51,007 INFO] Step 1240/ 2000; acc:  46.65; ppl: 22.89; xent: 3.13; lr: 1.00000; 19346/19823 tok/s;     23 sec\n",
            "[2022-02-19 10:45:51,322 INFO] Validation perplexity: 12.4398\n",
            "[2022-02-19 10:45:51,322 INFO] Validation accuracy: 56.3978\n",
            "[2022-02-19 10:45:51,371 INFO] Saving checkpoint ./models/nb-unite/model_step_1250.pt\n",
            "[2022-02-19 10:45:51,859 INFO] Step 1260/ 2000; acc:  45.51; ppl: 25.82; xent: 3.25; lr: 1.00000; 8015/8174 tok/s;     24 sec\n",
            "[2022-02-19 10:45:52,171 INFO] Step 1280/ 2000; acc:  50.66; ppl: 18.42; xent: 2.91; lr: 1.00000; 18108/20215 tok/s;     24 sec\n",
            "[2022-02-19 10:45:52,586 INFO] Step 1300/ 2000; acc:  46.17; ppl: 22.57; xent: 3.12; lr: 1.00000; 15933/16056 tok/s;     24 sec\n",
            "[2022-02-19 10:45:52,901 INFO] Step 1320/ 2000; acc:  50.87; ppl: 17.24; xent: 2.85; lr: 1.00000; 19531/20062 tok/s;     25 sec\n",
            "[2022-02-19 10:45:53,302 INFO] Step 1340/ 2000; acc:  45.66; ppl: 23.50; xent: 3.16; lr: 1.00000; 17219/18170 tok/s;     25 sec\n",
            "[2022-02-19 10:45:53,686 INFO] Step 1360/ 2000; acc:  47.62; ppl: 21.30; xent: 3.06; lr: 1.00000; 16969/17881 tok/s;     26 sec\n",
            "[2022-02-19 10:45:54,036 INFO] Step 1380/ 2000; acc:  48.11; ppl: 21.82; xent: 3.08; lr: 1.00000; 18690/19477 tok/s;     26 sec\n",
            "[2022-02-19 10:45:54,405 INFO] Step 1400/ 2000; acc:  47.45; ppl: 23.63; xent: 3.16; lr: 1.00000; 19572/19553 tok/s;     26 sec\n",
            "[2022-02-19 10:45:54,770 INFO] Step 1420/ 2000; acc:  49.13; ppl: 19.27; xent: 2.96; lr: 1.00000; 16087/17101 tok/s;     27 sec\n",
            "[2022-02-19 10:45:55,102 INFO] Step 1440/ 2000; acc:  51.38; ppl: 16.28; xent: 2.79; lr: 1.00000; 18699/19855 tok/s;     27 sec\n",
            "[2022-02-19 10:45:55,458 INFO] Step 1460/ 2000; acc:  47.70; ppl: 21.50; xent: 3.07; lr: 1.00000; 19377/20312 tok/s;     27 sec\n",
            "[2022-02-19 10:45:55,795 INFO] Step 1480/ 2000; acc:  53.76; ppl: 15.03; xent: 2.71; lr: 1.00000; 15451/17395 tok/s;     28 sec\n",
            "[2022-02-19 10:45:56,120 INFO] Step 1500/ 2000; acc:  50.27; ppl: 18.26; xent: 2.90; lr: 1.00000; 20680/20748 tok/s;     28 sec\n",
            "[2022-02-19 10:45:56,494 INFO] Step 1520/ 2000; acc:  46.97; ppl: 21.74; xent: 3.08; lr: 1.00000; 19324/20108 tok/s;     28 sec\n",
            "[2022-02-19 10:45:56,839 INFO] Step 1540/ 2000; acc:  53.54; ppl: 14.59; xent: 2.68; lr: 1.00000; 15346/17058 tok/s;     29 sec\n",
            "[2022-02-19 10:45:57,175 INFO] Step 1560/ 2000; acc:  49.88; ppl: 17.84; xent: 2.88; lr: 1.00000; 20284/20882 tok/s;     29 sec\n",
            "[2022-02-19 10:45:57,534 INFO] Step 1580/ 2000; acc:  46.95; ppl: 21.91; xent: 3.09; lr: 1.00000; 19824/20435 tok/s;     29 sec\n",
            "[2022-02-19 10:45:57,840 INFO] Step 1600/ 2000; acc:  53.60; ppl: 13.75; xent: 2.62; lr: 1.00000; 18991/19649 tok/s;     30 sec\n",
            "[2022-02-19 10:45:58,244 INFO] Step 1620/ 2000; acc:  50.40; ppl: 16.78; xent: 2.82; lr: 1.00000; 16344/16745 tok/s;     30 sec\n",
            "[2022-02-19 10:45:58,554 INFO] Step 1640/ 2000; acc:  51.17; ppl: 16.49; xent: 2.80; lr: 1.00000; 20023/20346 tok/s;     30 sec\n",
            "[2022-02-19 10:45:58,916 INFO] Step 1660/ 2000; acc:  48.17; ppl: 19.98; xent: 2.99; lr: 1.00000; 19184/19220 tok/s;     31 sec\n",
            "[2022-02-19 10:45:59,402 INFO] Step 1680/ 2000; acc:  51.20; ppl: 16.53; xent: 2.81; lr: 1.00000; 13383/14103 tok/s;     31 sec\n",
            "[2022-02-19 10:45:59,789 INFO] Step 1700/ 2000; acc:  52.05; ppl: 16.25; xent: 2.79; lr: 1.00000; 17005/16645 tok/s;     32 sec\n",
            "[2022-02-19 10:46:00,231 INFO] Step 1720/ 2000; acc:  47.94; ppl: 19.60; xent: 2.98; lr: 1.00000; 16071/16453 tok/s;     32 sec\n",
            "[2022-02-19 10:46:00,600 INFO] Step 1740/ 2000; acc:  49.88; ppl: 16.75; xent: 2.82; lr: 1.00000; 16223/17163 tok/s;     32 sec\n",
            "[2022-02-19 10:46:00,928 INFO] Step 1760/ 2000; acc:  51.59; ppl: 15.59; xent: 2.75; lr: 1.00000; 19550/20069 tok/s;     33 sec\n",
            "[2022-02-19 10:46:01,307 INFO] Step 1780/ 2000; acc:  49.57; ppl: 16.83; xent: 2.82; lr: 1.00000; 18550/19595 tok/s;     33 sec\n",
            "[2022-02-19 10:46:01,645 INFO] Step 1800/ 2000; acc:  53.80; ppl: 13.56; xent: 2.61; lr: 1.00000; 15610/17315 tok/s;     34 sec\n",
            "[2022-02-19 10:46:01,994 INFO] Step 1820/ 2000; acc:  50.80; ppl: 15.68; xent: 2.75; lr: 1.00000; 18695/20008 tok/s;     34 sec\n",
            "[2022-02-19 10:46:02,360 INFO] Step 1840/ 2000; acc:  49.10; ppl: 18.53; xent: 2.92; lr: 1.00000; 19576/19814 tok/s;     34 sec\n",
            "[2022-02-19 10:46:02,590 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-19 10:46:02,698 INFO] Step 1860/ 2000; acc:  56.02; ppl: 11.69; xent: 2.46; lr: 1.00000; 15329/16860 tok/s;     35 sec\n",
            "[2022-02-19 10:46:03,123 INFO] Validation perplexity: 9.47215\n",
            "[2022-02-19 10:46:03,123 INFO] Validation accuracy: 59.4344\n",
            "[2022-02-19 10:46:03,173 INFO] Saving checkpoint ./models/nb-unite/model_step_1875.pt\n",
            "[2022-02-19 10:46:03,519 INFO] Step 1880/ 2000; acc:  51.13; ppl: 15.80; xent: 2.76; lr: 1.00000; 8331/8344 tok/s;     35 sec\n",
            "[2022-02-19 10:46:03,893 INFO] Step 1900/ 2000; acc:  49.87; ppl: 17.65; xent: 2.87; lr: 1.00000; 18531/19264 tok/s;     36 sec\n",
            "[2022-02-19 10:46:04,244 INFO] Step 1920/ 2000; acc:  54.57; ppl: 12.16; xent: 2.50; lr: 1.00000; 16182/18075 tok/s;     36 sec\n",
            "[2022-02-19 10:46:04,652 INFO] Step 1940/ 2000; acc:  51.06; ppl: 15.72; xent: 2.76; lr: 1.00000; 16292/16613 tok/s;     37 sec\n",
            "[2022-02-19 10:46:04,968 INFO] Step 1960/ 2000; acc:  53.28; ppl: 13.75; xent: 2.62; lr: 1.00000; 19535/20350 tok/s;     37 sec\n",
            "[2022-02-19 10:46:05,370 INFO] Step 1980/ 2000; acc:  50.47; ppl: 16.12; xent: 2.78; lr: 1.00000; 17321/17962 tok/s;     37 sec\n",
            "[2022-02-19 10:46:05,753 INFO] Step 2000/ 2000; acc:  52.26; ppl: 14.41; xent: 2.67; lr: 1.00000; 17054/17943 tok/s;     38 sec\n",
            "[2022-02-19 10:46:05,807 INFO] Saving checkpoint ./models/nb-unite/model_step_2000.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-nb-unite.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okPst5huliAr",
        "outputId": "5679f371-c736-4880-ec0a-ebef4da4570c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-19 10:56:50,912 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 10:56:53,249 INFO] PRED AVG SCORE: -1.0046, PRED PPL: 2.7308\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -beam_size 3 -model models/base/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/beam/pred_2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA25-upSlzpL",
        "outputId": "09c4e1a5-0720-4003-eb42-d93e394a2efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 18.92, 54.7/26.0/16.2/8.0 (BP=0.914, ratio=0.917, hyp_len=3279, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/beam/pred_2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgJBNn62nRhG"
      },
      "source": [
        "On observe le meilleur score BLEU pour beam_size = 3.\n",
        "Le score BLEU augmente entre 1 et 3 puis diminue entre 3 et 10. (sauf une valeur étrange pour beam_size = 6 où le score BLEU est de 18,24% contre 18,40% pour beam_size = 7).\n",
        "\n",
        "<span style=\"color:red\"> Pourquoi? </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeiwNAcdDAhr"
      },
      "source": [
        "**Q16** Quelle est le meilleur score BLEU que vous pouvez obtenir sur dev ? Maintenant, vous pouvez tester votre meilleur modèle en calculant son score BLEU sur l'ensemble de test. Quel résultat vous obtenez?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndHKOgzWoTz6"
      },
      "source": [
        "On prend un nombre de steps de 20000 pour être large et on ajoute l'option early_stopping à 3. On met 3 couches pour l'encodeur et 1 couche pour le décodeur, 512 unités pour l'encodeur et pour le décodeur et on met la taille du beam à 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so2CXf8GlDx6",
        "outputId": "5da9688f-bcb5-46e3-ab6e-5445a2c1c44e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 11:15:23,006 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-19 11:15:23,007 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-19 11:15:23,007 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-19 11:15:23,008 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-19 11:15:23,008 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-19 11:15:23,008 INFO] Loading vocab from text file...\n",
            "[2022-02-19 11:15:23,008 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-19 11:15:23,028 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-19 11:15:23,033 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-19 11:15:23,049 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-19 11:15:23,053 INFO] Building fields with vocab in counters...\n",
            "[2022-02-19 11:15:23,061 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-19 11:15:23,074 INFO]  * src vocab size: 9980.\n",
            "[2022-02-19 11:15:23,074 INFO]  * src vocab size = 9980\n",
            "[2022-02-19 11:15:23,074 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-19 11:15:23,077 INFO] Building model...\n",
            "[2022-02-19 11:15:27,246 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 256, num_layers=3, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1012, 512)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-19 11:15:27,246 INFO] encoder: 9696304\n",
            "[2022-02-19 11:15:27,247 INFO] decoder: 11429822\n",
            "[2022-02-19 11:15:27,247 INFO] * number of parameters: 21126126\n",
            "[2022-02-19 11:15:27,248 INFO] Starting training on GPU: [0]\n",
            "[2022-02-19 11:15:27,249 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-19 11:15:27,249 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-19 11:15:27,249 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-19 11:15:27,787 INFO] Step 20/20000; acc:   8.73; ppl: 4363.06; xent: 8.38; lr: 1.00000; 10677/11188 tok/s;      1 sec\n",
            "[2022-02-19 11:15:28,316 INFO] Step 40/20000; acc:  12.59; ppl: 696.26; xent: 6.55; lr: 1.00000; 13588/14055 tok/s;      1 sec\n",
            "[2022-02-19 11:15:28,789 INFO] Step 60/20000; acc:  16.85; ppl: 316.41; xent: 5.76; lr: 1.00000; 14101/14992 tok/s;      2 sec\n",
            "[2022-02-19 11:15:29,229 INFO] Step 80/20000; acc:  19.99; ppl: 230.19; xent: 5.44; lr: 1.00000; 13517/14192 tok/s;      2 sec\n",
            "[2022-02-19 11:15:29,694 INFO] Step 100/20000; acc:  19.36; ppl: 207.90; xent: 5.34; lr: 1.00000; 14834/15060 tok/s;      2 sec\n",
            "[2022-02-19 11:15:30,160 INFO] Step 120/20000; acc:  21.70; ppl: 164.68; xent: 5.10; lr: 1.00000; 14937/15556 tok/s;      3 sec\n",
            "[2022-02-19 11:15:30,574 INFO] Step 140/20000; acc:  28.58; ppl: 109.56; xent: 4.70; lr: 1.00000; 13907/14676 tok/s;      3 sec\n",
            "[2022-02-19 11:15:31,005 INFO] Step 160/20000; acc:  26.94; ppl: 136.76; xent: 4.92; lr: 1.00000; 15998/15851 tok/s;      4 sec\n",
            "[2022-02-19 11:15:31,451 INFO] Step 180/20000; acc:  27.04; ppl: 115.62; xent: 4.75; lr: 1.00000; 15280/16755 tok/s;      4 sec\n",
            "[2022-02-19 11:15:31,876 INFO] Step 200/20000; acc:  30.97; ppl: 93.88; xent: 4.54; lr: 1.00000; 14562/14581 tok/s;      5 sec\n",
            "[2022-02-19 11:15:32,316 INFO] Step 220/20000; acc:  31.63; ppl: 88.09; xent: 4.48; lr: 1.00000; 13951/15039 tok/s;      5 sec\n",
            "[2022-02-19 11:15:32,738 INFO] Step 240/20000; acc:  31.86; ppl: 83.31; xent: 4.42; lr: 1.00000; 15938/16227 tok/s;      5 sec\n",
            "[2022-02-19 11:15:33,164 INFO] Step 260/20000; acc:  34.15; ppl: 68.88; xent: 4.23; lr: 1.00000; 13441/15029 tok/s;      6 sec\n",
            "[2022-02-19 11:15:33,609 INFO] Step 280/20000; acc:  35.08; ppl: 72.62; xent: 4.29; lr: 1.00000; 13953/14356 tok/s;      6 sec\n",
            "[2022-02-19 11:15:34,037 INFO] Step 300/20000; acc:  31.97; ppl: 74.50; xent: 4.31; lr: 1.00000; 16493/16676 tok/s;      7 sec\n",
            "[2022-02-19 11:15:34,436 INFO] Step 320/20000; acc:  34.95; ppl: 58.19; xent: 4.06; lr: 1.00000; 14702/16712 tok/s;      7 sec\n",
            "[2022-02-19 11:15:34,898 INFO] Step 340/20000; acc:  39.70; ppl: 47.03; xent: 3.85; lr: 1.00000; 12716/12992 tok/s;      8 sec\n",
            "[2022-02-19 11:15:35,348 INFO] Step 360/20000; acc:  32.81; ppl: 72.71; xent: 4.29; lr: 1.00000; 16161/16150 tok/s;      8 sec\n",
            "[2022-02-19 11:15:35,796 INFO] Step 380/20000; acc:  34.28; ppl: 62.85; xent: 4.14; lr: 1.00000; 15143/15762 tok/s;      9 sec\n",
            "[2022-02-19 11:15:36,200 INFO] Step 400/20000; acc:  40.14; ppl: 43.57; xent: 3.77; lr: 1.00000; 14869/14762 tok/s;      9 sec\n",
            "[2022-02-19 11:15:36,642 INFO] Step 420/20000; acc:  34.85; ppl: 65.57; xent: 4.18; lr: 1.00000; 15564/15852 tok/s;      9 sec\n",
            "[2022-02-19 11:15:37,088 INFO] Step 440/20000; acc:  35.88; ppl: 54.53; xent: 4.00; lr: 1.00000; 15804/16188 tok/s;     10 sec\n",
            "[2022-02-19 11:15:37,496 INFO] Step 460/20000; acc:  38.65; ppl: 46.57; xent: 3.84; lr: 1.00000; 14192/14548 tok/s;     10 sec\n",
            "[2022-02-19 11:15:37,940 INFO] Step 480/20000; acc:  35.54; ppl: 54.23; xent: 3.99; lr: 1.00000; 15158/15816 tok/s;     11 sec\n",
            "[2022-02-19 11:15:38,382 INFO] Step 500/20000; acc:  37.36; ppl: 48.77; xent: 3.89; lr: 1.00000; 15454/16411 tok/s;     11 sec\n",
            "[2022-02-19 11:15:38,840 INFO] Step 520/20000; acc:  39.82; ppl: 43.66; xent: 3.78; lr: 1.00000; 13466/13350 tok/s;     12 sec\n",
            "[2022-02-19 11:15:39,261 INFO] Step 540/20000; acc:  39.77; ppl: 46.80; xent: 3.85; lr: 1.00000; 14712/15849 tok/s;     12 sec\n",
            "[2022-02-19 11:15:39,696 INFO] Step 560/20000; acc:  38.36; ppl: 46.16; xent: 3.83; lr: 1.00000; 15856/16178 tok/s;     12 sec\n",
            "[2022-02-19 11:15:40,030 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-19 11:15:40,144 INFO] Step 580/20000; acc:  40.99; ppl: 37.51; xent: 3.62; lr: 1.00000; 12856/14382 tok/s;     13 sec\n",
            "[2022-02-19 11:15:40,580 INFO] Step 600/20000; acc:  41.67; ppl: 39.68; xent: 3.68; lr: 1.00000; 14165/14169 tok/s;     13 sec\n",
            "[2022-02-19 11:15:41,017 INFO] Step 620/20000; acc:  38.61; ppl: 46.74; xent: 3.84; lr: 1.00000; 16088/16580 tok/s;     14 sec\n",
            "[2022-02-19 11:15:41,126 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-19 11:15:41,317 INFO] Validation perplexity: 24.8066\n",
            "[2022-02-19 11:15:41,318 INFO] Validation accuracy: 45.6653\n",
            "[2022-02-19 11:15:41,318 INFO] Model is improving ppl: inf --> 24.8066.\n",
            "[2022-02-19 11:15:41,318 INFO] Model is improving acc: -inf --> 45.6653.\n",
            "[2022-02-19 11:15:41,367 INFO] Saving checkpoint ./models/best/model_step_625.pt\n",
            "[2022-02-19 11:15:41,952 INFO] Step 640/20000; acc:  41.93; ppl: 34.59; xent: 3.54; lr: 1.00000; 6256/7106 tok/s;     15 sec\n",
            "[2022-02-19 11:15:42,401 INFO] Step 660/20000; acc:  45.53; ppl: 29.19; xent: 3.37; lr: 1.00000; 12749/13422 tok/s;     15 sec\n",
            "[2022-02-19 11:15:42,897 INFO] Step 680/20000; acc:  37.98; ppl: 44.78; xent: 3.80; lr: 1.00000; 14399/14484 tok/s;     16 sec\n",
            "[2022-02-19 11:15:43,348 INFO] Step 700/20000; acc:  40.07; ppl: 36.82; xent: 3.61; lr: 1.00000; 14620/15798 tok/s;     16 sec\n",
            "[2022-02-19 11:15:43,785 INFO] Step 720/20000; acc:  43.86; ppl: 30.88; xent: 3.43; lr: 1.00000; 13726/14226 tok/s;     17 sec\n",
            "[2022-02-19 11:15:44,245 INFO] Step 740/20000; acc:  41.24; ppl: 38.86; xent: 3.66; lr: 1.00000; 15193/15474 tok/s;     17 sec\n",
            "[2022-02-19 11:15:44,707 INFO] Step 760/20000; acc:  40.91; ppl: 36.81; xent: 3.61; lr: 1.00000; 15342/15739 tok/s;     17 sec\n",
            "[2022-02-19 11:15:45,127 INFO] Step 780/20000; acc:  45.49; ppl: 28.66; xent: 3.36; lr: 1.00000; 13690/14334 tok/s;     18 sec\n",
            "[2022-02-19 11:15:45,554 INFO] Step 800/20000; acc:  41.39; ppl: 39.13; xent: 3.67; lr: 1.00000; 15721/15974 tok/s;     18 sec\n",
            "[2022-02-19 11:15:45,986 INFO] Step 820/20000; acc:  42.08; ppl: 33.74; xent: 3.52; lr: 1.00000; 15533/16530 tok/s;     19 sec\n",
            "[2022-02-19 11:15:46,428 INFO] Step 840/20000; acc:  46.53; ppl: 26.22; xent: 3.27; lr: 1.00000; 13643/14561 tok/s;     19 sec\n",
            "[2022-02-19 11:15:46,864 INFO] Step 860/20000; acc:  45.34; ppl: 28.83; xent: 3.36; lr: 1.00000; 14299/15023 tok/s;     20 sec\n",
            "[2022-02-19 11:15:47,282 INFO] Step 880/20000; acc:  43.15; ppl: 30.59; xent: 3.42; lr: 1.00000; 16213/16646 tok/s;     20 sec\n",
            "[2022-02-19 11:15:47,712 INFO] Step 900/20000; acc:  44.94; ppl: 26.15; xent: 3.26; lr: 1.00000; 13444/15081 tok/s;     20 sec\n",
            "[2022-02-19 11:15:48,191 INFO] Step 920/20000; acc:  45.17; ppl: 29.59; xent: 3.39; lr: 1.00000; 13127/13765 tok/s;     21 sec\n",
            "[2022-02-19 11:15:48,749 INFO] Step 940/20000; acc:  42.31; ppl: 32.52; xent: 3.48; lr: 1.00000; 12866/12766 tok/s;     22 sec\n",
            "[2022-02-19 11:15:49,232 INFO] Step 960/20000; acc:  46.31; ppl: 25.01; xent: 3.22; lr: 1.00000; 12314/13539 tok/s;     22 sec\n",
            "[2022-02-19 11:15:49,745 INFO] Step 980/20000; acc:  48.38; ppl: 22.44; xent: 3.11; lr: 1.00000; 11343/11756 tok/s;     22 sec\n",
            "[2022-02-19 11:15:50,203 INFO] Step 1000/20000; acc:  42.48; ppl: 30.92; xent: 3.43; lr: 1.00000; 15550/15304 tok/s;     23 sec\n",
            "[2022-02-19 11:15:50,638 INFO] Step 1020/20000; acc:  45.27; ppl: 24.95; xent: 3.22; lr: 1.00000; 15363/16245 tok/s;     23 sec\n",
            "[2022-02-19 11:15:51,069 INFO] Step 1040/20000; acc:  48.49; ppl: 21.44; xent: 3.07; lr: 1.00000; 14036/13957 tok/s;     24 sec\n",
            "[2022-02-19 11:15:51,528 INFO] Step 1060/20000; acc:  43.25; ppl: 31.73; xent: 3.46; lr: 1.00000; 15383/15536 tok/s;     24 sec\n",
            "[2022-02-19 11:15:51,979 INFO] Step 1080/20000; acc:  44.13; ppl: 26.72; xent: 3.29; lr: 1.00000; 15825/16186 tok/s;     25 sec\n",
            "[2022-02-19 11:15:52,394 INFO] Step 1100/20000; acc:  49.39; ppl: 21.73; xent: 3.08; lr: 1.00000; 13981/14350 tok/s;     25 sec\n",
            "[2022-02-19 11:15:52,822 INFO] Step 1120/20000; acc:  45.23; ppl: 26.56; xent: 3.28; lr: 1.00000; 15575/16211 tok/s;     26 sec\n",
            "[2022-02-19 11:15:53,272 INFO] Step 1140/20000; acc:  44.94; ppl: 25.71; xent: 3.25; lr: 1.00000; 15080/16253 tok/s;     26 sec\n",
            "[2022-02-19 11:15:53,722 INFO] Step 1160/20000; acc:  47.42; ppl: 23.30; xent: 3.15; lr: 1.00000; 13626/13623 tok/s;     26 sec\n",
            "[2022-02-19 11:15:54,162 INFO] Step 1180/20000; acc:  47.65; ppl: 23.60; xent: 3.16; lr: 1.00000; 13945/15258 tok/s;     27 sec\n",
            "[2022-02-19 11:15:54,600 INFO] Step 1200/20000; acc:  46.43; ppl: 23.93; xent: 3.18; lr: 1.00000; 15494/16037 tok/s;     27 sec\n",
            "[2022-02-19 11:15:54,919 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-19 11:15:55,034 INFO] Step 1220/20000; acc:  48.65; ppl: 19.99; xent: 3.00; lr: 1.00000; 13262/14531 tok/s;     28 sec\n",
            "[2022-02-19 11:15:55,441 INFO] Step 1240/20000; acc:  47.41; ppl: 23.31; xent: 3.15; lr: 1.00000; 15088/15310 tok/s;     28 sec\n",
            "[2022-02-19 11:15:55,866 INFO] Validation perplexity: 12.9372\n",
            "[2022-02-19 11:15:55,866 INFO] Validation accuracy: 54.8911\n",
            "[2022-02-19 11:15:55,866 INFO] Model is improving ppl: 24.8066 --> 12.9372.\n",
            "[2022-02-19 11:15:55,866 INFO] Model is improving acc: 45.6653 --> 54.8911.\n",
            "[2022-02-19 11:15:55,916 INFO] Saving checkpoint ./models/best/model_step_1250.pt\n",
            "[2022-02-19 11:15:56,436 INFO] Step 1260/20000; acc:  44.91; ppl: 25.73; xent: 3.25; lr: 1.00000; 7149/7296 tok/s;     29 sec\n",
            "[2022-02-19 11:15:56,868 INFO] Step 1280/20000; acc:  49.35; ppl: 19.00; xent: 2.94; lr: 1.00000; 13651/15385 tok/s;     30 sec\n",
            "[2022-02-19 11:15:57,334 INFO] Step 1300/20000; acc:  51.88; ppl: 17.36; xent: 2.85; lr: 1.00000; 12427/13158 tok/s;     30 sec\n",
            "[2022-02-19 11:15:57,833 INFO] Step 1320/20000; acc:  45.32; ppl: 24.86; xent: 3.21; lr: 1.00000; 14441/14659 tok/s;     31 sec\n",
            "[2022-02-19 11:15:58,284 INFO] Step 1340/20000; acc:  47.00; ppl: 20.35; xent: 3.01; lr: 1.00000; 14838/15589 tok/s;     31 sec\n",
            "[2022-02-19 11:15:58,716 INFO] Step 1360/20000; acc:  50.62; ppl: 17.27; xent: 2.85; lr: 1.00000; 13872/14321 tok/s;     31 sec\n",
            "[2022-02-19 11:15:59,176 INFO] Step 1380/20000; acc:  47.25; ppl: 23.30; xent: 3.15; lr: 1.00000; 15159/15311 tok/s;     32 sec\n",
            "[2022-02-19 11:15:59,635 INFO] Step 1400/20000; acc:  47.62; ppl: 22.31; xent: 3.11; lr: 1.00000; 15357/16041 tok/s;     32 sec\n",
            "[2022-02-19 11:16:00,069 INFO] Step 1420/20000; acc:  52.97; ppl: 16.66; xent: 2.81; lr: 1.00000; 13070/13581 tok/s;     33 sec\n",
            "[2022-02-19 11:16:00,519 INFO] Step 1440/20000; acc:  48.45; ppl: 21.50; xent: 3.07; lr: 1.00000; 14545/15344 tok/s;     33 sec\n",
            "[2022-02-19 11:16:00,946 INFO] Step 1460/20000; acc:  48.57; ppl: 19.10; xent: 2.95; lr: 1.00000; 15492/16109 tok/s;     34 sec\n",
            "[2022-02-19 11:16:01,380 INFO] Step 1480/20000; acc:  50.17; ppl: 17.45; xent: 2.86; lr: 1.00000; 13746/15079 tok/s;     34 sec\n",
            "[2022-02-19 11:16:01,809 INFO] Step 1500/20000; acc:  51.56; ppl: 17.64; xent: 2.87; lr: 1.00000; 14598/15212 tok/s;     35 sec\n",
            "[2022-02-19 11:16:02,241 INFO] Step 1520/20000; acc:  48.88; ppl: 19.99; xent: 3.00; lr: 1.00000; 15857/16299 tok/s;     35 sec\n",
            "[2022-02-19 11:16:02,684 INFO] Step 1540/20000; acc:  51.36; ppl: 15.73; xent: 2.76; lr: 1.00000; 13184/14697 tok/s;     35 sec\n",
            "[2022-02-19 11:16:03,135 INFO] Step 1560/20000; acc:  50.40; ppl: 18.03; xent: 2.89; lr: 1.00000; 14140/14613 tok/s;     36 sec\n",
            "[2022-02-19 11:16:03,675 INFO] Step 1580/20000; acc:  47.83; ppl: 19.62; xent: 2.98; lr: 1.00000; 13407/13181 tok/s;     36 sec\n",
            "[2022-02-19 11:16:04,158 INFO] Step 1600/20000; acc:  50.41; ppl: 16.73; xent: 2.82; lr: 1.00000; 12491/13486 tok/s;     37 sec\n",
            "[2022-02-19 11:16:04,717 INFO] Step 1620/20000; acc:  53.58; ppl: 14.49; xent: 2.67; lr: 1.00000; 10396/10738 tok/s;     37 sec\n",
            "[2022-02-19 11:16:05,195 INFO] Step 1640/20000; acc:  46.92; ppl: 22.31; xent: 3.10; lr: 1.00000; 15159/14891 tok/s;     38 sec\n",
            "[2022-02-19 11:16:05,631 INFO] Step 1660/20000; acc:  49.99; ppl: 16.33; xent: 2.79; lr: 1.00000; 15385/16427 tok/s;     38 sec\n",
            "[2022-02-19 11:16:06,056 INFO] Step 1680/20000; acc:  53.31; ppl: 15.06; xent: 2.71; lr: 1.00000; 14136/14073 tok/s;     39 sec\n",
            "[2022-02-19 11:16:06,521 INFO] Step 1700/20000; acc:  49.12; ppl: 18.73; xent: 2.93; lr: 1.00000; 14824/15106 tok/s;     39 sec\n",
            "[2022-02-19 11:16:06,976 INFO] Step 1720/20000; acc:  48.72; ppl: 18.09; xent: 2.90; lr: 1.00000; 15451/16021 tok/s;     40 sec\n",
            "[2022-02-19 11:16:07,400 INFO] Step 1740/20000; acc:  53.23; ppl: 14.24; xent: 2.66; lr: 1.00000; 13563/13766 tok/s;     40 sec\n",
            "[2022-02-19 11:16:07,846 INFO] Step 1760/20000; acc:  50.05; ppl: 17.33; xent: 2.85; lr: 1.00000; 15005/15533 tok/s;     41 sec\n",
            "[2022-02-19 11:16:08,290 INFO] Step 1780/20000; acc:  50.64; ppl: 16.33; xent: 2.79; lr: 1.00000; 15330/16307 tok/s;     41 sec\n",
            "[2022-02-19 11:16:08,744 INFO] Step 1800/20000; acc:  50.43; ppl: 16.42; xent: 2.80; lr: 1.00000; 13356/13942 tok/s;     41 sec\n",
            "[2022-02-19 11:16:09,177 INFO] Step 1820/20000; acc:  51.45; ppl: 16.09; xent: 2.78; lr: 1.00000; 14012/15375 tok/s;     42 sec\n",
            "[2022-02-19 11:16:09,615 INFO] Step 1840/20000; acc:  51.14; ppl: 15.45; xent: 2.74; lr: 1.00000; 15475/15902 tok/s;     42 sec\n",
            "[2022-02-19 11:16:09,929 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-19 11:16:10,045 INFO] Step 1860/20000; acc:  53.80; ppl: 13.71; xent: 2.62; lr: 1.00000; 13415/14599 tok/s;     43 sec\n",
            "[2022-02-19 11:16:10,533 INFO] Validation perplexity: 9.30617\n",
            "[2022-02-19 11:16:10,534 INFO] Validation accuracy: 60.2921\n",
            "[2022-02-19 11:16:10,534 INFO] Model is improving ppl: 12.9372 --> 9.30617.\n",
            "[2022-02-19 11:16:10,534 INFO] Model is improving acc: 54.8911 --> 60.2921.\n",
            "[2022-02-19 11:16:10,585 INFO] Saving checkpoint ./models/best/model_step_1875.pt\n",
            "[2022-02-19 11:16:11,024 INFO] Step 1880/20000; acc:  51.76; ppl: 16.21; xent: 2.79; lr: 1.00000; 6353/6582 tok/s;     44 sec\n",
            "[2022-02-19 11:16:11,482 INFO] Step 1900/20000; acc:  50.58; ppl: 17.00; xent: 2.83; lr: 1.00000; 15624/15985 tok/s;     44 sec\n",
            "[2022-02-19 11:16:11,918 INFO] Step 1920/20000; acc:  53.32; ppl: 13.40; xent: 2.60; lr: 1.00000; 13656/14982 tok/s;     45 sec\n",
            "[2022-02-19 11:16:12,395 INFO] Step 1940/20000; acc:  55.64; ppl: 12.24; xent: 2.50; lr: 1.00000; 12206/12888 tok/s;     45 sec\n",
            "[2022-02-19 11:16:12,897 INFO] Step 1960/20000; acc:  49.20; ppl: 17.54; xent: 2.86; lr: 1.00000; 14426/14658 tok/s;     46 sec\n",
            "[2022-02-19 11:16:13,352 INFO] Step 1980/20000; acc:  51.67; ppl: 14.93; xent: 2.70; lr: 1.00000; 14778/15685 tok/s;     46 sec\n",
            "[2022-02-19 11:16:13,770 INFO] Step 2000/20000; acc:  55.82; ppl: 11.49; xent: 2.44; lr: 1.00000; 14409/14624 tok/s;     47 sec\n",
            "[2022-02-19 11:16:14,226 INFO] Step 2020/20000; acc:  50.76; ppl: 17.31; xent: 2.85; lr: 1.00000; 15267/15715 tok/s;     47 sec\n",
            "[2022-02-19 11:16:14,693 INFO] Step 2040/20000; acc:  51.22; ppl: 15.87; xent: 2.76; lr: 1.00000; 15126/15706 tok/s;     47 sec\n",
            "[2022-02-19 11:16:15,124 INFO] Step 2060/20000; acc:  55.22; ppl: 12.65; xent: 2.54; lr: 1.00000; 13219/13707 tok/s;     48 sec\n",
            "[2022-02-19 11:16:15,585 INFO] Step 2080/20000; acc:  52.59; ppl: 14.50; xent: 2.67; lr: 1.00000; 14062/14653 tok/s;     48 sec\n",
            "[2022-02-19 11:16:16,004 INFO] Step 2100/20000; acc:  53.71; ppl: 13.65; xent: 2.61; lr: 1.00000; 15688/16404 tok/s;     49 sec\n",
            "[2022-02-19 11:16:16,442 INFO] Step 2120/20000; acc:  55.56; ppl: 11.65; xent: 2.45; lr: 1.00000; 13547/14696 tok/s;     49 sec\n",
            "[2022-02-19 11:16:16,878 INFO] Step 2140/20000; acc:  53.24; ppl: 13.75; xent: 2.62; lr: 1.00000; 14217/15518 tok/s;     50 sec\n",
            "[2022-02-19 11:16:17,307 INFO] Step 2160/20000; acc:  53.20; ppl: 13.74; xent: 2.62; lr: 1.00000; 15776/16046 tok/s;     50 sec\n",
            "[2022-02-19 11:16:17,740 INFO] Step 2180/20000; acc:  55.43; ppl: 11.20; xent: 2.42; lr: 1.00000; 13476/14523 tok/s;     50 sec\n",
            "[2022-02-19 11:16:18,213 INFO] Step 2200/20000; acc:  53.05; ppl: 14.27; xent: 2.66; lr: 1.00000; 13646/14349 tok/s;     51 sec\n",
            "[2022-02-19 11:16:18,781 INFO] Step 2220/20000; acc:  51.75; ppl: 13.97; xent: 2.64; lr: 1.00000; 12849/13046 tok/s;     52 sec\n",
            "[2022-02-19 11:16:19,258 INFO] Step 2240/20000; acc:  53.92; ppl: 12.48; xent: 2.52; lr: 1.00000; 12976/13073 tok/s;     52 sec\n",
            "[2022-02-19 11:16:19,751 INFO] Step 2260/20000; acc:  57.86; ppl: 10.10; xent: 2.31; lr: 1.00000; 11765/12047 tok/s;     53 sec\n",
            "[2022-02-19 11:16:20,212 INFO] Step 2280/20000; acc:  50.63; ppl: 16.22; xent: 2.79; lr: 1.00000; 15662/15319 tok/s;     53 sec\n",
            "[2022-02-19 11:16:20,662 INFO] Step 2300/20000; acc:  54.26; ppl: 12.04; xent: 2.49; lr: 1.00000; 14861/15937 tok/s;     53 sec\n",
            "[2022-02-19 11:16:21,090 INFO] Step 2320/20000; acc:  56.42; ppl: 11.24; xent: 2.42; lr: 1.00000; 14078/14256 tok/s;     54 sec\n",
            "[2022-02-19 11:16:21,555 INFO] Step 2340/20000; acc:  51.94; ppl: 14.43; xent: 2.67; lr: 1.00000; 14873/15112 tok/s;     54 sec\n",
            "[2022-02-19 11:16:22,018 INFO] Step 2360/20000; acc:  52.33; ppl: 13.23; xent: 2.58; lr: 1.00000; 15248/16068 tok/s;     55 sec\n",
            "[2022-02-19 11:16:22,421 INFO] Step 2380/20000; acc:  57.27; ppl: 10.27; xent: 2.33; lr: 1.00000; 14203/14230 tok/s;     55 sec\n",
            "[2022-02-19 11:16:22,869 INFO] Step 2400/20000; acc:  53.41; ppl: 13.48; xent: 2.60; lr: 1.00000; 14679/15219 tok/s;     56 sec\n",
            "[2022-02-19 11:16:23,305 INFO] Step 2420/20000; acc:  54.16; ppl: 12.02; xent: 2.49; lr: 1.00000; 15396/16181 tok/s;     56 sec\n",
            "[2022-02-19 11:16:23,761 INFO] Step 2440/20000; acc:  56.14; ppl: 10.44; xent: 2.35; lr: 1.00000; 13150/14223 tok/s;     57 sec\n",
            "[2022-02-19 11:16:24,206 INFO] Step 2460/20000; acc:  55.03; ppl: 11.20; xent: 2.42; lr: 1.00000; 13760/15138 tok/s;     57 sec\n",
            "[2022-02-19 11:16:24,642 INFO] Step 2480/20000; acc:  54.00; ppl: 12.66; xent: 2.54; lr: 1.00000; 15659/15696 tok/s;     57 sec\n",
            "[2022-02-19 11:16:24,961 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 5\n",
            "[2022-02-19 11:16:25,079 INFO] Step 2500/20000; acc:  57.51; ppl: 10.05; xent: 2.31; lr: 1.00000; 13224/14450 tok/s;     58 sec\n",
            "[2022-02-19 11:16:25,282 INFO] Validation perplexity: 7.94026\n",
            "[2022-02-19 11:16:25,282 INFO] Validation accuracy: 62.1001\n",
            "[2022-02-19 11:16:25,282 INFO] Model is improving ppl: 9.30617 --> 7.94026.\n",
            "[2022-02-19 11:16:25,282 INFO] Model is improving acc: 60.2921 --> 62.1001.\n",
            "[2022-02-19 11:16:25,334 INFO] Saving checkpoint ./models/best/model_step_2500.pt\n",
            "[2022-02-19 11:16:26,083 INFO] Step 2520/20000; acc:  54.72; ppl: 12.00; xent: 2.48; lr: 1.00000; 6274/6580 tok/s;     59 sec\n",
            "[2022-02-19 11:16:26,522 INFO] Step 2540/20000; acc:  52.53; ppl: 12.82; xent: 2.55; lr: 1.00000; 16364/16774 tok/s;     59 sec\n",
            "[2022-02-19 11:16:26,959 INFO] Step 2560/20000; acc:  55.70; ppl: 10.71; xent: 2.37; lr: 1.00000; 13740/14912 tok/s;     60 sec\n",
            "[2022-02-19 11:16:27,416 INFO] Step 2580/20000; acc:  58.90; ppl:  9.16; xent: 2.22; lr: 1.00000; 12808/13640 tok/s;     60 sec\n",
            "[2022-02-19 11:16:27,901 INFO] Step 2600/20000; acc:  52.13; ppl: 14.16; xent: 2.65; lr: 1.00000; 14940/15045 tok/s;     61 sec\n",
            "[2022-02-19 11:16:28,371 INFO] Step 2620/20000; acc:  55.96; ppl: 11.14; xent: 2.41; lr: 1.00000; 14427/14963 tok/s;     61 sec\n",
            "[2022-02-19 11:16:28,839 INFO] Step 2640/20000; acc:  59.77; ppl:  8.29; xent: 2.12; lr: 1.00000; 12719/13352 tok/s;     62 sec\n",
            "[2022-02-19 11:16:29,270 INFO] Step 2660/20000; acc:  53.67; ppl: 13.30; xent: 2.59; lr: 1.00000; 15900/15804 tok/s;     62 sec\n",
            "[2022-02-19 11:16:29,743 INFO] Step 2680/20000; acc:  54.56; ppl: 11.19; xent: 2.41; lr: 1.00000; 14479/15997 tok/s;     62 sec\n",
            "[2022-02-19 11:16:30,161 INFO] Step 2700/20000; acc:  58.56; ppl:  9.79; xent: 2.28; lr: 1.00000; 13638/13760 tok/s;     63 sec\n",
            "[2022-02-19 11:16:30,620 INFO] Step 2720/20000; acc:  54.69; ppl: 11.43; xent: 2.44; lr: 1.00000; 14286/14948 tok/s;     63 sec\n",
            "[2022-02-19 11:16:31,041 INFO] Step 2740/20000; acc:  57.05; ppl: 10.06; xent: 2.31; lr: 1.00000; 15700/16139 tok/s;     64 sec\n",
            "[2022-02-19 11:16:31,481 INFO] Step 2760/20000; acc:  58.24; ppl:  9.01; xent: 2.20; lr: 1.00000; 13586/14876 tok/s;     64 sec\n",
            "[2022-02-19 11:16:31,926 INFO] Step 2780/20000; acc:  55.37; ppl: 10.65; xent: 2.37; lr: 1.00000; 14109/14777 tok/s;     65 sec\n",
            "[2022-02-19 11:16:32,358 INFO] Step 2800/20000; acc:  55.75; ppl: 10.98; xent: 2.40; lr: 1.00000; 15780/16185 tok/s;     65 sec\n",
            "[2022-02-19 11:16:32,796 INFO] Step 2820/20000; acc:  58.16; ppl:  8.89; xent: 2.19; lr: 1.00000; 13348/14657 tok/s;     66 sec\n",
            "[2022-02-19 11:16:33,273 INFO] Step 2840/20000; acc:  56.95; ppl: 10.57; xent: 2.36; lr: 1.00000; 13485/14027 tok/s;     66 sec\n",
            "[2022-02-19 11:16:33,798 INFO] Step 2860/20000; acc:  55.07; ppl: 10.44; xent: 2.35; lr: 1.00000; 13875/13982 tok/s;     67 sec\n",
            "[2022-02-19 11:16:34,291 INFO] Step 2880/20000; acc:  58.52; ppl:  9.01; xent: 2.20; lr: 1.00000; 12437/12618 tok/s;     67 sec\n",
            "[2022-02-19 11:16:34,783 INFO] Step 2900/20000; acc:  60.48; ppl:  7.61; xent: 2.03; lr: 1.00000; 11896/12254 tok/s;     68 sec\n",
            "[2022-02-19 11:16:35,254 INFO] Step 2920/20000; acc:  53.52; ppl: 13.17; xent: 2.58; lr: 1.00000; 15471/15458 tok/s;     68 sec\n",
            "[2022-02-19 11:16:35,719 INFO] Step 2940/20000; acc:  56.03; ppl:  9.95; xent: 2.30; lr: 1.00000; 14775/15316 tok/s;     68 sec\n",
            "[2022-02-19 11:16:36,141 INFO] Step 2960/20000; acc:  60.58; ppl:  8.03; xent: 2.08; lr: 1.00000; 14068/14277 tok/s;     69 sec\n",
            "[2022-02-19 11:16:36,592 INFO] Step 2980/20000; acc:  54.25; ppl: 11.01; xent: 2.40; lr: 1.00000; 14889/15390 tok/s;     69 sec\n",
            "[2022-02-19 11:16:37,056 INFO] Step 3000/20000; acc:  56.17; ppl:  9.94; xent: 2.30; lr: 1.00000; 14757/15917 tok/s;     70 sec\n",
            "[2022-02-19 11:16:37,487 INFO] Step 3020/20000; acc:  59.47; ppl:  8.69; xent: 2.16; lr: 1.00000; 13186/13335 tok/s;     70 sec\n",
            "[2022-02-19 11:16:37,928 INFO] Step 3040/20000; acc:  56.01; ppl: 10.53; xent: 2.35; lr: 1.00000; 14972/15584 tok/s;     71 sec\n",
            "[2022-02-19 11:16:38,354 INFO] Step 3060/20000; acc:  56.62; ppl:  9.46; xent: 2.25; lr: 1.00000; 15806/16403 tok/s;     71 sec\n",
            "[2022-02-19 11:16:38,611 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 6\n",
            "[2022-02-19 11:16:38,805 INFO] Step 3080/20000; acc:  59.25; ppl:  8.24; xent: 2.11; lr: 1.00000; 13360/14343 tok/s;     72 sec\n",
            "[2022-02-19 11:16:39,270 INFO] Step 3100/20000; acc:  57.93; ppl:  9.51; xent: 2.25; lr: 1.00000; 13365/14039 tok/s;     72 sec\n",
            "[2022-02-19 11:16:39,702 INFO] Step 3120/20000; acc:  55.43; ppl: 10.64; xent: 2.36; lr: 1.00000; 15843/16216 tok/s;     72 sec\n",
            "[2022-02-19 11:16:40,003 INFO] Validation perplexity: 6.58837\n",
            "[2022-02-19 11:16:40,003 INFO] Validation accuracy: 65.3686\n",
            "[2022-02-19 11:16:40,003 INFO] Model is improving ppl: 7.94026 --> 6.58837.\n",
            "[2022-02-19 11:16:40,004 INFO] Model is improving acc: 62.1001 --> 65.3686.\n",
            "[2022-02-19 11:16:40,052 INFO] Saving checkpoint ./models/best/model_step_3125.pt\n",
            "[2022-02-19 11:16:40,680 INFO] Step 3140/20000; acc:  59.90; ppl:  7.54; xent: 2.02; lr: 1.00000; 5911/6698 tok/s;     73 sec\n",
            "[2022-02-19 11:16:41,125 INFO] Step 3160/20000; acc:  58.18; ppl:  8.98; xent: 2.20; lr: 1.00000; 14081/14512 tok/s;     74 sec\n",
            "[2022-02-19 11:16:41,564 INFO] Step 3180/20000; acc:  55.70; ppl: 10.15; xent: 2.32; lr: 1.00000; 16292/16222 tok/s;     74 sec\n",
            "[2022-02-19 11:16:41,993 INFO] Step 3200/20000; acc:  58.21; ppl:  8.40; xent: 2.13; lr: 1.00000; 13888/15396 tok/s;     75 sec\n",
            "[2022-02-19 11:16:42,459 INFO] Step 3220/20000; acc:  61.24; ppl:  7.56; xent: 2.02; lr: 1.00000; 12627/13380 tok/s;     75 sec\n",
            "[2022-02-19 11:16:42,938 INFO] Step 3240/20000; acc:  53.95; ppl: 11.85; xent: 2.47; lr: 1.00000; 15395/15347 tok/s;     76 sec\n",
            "[2022-02-19 11:16:43,401 INFO] Step 3260/20000; acc:  57.63; ppl:  9.34; xent: 2.23; lr: 1.00000; 14851/15449 tok/s;     76 sec\n",
            "[2022-02-19 11:16:43,819 INFO] Step 3280/20000; acc:  61.22; ppl:  7.13; xent: 1.96; lr: 1.00000; 14142/14536 tok/s;     77 sec\n",
            "[2022-02-19 11:16:44,270 INFO] Step 3300/20000; acc:  56.16; ppl: 10.37; xent: 2.34; lr: 1.00000; 14821/15242 tok/s;     77 sec\n",
            "[2022-02-19 11:16:44,698 INFO] Step 3320/20000; acc:  57.90; ppl:  8.79; xent: 2.17; lr: 1.00000; 15701/17165 tok/s;     77 sec\n",
            "[2022-02-19 11:16:45,126 INFO] Step 3340/20000; acc:  62.50; ppl:  6.89; xent: 1.93; lr: 1.00000; 13153/13679 tok/s;     78 sec\n",
            "[2022-02-19 11:16:45,576 INFO] Step 3360/20000; acc:  58.04; ppl:  8.83; xent: 2.18; lr: 1.00000; 14756/15139 tok/s;     78 sec\n",
            "[2022-02-19 11:16:46,003 INFO] Step 3380/20000; acc:  59.08; ppl:  8.12; xent: 2.09; lr: 1.00000; 15549/16279 tok/s;     79 sec\n",
            "[2022-02-19 11:16:46,461 INFO] Step 3400/20000; acc:  60.49; ppl:  7.46; xent: 2.01; lr: 1.00000; 13135/14414 tok/s;     79 sec\n",
            "[2022-02-19 11:16:46,890 INFO] Step 3420/20000; acc:  58.79; ppl:  8.52; xent: 2.14; lr: 1.00000; 14846/15560 tok/s;     80 sec\n",
            "[2022-02-19 11:16:47,330 INFO] Step 3440/20000; acc:  57.57; ppl:  9.01; xent: 2.20; lr: 1.00000; 15714/16142 tok/s;     80 sec\n",
            "[2022-02-19 11:16:47,780 INFO] Step 3460/20000; acc:  61.47; ppl:  7.17; xent: 1.97; lr: 1.00000; 13166/13939 tok/s;     81 sec\n",
            "[2022-02-19 11:16:48,249 INFO] Step 3480/20000; acc:  59.35; ppl:  8.17; xent: 2.10; lr: 1.00000; 13401/13941 tok/s;     81 sec\n",
            "[2022-02-19 11:16:48,812 INFO] Step 3500/20000; acc:  57.23; ppl:  8.83; xent: 2.18; lr: 1.00000; 12821/12686 tok/s;     82 sec\n",
            "[2022-02-19 11:16:49,300 INFO] Step 3520/20000; acc:  61.22; ppl:  6.77; xent: 1.91; lr: 1.00000; 12374/12879 tok/s;     82 sec\n",
            "[2022-02-19 11:16:49,812 INFO] Step 3540/20000; acc:  63.24; ppl:  6.12; xent: 1.81; lr: 1.00000; 11554/12057 tok/s;     83 sec\n",
            "[2022-02-19 11:16:50,295 INFO] Step 3560/20000; acc:  54.56; ppl: 11.30; xent: 2.42; lr: 1.00000; 15501/15354 tok/s;     83 sec\n",
            "[2022-02-19 11:16:50,747 INFO] Step 3580/20000; acc:  58.01; ppl:  8.36; xent: 2.12; lr: 1.00000; 15430/15713 tok/s;     83 sec\n",
            "[2022-02-19 11:16:51,177 INFO] Step 3600/20000; acc:  62.18; ppl:  6.52; xent: 1.87; lr: 1.00000; 13774/14110 tok/s;     84 sec\n",
            "[2022-02-19 11:16:51,613 INFO] Step 3620/20000; acc:  57.34; ppl:  8.71; xent: 2.16; lr: 1.00000; 15221/15893 tok/s;     84 sec\n",
            "[2022-02-19 11:16:52,059 INFO] Step 3640/20000; acc:  58.01; ppl:  8.48; xent: 2.14; lr: 1.00000; 15227/16629 tok/s;     85 sec\n",
            "[2022-02-19 11:16:52,487 INFO] Step 3660/20000; acc:  61.62; ppl:  6.87; xent: 1.93; lr: 1.00000; 13214/13335 tok/s;     85 sec\n",
            "[2022-02-19 11:16:52,936 INFO] Step 3680/20000; acc:  58.47; ppl:  8.40; xent: 2.13; lr: 1.00000; 14626/15346 tok/s;     86 sec\n",
            "[2022-02-19 11:16:53,360 INFO] Step 3700/20000; acc:  59.31; ppl:  7.69; xent: 2.04; lr: 1.00000; 15727/16489 tok/s;     86 sec\n",
            "[2022-02-19 11:16:53,604 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 7\n",
            "[2022-02-19 11:16:53,796 INFO] Step 3720/20000; acc:  61.26; ppl:  6.92; xent: 1.93; lr: 1.00000; 13774/14845 tok/s;     87 sec\n",
            "[2022-02-19 11:16:54,230 INFO] Step 3740/20000; acc:  59.12; ppl:  7.76; xent: 2.05; lr: 1.00000; 14231/15019 tok/s;     87 sec\n",
            "[2022-02-19 11:16:54,625 INFO] Validation perplexity: 6.17105\n",
            "[2022-02-19 11:16:54,625 INFO] Validation accuracy: 66.4349\n",
            "[2022-02-19 11:16:54,625 INFO] Model is improving ppl: 6.58837 --> 6.17105.\n",
            "[2022-02-19 11:16:54,625 INFO] Model is improving acc: 65.3686 --> 66.4349.\n",
            "[2022-02-19 11:16:54,677 INFO] Saving checkpoint ./models/best/model_step_3750.pt\n",
            "[2022-02-19 11:16:55,206 INFO] Step 3760/20000; acc:  57.79; ppl:  8.83; xent: 2.18; lr: 1.00000; 7049/7204 tok/s;     88 sec\n",
            "[2022-02-19 11:16:55,673 INFO] Step 3780/20000; acc:  62.53; ppl:  6.28; xent: 1.84; lr: 1.00000; 12526/13919 tok/s;     88 sec\n",
            "[2022-02-19 11:16:56,132 INFO] Step 3800/20000; acc:  60.41; ppl:  7.68; xent: 2.04; lr: 1.00000; 13781/14384 tok/s;     89 sec\n",
            "[2022-02-19 11:16:56,590 INFO] Step 3820/20000; acc:  57.65; ppl:  8.14; xent: 2.10; lr: 1.00000; 15764/15922 tok/s;     89 sec\n",
            "[2022-02-19 11:16:57,004 INFO] Step 3840/20000; acc:  62.38; ppl:  6.41; xent: 1.86; lr: 1.00000; 14645/15401 tok/s;     90 sec\n",
            "[2022-02-19 11:16:57,472 INFO] Step 3860/20000; acc:  64.36; ppl:  5.97; xent: 1.79; lr: 1.00000; 12529/13339 tok/s;     90 sec\n",
            "[2022-02-19 11:16:57,943 INFO] Step 3880/20000; acc:  57.36; ppl:  9.46; xent: 2.25; lr: 1.00000; 15652/15642 tok/s;     91 sec\n",
            "[2022-02-19 11:16:58,419 INFO] Step 3900/20000; acc:  58.82; ppl:  8.27; xent: 2.11; lr: 1.00000; 14425/15103 tok/s;     91 sec\n",
            "[2022-02-19 11:16:58,843 INFO] Step 3920/20000; acc:  65.06; ppl:  5.62; xent: 1.73; lr: 1.00000; 13654/14287 tok/s;     92 sec\n",
            "[2022-02-19 11:16:59,293 INFO] Step 3940/20000; acc:  58.94; ppl:  8.14; xent: 2.10; lr: 1.00000; 14510/15330 tok/s;     92 sec\n",
            "[2022-02-19 11:16:59,723 INFO] Step 3960/20000; acc:  61.05; ppl:  6.84; xent: 1.92; lr: 1.00000; 15439/16023 tok/s;     92 sec\n",
            "[2022-02-19 11:17:00,135 INFO] Step 3980/20000; acc:  64.28; ppl:  5.73; xent: 1.75; lr: 1.00000; 13512/14712 tok/s;     93 sec\n",
            "[2022-02-19 11:17:00,589 INFO] Step 4000/20000; acc:  60.28; ppl:  7.61; xent: 2.03; lr: 1.00000; 14716/15032 tok/s;     93 sec\n",
            "[2022-02-19 11:17:01,019 INFO] Step 4020/20000; acc:  60.17; ppl:  7.00; xent: 1.95; lr: 1.00000; 15536/16528 tok/s;     94 sec\n",
            "[2022-02-19 11:17:01,483 INFO] Step 4040/20000; acc:  62.07; ppl:  6.28; xent: 1.84; lr: 1.00000; 13132/14264 tok/s;     94 sec\n",
            "[2022-02-19 11:17:01,917 INFO] Step 4060/20000; acc:  61.34; ppl:  7.05; xent: 1.95; lr: 1.00000; 14914/15140 tok/s;     95 sec\n",
            "[2022-02-19 11:17:02,351 INFO] Step 4080/20000; acc:  59.63; ppl:  7.53; xent: 2.02; lr: 1.00000; 16076/16279 tok/s;     95 sec\n",
            "[2022-02-19 11:17:02,797 INFO] Step 4100/20000; acc:  62.49; ppl:  6.14; xent: 1.82; lr: 1.00000; 13339/14261 tok/s;     96 sec\n",
            "[2022-02-19 11:17:03,246 INFO] Step 4120/20000; acc:  60.38; ppl:  7.29; xent: 1.99; lr: 1.00000; 14179/14529 tok/s;     96 sec\n",
            "[2022-02-19 11:17:03,781 INFO] Step 4140/20000; acc:  58.16; ppl:  8.01; xent: 2.08; lr: 1.00000; 13605/13495 tok/s;     97 sec\n",
            "[2022-02-19 11:17:04,255 INFO] Step 4160/20000; acc:  63.81; ppl:  5.70; xent: 1.74; lr: 1.00000; 12769/13350 tok/s;     97 sec\n",
            "[2022-02-19 11:17:04,824 INFO] Step 4180/20000; acc:  65.07; ppl:  5.39; xent: 1.68; lr: 1.00000; 10306/10649 tok/s;     98 sec\n",
            "[2022-02-19 11:17:05,322 INFO] Step 4200/20000; acc:  57.22; ppl:  9.08; xent: 2.21; lr: 1.00000; 14661/14783 tok/s;     98 sec\n",
            "[2022-02-19 11:17:05,777 INFO] Step 4220/20000; acc:  61.05; ppl:  6.87; xent: 1.93; lr: 1.00000; 15089/15604 tok/s;     99 sec\n",
            "[2022-02-19 11:17:06,225 INFO] Step 4240/20000; acc:  65.13; ppl:  5.24; xent: 1.66; lr: 1.00000; 13147/13349 tok/s;     99 sec\n",
            "[2022-02-19 11:17:06,676 INFO] Step 4260/20000; acc:  59.94; ppl:  7.42; xent: 2.00; lr: 1.00000; 14830/15314 tok/s;     99 sec\n",
            "[2022-02-19 11:17:07,138 INFO] Step 4280/20000; acc:  60.91; ppl:  6.69; xent: 1.90; lr: 1.00000; 14694/15982 tok/s;    100 sec\n",
            "[2022-02-19 11:17:07,584 INFO] Step 4300/20000; acc:  62.27; ppl:  6.10; xent: 1.81; lr: 1.00000; 12529/13184 tok/s;    100 sec\n",
            "[2022-02-19 11:17:08,039 INFO] Step 4320/20000; acc:  61.75; ppl:  6.85; xent: 1.92; lr: 1.00000; 14280/15069 tok/s;    101 sec\n",
            "[2022-02-19 11:17:08,459 INFO] Step 4340/20000; acc:  62.18; ppl:  6.22; xent: 1.83; lr: 1.00000; 15798/16397 tok/s;    101 sec\n",
            "[2022-02-19 11:17:08,708 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 8\n",
            "[2022-02-19 11:17:08,914 INFO] Step 4360/20000; acc:  63.97; ppl:  5.64; xent: 1.73; lr: 1.00000; 13259/14304 tok/s;    102 sec\n",
            "[2022-02-19 11:17:09,436 INFO] Validation perplexity: 6.00049\n",
            "[2022-02-19 11:17:09,436 INFO] Validation accuracy: 67.3157\n",
            "[2022-02-19 11:17:09,436 INFO] Model is improving ppl: 6.17105 --> 6.00049.\n",
            "[2022-02-19 11:17:09,436 INFO] Model is improving acc: 66.4349 --> 67.3157.\n",
            "[2022-02-19 11:17:09,486 INFO] Saving checkpoint ./models/best/model_step_4375.pt\n",
            "[2022-02-19 11:17:09,906 INFO] Step 4380/20000; acc:  60.66; ppl:  6.78; xent: 1.91; lr: 1.00000; 6342/6681 tok/s;    103 sec\n",
            "[2022-02-19 11:17:10,357 INFO] Step 4400/20000; acc:  60.19; ppl:  7.42; xent: 2.00; lr: 1.00000; 15393/15850 tok/s;    103 sec\n",
            "[2022-02-19 11:17:10,831 INFO] Step 4420/20000; acc:  65.10; ppl:  5.32; xent: 1.67; lr: 1.00000; 12410/13533 tok/s;    104 sec\n",
            "[2022-02-19 11:17:11,290 INFO] Step 4440/20000; acc:  61.91; ppl:  6.70; xent: 1.90; lr: 1.00000; 13886/14525 tok/s;    104 sec\n",
            "[2022-02-19 11:17:11,748 INFO] Step 4460/20000; acc:  59.67; ppl:  7.26; xent: 1.98; lr: 1.00000; 15829/16069 tok/s;    104 sec\n",
            "[2022-02-19 11:17:12,154 INFO] Step 4480/20000; acc:  64.56; ppl:  5.41; xent: 1.69; lr: 1.00000; 15081/15683 tok/s;    105 sec\n",
            "[2022-02-19 11:17:12,618 INFO] Step 4500/20000; acc:  67.04; ppl:  4.65; xent: 1.54; lr: 1.00000; 12663/13514 tok/s;    105 sec\n",
            "[2022-02-19 11:17:13,102 INFO] Step 4520/20000; acc:  58.33; ppl:  8.37; xent: 2.12; lr: 1.00000; 15221/15345 tok/s;    106 sec\n",
            "[2022-02-19 11:17:13,573 INFO] Step 4540/20000; acc:  60.12; ppl:  7.11; xent: 1.96; lr: 1.00000; 14604/15179 tok/s;    106 sec\n",
            "[2022-02-19 11:17:14,001 INFO] Step 4560/20000; acc:  67.04; ppl:  4.77; xent: 1.56; lr: 1.00000; 13552/14073 tok/s;    107 sec\n",
            "[2022-02-19 11:17:14,455 INFO] Step 4580/20000; acc:  61.29; ppl:  6.61; xent: 1.89; lr: 1.00000; 14155/15071 tok/s;    107 sec\n",
            "[2022-02-19 11:17:14,884 INFO] Step 4600/20000; acc:  63.31; ppl:  5.83; xent: 1.76; lr: 1.00000; 15340/15886 tok/s;    108 sec\n",
            "[2022-02-19 11:17:15,294 INFO] Step 4620/20000; acc:  67.10; ppl:  4.62; xent: 1.53; lr: 1.00000; 13477/14719 tok/s;    108 sec\n",
            "[2022-02-19 11:17:15,760 INFO] Step 4640/20000; acc:  61.80; ppl:  6.60; xent: 1.89; lr: 1.00000; 14223/14940 tok/s;    109 sec\n",
            "[2022-02-19 11:17:16,177 INFO] Step 4660/20000; acc:  62.50; ppl:  6.15; xent: 1.82; lr: 1.00000; 15934/16603 tok/s;    109 sec\n",
            "[2022-02-19 11:17:16,618 INFO] Step 4680/20000; acc:  64.52; ppl:  5.41; xent: 1.69; lr: 1.00000; 13859/14861 tok/s;    109 sec\n",
            "[2022-02-19 11:17:17,060 INFO] Step 4700/20000; acc:  62.76; ppl:  6.35; xent: 1.85; lr: 1.00000; 14881/15174 tok/s;    110 sec\n",
            "[2022-02-19 11:17:17,521 INFO] Step 4720/20000; acc:  60.59; ppl:  6.63; xent: 1.89; lr: 1.00000; 15285/15920 tok/s;    110 sec\n",
            "[2022-02-19 11:17:17,963 INFO] Step 4740/20000; acc:  64.80; ppl:  5.26; xent: 1.66; lr: 1.00000; 13750/13766 tok/s;    111 sec\n",
            "[2022-02-19 11:17:18,459 INFO] Step 4760/20000; acc:  62.85; ppl:  6.23; xent: 1.83; lr: 1.00000; 12762/13008 tok/s;    111 sec\n",
            "[2022-02-19 11:17:18,992 INFO] Step 4780/20000; acc:  60.49; ppl:  6.63; xent: 1.89; lr: 1.00000; 13662/13408 tok/s;    112 sec\n",
            "[2022-02-19 11:17:19,488 INFO] Step 4800/20000; acc:  65.47; ppl:  4.96; xent: 1.60; lr: 1.00000; 12147/12983 tok/s;    112 sec\n",
            "[2022-02-19 11:17:20,017 INFO] Step 4820/20000; acc:  65.38; ppl:  4.86; xent: 1.58; lr: 1.00000; 11123/11592 tok/s;    113 sec\n",
            "[2022-02-19 11:17:20,498 INFO] Step 4840/20000; acc:  59.14; ppl:  7.76; xent: 2.05; lr: 1.00000; 15205/15403 tok/s;    113 sec\n",
            "[2022-02-19 11:17:20,958 INFO] Step 4860/20000; acc:  62.10; ppl:  6.00; xent: 1.79; lr: 1.00000; 15013/15697 tok/s;    114 sec\n",
            "[2022-02-19 11:17:21,374 INFO] Step 4880/20000; acc:  67.78; ppl:  4.44; xent: 1.49; lr: 1.00000; 14001/14145 tok/s;    114 sec\n",
            "[2022-02-19 11:17:21,824 INFO] Step 4900/20000; acc:  62.01; ppl:  6.18; xent: 1.82; lr: 1.00000; 14577/15314 tok/s;    115 sec\n",
            "[2022-02-19 11:17:22,260 INFO] Step 4920/20000; acc:  63.07; ppl:  5.81; xent: 1.76; lr: 1.00000; 15501/16140 tok/s;    115 sec\n",
            "[2022-02-19 11:17:22,677 INFO] Step 4940/20000; acc:  66.35; ppl:  4.64; xent: 1.53; lr: 1.00000; 13259/14509 tok/s;    115 sec\n",
            "[2022-02-19 11:17:23,142 INFO] Step 4960/20000; acc:  63.88; ppl:  5.67; xent: 1.73; lr: 1.00000; 14085/15015 tok/s;    116 sec\n",
            "[2022-02-19 11:17:23,563 INFO] Step 4980/20000; acc:  63.57; ppl:  5.77; xent: 1.75; lr: 1.00000; 15879/16079 tok/s;    116 sec\n",
            "[2022-02-19 11:17:23,816 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 9\n",
            "[2022-02-19 11:17:24,028 INFO] Step 5000/20000; acc:  66.03; ppl:  4.90; xent: 1.59; lr: 1.00000; 12949/14049 tok/s;    117 sec\n",
            "[2022-02-19 11:17:24,229 INFO] Validation perplexity: 5.83275\n",
            "[2022-02-19 11:17:24,229 INFO] Validation accuracy: 67.7561\n",
            "[2022-02-19 11:17:24,229 INFO] Model is improving ppl: 6.00049 --> 5.83275.\n",
            "[2022-02-19 11:17:24,229 INFO] Model is improving acc: 67.3157 --> 67.7561.\n",
            "[2022-02-19 11:17:24,280 INFO] Saving checkpoint ./models/best/model_step_5000.pt\n",
            "[2022-02-19 11:17:25,036 INFO] Step 5020/20000; acc:  62.97; ppl:  6.01; xent: 1.79; lr: 1.00000; 6323/6712 tok/s;    118 sec\n",
            "[2022-02-19 11:17:25,473 INFO] Step 5040/20000; acc:  61.51; ppl:  6.31; xent: 1.84; lr: 1.00000; 15915/16315 tok/s;    118 sec\n",
            "[2022-02-19 11:17:25,945 INFO] Step 5060/20000; acc:  65.23; ppl:  5.15; xent: 1.64; lr: 1.00000; 12553/13703 tok/s;    119 sec\n",
            "[2022-02-19 11:17:26,386 INFO] Step 5080/20000; acc:  62.75; ppl:  5.88; xent: 1.77; lr: 1.00000; 14428/15175 tok/s;    119 sec\n",
            "[2022-02-19 11:17:26,851 INFO] Step 5100/20000; acc:  61.26; ppl:  6.28; xent: 1.84; lr: 1.00000; 15699/16000 tok/s;    120 sec\n",
            "[2022-02-19 11:17:27,266 INFO] Step 5120/20000; acc:  66.03; ppl:  4.86; xent: 1.58; lr: 1.00000; 14903/15074 tok/s;    120 sec\n",
            "[2022-02-19 11:17:27,727 INFO] Step 5140/20000; acc:  68.52; ppl:  4.06; xent: 1.40; lr: 1.00000; 12555/13514 tok/s;    120 sec\n",
            "[2022-02-19 11:17:28,196 INFO] Step 5160/20000; acc:  60.17; ppl:  7.15; xent: 1.97; lr: 1.00000; 15369/15403 tok/s;    121 sec\n",
            "[2022-02-19 11:17:28,661 INFO] Step 5180/20000; acc:  63.42; ppl:  5.66; xent: 1.73; lr: 1.00000; 14442/15605 tok/s;    121 sec\n",
            "[2022-02-19 11:17:29,126 INFO] Step 5200/20000; acc:  66.78; ppl:  4.39; xent: 1.48; lr: 1.00000; 12479/12924 tok/s;    122 sec\n",
            "[2022-02-19 11:17:29,591 INFO] Step 5220/20000; acc:  62.45; ppl:  6.12; xent: 1.81; lr: 1.00000; 14061/14637 tok/s;    122 sec\n",
            "[2022-02-19 11:17:30,029 INFO] Step 5240/20000; acc:  66.60; ppl:  4.78; xent: 1.56; lr: 1.00000; 15124/15586 tok/s;    123 sec\n",
            "[2022-02-19 11:17:30,460 INFO] Step 5260/20000; acc:  67.56; ppl:  4.34; xent: 1.47; lr: 1.00000; 12906/14102 tok/s;    123 sec\n",
            "[2022-02-19 11:17:30,925 INFO] Step 5280/20000; acc:  63.27; ppl:  5.65; xent: 1.73; lr: 1.00000; 14426/14663 tok/s;    124 sec\n",
            "[2022-02-19 11:17:31,354 INFO] Step 5300/20000; acc:  63.76; ppl:  5.46; xent: 1.70; lr: 1.00000; 15558/16438 tok/s;    124 sec\n",
            "[2022-02-19 11:17:31,799 INFO] Step 5320/20000; acc:  65.83; ppl:  4.77; xent: 1.56; lr: 1.00000; 13710/14888 tok/s;    125 sec\n",
            "[2022-02-19 11:17:32,226 INFO] Step 5340/20000; acc:  63.62; ppl:  5.67; xent: 1.73; lr: 1.00000; 15343/15329 tok/s;    125 sec\n",
            "[2022-02-19 11:17:32,670 INFO] Step 5360/20000; acc:  63.62; ppl:  5.49; xent: 1.70; lr: 1.00000; 15812/16586 tok/s;    125 sec\n",
            "[2022-02-19 11:17:33,117 INFO] Step 5380/20000; acc:  66.69; ppl:  4.68; xent: 1.54; lr: 1.00000; 13523/13496 tok/s;    126 sec\n",
            "[2022-02-19 11:17:33,645 INFO] Step 5400/20000; acc:  64.88; ppl:  5.53; xent: 1.71; lr: 1.00000; 12057/12573 tok/s;    126 sec\n",
            "[2022-02-19 11:17:34,181 INFO] Step 5420/20000; acc:  61.52; ppl:  6.18; xent: 1.82; lr: 1.00000; 13747/13747 tok/s;    127 sec\n",
            "[2022-02-19 11:17:34,663 INFO] Step 5440/20000; acc:  67.12; ppl:  4.42; xent: 1.49; lr: 1.00000; 12809/12998 tok/s;    127 sec\n",
            "[2022-02-19 11:17:35,128 INFO] Step 5460/20000; acc:  68.74; ppl:  4.02; xent: 1.39; lr: 1.00000; 12384/13175 tok/s;    128 sec\n",
            "[2022-02-19 11:17:35,614 INFO] Step 5480/20000; acc:  60.54; ppl:  6.51; xent: 1.87; lr: 1.00000; 14631/14983 tok/s;    128 sec\n",
            "[2022-02-19 11:17:36,075 INFO] Step 5500/20000; acc:  63.39; ppl:  5.57; xent: 1.72; lr: 1.00000; 14540/15424 tok/s;    129 sec\n",
            "[2022-02-19 11:17:36,506 INFO] Step 5520/20000; acc:  69.08; ppl:  3.90; xent: 1.36; lr: 1.00000; 13449/13858 tok/s;    129 sec\n",
            "[2022-02-19 11:17:36,948 INFO] Step 5540/20000; acc:  63.64; ppl:  5.68; xent: 1.74; lr: 1.00000; 14993/15433 tok/s;    130 sec\n",
            "[2022-02-19 11:17:37,387 INFO] Step 5560/20000; acc:  65.58; ppl:  4.97; xent: 1.60; lr: 1.00000; 15320/16072 tok/s;    130 sec\n",
            "[2022-02-19 11:17:37,566 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 10\n",
            "[2022-02-19 11:17:37,822 INFO] Step 5580/20000; acc:  67.12; ppl:  4.22; xent: 1.44; lr: 1.00000; 12873/13742 tok/s;    131 sec\n",
            "[2022-02-19 11:17:38,291 INFO] Step 5600/20000; acc:  63.94; ppl:  5.59; xent: 1.72; lr: 1.00000; 14161/14571 tok/s;    131 sec\n",
            "[2022-02-19 11:17:38,728 INFO] Step 5620/20000; acc:  63.99; ppl:  5.52; xent: 1.71; lr: 1.00000; 15327/16144 tok/s;    131 sec\n",
            "[2022-02-19 11:17:39,072 INFO] Validation perplexity: 5.77013\n",
            "[2022-02-19 11:17:39,073 INFO] Validation accuracy: 68.5675\n",
            "[2022-02-19 11:17:39,073 INFO] Model is improving ppl: 5.83275 --> 5.77013.\n",
            "[2022-02-19 11:17:39,073 INFO] Model is improving acc: 67.7561 --> 68.5675.\n",
            "[2022-02-19 11:17:39,127 INFO] Saving checkpoint ./models/best/model_step_5625.pt\n",
            "[2022-02-19 11:17:39,754 INFO] Step 5640/20000; acc:  67.49; ppl:  4.22; xent: 1.44; lr: 1.00000; 5892/6419 tok/s;    133 sec\n",
            "[2022-02-19 11:17:40,210 INFO] Step 5660/20000; acc:  65.13; ppl:  5.21; xent: 1.65; lr: 1.00000; 13929/14647 tok/s;    133 sec\n",
            "[2022-02-19 11:17:40,659 INFO] Step 5680/20000; acc:  63.12; ppl:  5.47; xent: 1.70; lr: 1.00000; 15411/15540 tok/s;    133 sec\n",
            "[2022-02-19 11:17:41,119 INFO] Step 5700/20000; acc:  66.78; ppl:  4.50; xent: 1.50; lr: 1.00000; 12831/13953 tok/s;    134 sec\n",
            "[2022-02-19 11:17:41,552 INFO] Step 5720/20000; acc:  64.53; ppl:  5.49; xent: 1.70; lr: 1.00000; 14842/15801 tok/s;    134 sec\n",
            "[2022-02-19 11:17:42,021 INFO] Step 5740/20000; acc:  63.04; ppl:  5.83; xent: 1.76; lr: 1.00000; 15793/15982 tok/s;    135 sec\n",
            "[2022-02-19 11:17:42,432 INFO] Step 5760/20000; acc:  67.11; ppl:  4.48; xent: 1.50; lr: 1.00000; 15138/15367 tok/s;    135 sec\n",
            "[2022-02-19 11:17:42,883 INFO] Step 5780/20000; acc:  70.12; ppl:  3.72; xent: 1.31; lr: 1.00000; 12680/13508 tok/s;    136 sec\n",
            "[2022-02-19 11:17:43,356 INFO] Step 5800/20000; acc:  61.92; ppl:  6.19; xent: 1.82; lr: 1.00000; 14932/15228 tok/s;    136 sec\n",
            "[2022-02-19 11:17:43,808 INFO] Step 5820/20000; acc:  65.22; ppl:  4.96; xent: 1.60; lr: 1.00000; 14532/15943 tok/s;    137 sec\n",
            "[2022-02-19 11:17:44,244 INFO] Step 5840/20000; acc:  70.14; ppl:  3.63; xent: 1.29; lr: 1.00000; 13342/13696 tok/s;    137 sec\n",
            "[2022-02-19 11:17:44,703 INFO] Step 5860/20000; acc:  63.89; ppl:  5.41; xent: 1.69; lr: 1.00000; 14359/14716 tok/s;    137 sec\n",
            "[2022-02-19 11:17:45,124 INFO] Step 5880/20000; acc:  66.93; ppl:  4.52; xent: 1.51; lr: 1.00000; 15758/16661 tok/s;    138 sec\n",
            "[2022-02-19 11:17:45,563 INFO] Step 5900/20000; acc:  69.25; ppl:  3.84; xent: 1.35; lr: 1.00000; 12771/13936 tok/s;    138 sec\n",
            "[2022-02-19 11:17:46,018 INFO] Step 5920/20000; acc:  63.95; ppl:  5.36; xent: 1.68; lr: 1.00000; 14948/15333 tok/s;    139 sec\n",
            "[2022-02-19 11:17:46,448 INFO] Step 5940/20000; acc:  64.74; ppl:  5.07; xent: 1.62; lr: 1.00000; 15712/16696 tok/s;    139 sec\n",
            "[2022-02-19 11:17:46,904 INFO] Step 5960/20000; acc:  67.07; ppl:  4.40; xent: 1.48; lr: 1.00000; 13539/13989 tok/s;    140 sec\n",
            "[2022-02-19 11:17:47,329 INFO] Step 5980/20000; acc:  66.10; ppl:  4.69; xent: 1.55; lr: 1.00000; 15077/15139 tok/s;    140 sec\n",
            "[2022-02-19 11:17:47,762 INFO] Step 6000/20000; acc:  63.75; ppl:  5.18; xent: 1.64; lr: 1.00000; 16044/16718 tok/s;    141 sec\n",
            "[2022-02-19 11:17:48,193 INFO] Step 6020/20000; acc:  68.64; ppl:  4.06; xent: 1.40; lr: 1.00000; 13963/14060 tok/s;    141 sec\n",
            "[2022-02-19 11:17:48,758 INFO] Step 6040/20000; acc:  65.35; ppl:  5.24; xent: 1.66; lr: 1.00000; 11438/12074 tok/s;    142 sec\n",
            "[2022-02-19 11:17:49,299 INFO] Step 6060/20000; acc:  62.43; ppl:  5.77; xent: 1.75; lr: 1.00000; 13814/13786 tok/s;    142 sec\n",
            "[2022-02-19 11:17:49,760 INFO] Step 6080/20000; acc:  67.37; ppl:  4.11; xent: 1.41; lr: 1.00000; 13586/13581 tok/s;    143 sec\n",
            "[2022-02-19 11:17:50,208 INFO] Step 6100/20000; acc:  70.82; ppl:  3.48; xent: 1.25; lr: 1.00000; 12843/13744 tok/s;    143 sec\n",
            "[2022-02-19 11:17:50,665 INFO] Step 6120/20000; acc:  62.78; ppl:  5.79; xent: 1.76; lr: 1.00000; 15406/15828 tok/s;    143 sec\n",
            "[2022-02-19 11:17:51,111 INFO] Step 6140/20000; acc:  64.51; ppl:  5.04; xent: 1.62; lr: 1.00000; 14964/16033 tok/s;    144 sec\n",
            "[2022-02-19 11:17:51,537 INFO] Step 6160/20000; acc:  69.83; ppl:  3.59; xent: 1.28; lr: 1.00000; 13472/13955 tok/s;    144 sec\n",
            "[2022-02-19 11:17:51,994 INFO] Step 6180/20000; acc:  65.65; ppl:  5.13; xent: 1.64; lr: 1.00000; 14333/15078 tok/s;    145 sec\n",
            "[2022-02-19 11:17:52,418 INFO] Step 6200/20000; acc:  65.99; ppl:  4.63; xent: 1.53; lr: 1.00000; 15829/16375 tok/s;    145 sec\n",
            "[2022-02-19 11:17:52,579 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 11\n",
            "[2022-02-19 11:17:52,837 INFO] Step 6220/20000; acc:  68.85; ppl:  3.94; xent: 1.37; lr: 1.00000; 13334/14421 tok/s;    146 sec\n",
            "[2022-02-19 11:17:53,276 INFO] Step 6240/20000; acc:  65.39; ppl:  4.89; xent: 1.59; lr: 1.00000; 15068/15609 tok/s;    146 sec\n",
            "[2022-02-19 11:17:53,689 INFO] Validation perplexity: 5.71967\n",
            "[2022-02-19 11:17:53,689 INFO] Validation accuracy: 68.4516\n",
            "[2022-02-19 11:17:53,689 INFO] Stalled patience: 2/3\n",
            "[2022-02-19 11:17:53,740 INFO] Saving checkpoint ./models/best/model_step_6250.pt\n",
            "[2022-02-19 11:17:54,259 INFO] Step 6260/20000; acc:  64.13; ppl:  5.16; xent: 1.64; lr: 1.00000; 6835/7191 tok/s;    147 sec\n",
            "[2022-02-19 11:17:54,727 INFO] Step 6280/20000; acc:  68.18; ppl:  3.99; xent: 1.38; lr: 1.00000; 13052/14126 tok/s;    147 sec\n",
            "[2022-02-19 11:17:55,176 INFO] Step 6300/20000; acc:  67.64; ppl:  4.65; xent: 1.54; lr: 1.00000; 14364/14712 tok/s;    148 sec\n",
            "[2022-02-19 11:17:55,628 INFO] Step 6320/20000; acc:  63.96; ppl:  5.02; xent: 1.61; lr: 1.00000; 15403/15874 tok/s;    148 sec\n",
            "[2022-02-19 11:17:56,082 INFO] Step 6340/20000; acc:  68.83; ppl:  3.86; xent: 1.35; lr: 1.00000; 13199/13976 tok/s;    149 sec\n",
            "[2022-02-19 11:17:56,508 INFO] Step 6360/20000; acc:  64.83; ppl:  5.15; xent: 1.64; lr: 1.00000; 15146/15938 tok/s;    149 sec\n",
            "[2022-02-19 11:17:56,976 INFO] Step 6380/20000; acc:  63.19; ppl:  5.38; xent: 1.68; lr: 1.00000; 15793/16112 tok/s;    150 sec\n",
            "[2022-02-19 11:17:57,388 INFO] Step 6400/20000; acc:  67.91; ppl:  4.12; xent: 1.42; lr: 1.00000; 15095/15353 tok/s;    150 sec\n",
            "[2022-02-19 11:17:57,849 INFO] Step 6420/20000; acc:  72.24; ppl:  3.19; xent: 1.16; lr: 1.00000; 12142/13065 tok/s;    151 sec\n",
            "[2022-02-19 11:17:58,325 INFO] Step 6440/20000; acc:  63.23; ppl:  5.53; xent: 1.71; lr: 1.00000; 14401/15183 tok/s;    151 sec\n",
            "[2022-02-19 11:17:58,753 INFO] Step 6460/20000; acc:  67.70; ppl:  4.14; xent: 1.42; lr: 1.00000; 15108/15834 tok/s;    152 sec\n",
            "[2022-02-19 11:17:59,181 INFO] Step 6480/20000; acc:  70.38; ppl:  3.52; xent: 1.26; lr: 1.00000; 13543/14376 tok/s;    152 sec\n",
            "[2022-02-19 11:17:59,641 INFO] Step 6500/20000; acc:  65.71; ppl:  4.91; xent: 1.59; lr: 1.00000; 14489/14650 tok/s;    152 sec\n",
            "[2022-02-19 11:18:00,068 INFO] Step 6520/20000; acc:  67.05; ppl:  4.30; xent: 1.46; lr: 1.00000; 15587/17101 tok/s;    153 sec\n",
            "[2022-02-19 11:18:00,496 INFO] Step 6540/20000; acc:  69.52; ppl:  3.65; xent: 1.29; lr: 1.00000; 13252/14208 tok/s;    153 sec\n",
            "[2022-02-19 11:18:00,954 INFO] Step 6560/20000; acc:  66.19; ppl:  4.77; xent: 1.56; lr: 1.00000; 15051/15068 tok/s;    154 sec\n",
            "[2022-02-19 11:18:01,395 INFO] Step 6580/20000; acc:  65.32; ppl:  4.84; xent: 1.58; lr: 1.00000; 15497/16320 tok/s;    154 sec\n",
            "[2022-02-19 11:18:01,845 INFO] Step 6600/20000; acc:  69.41; ppl:  3.77; xent: 1.33; lr: 1.00000; 13805/14103 tok/s;    155 sec\n",
            "[2022-02-19 11:18:02,269 INFO] Step 6620/20000; acc:  67.44; ppl:  4.43; xent: 1.49; lr: 1.00000; 15264/15174 tok/s;    155 sec\n",
            "[2022-02-19 11:18:02,717 INFO] Step 6640/20000; acc:  63.95; ppl:  5.02; xent: 1.61; lr: 1.00000; 15621/16141 tok/s;    155 sec\n",
            "[2022-02-19 11:18:03,156 INFO] Step 6660/20000; acc:  69.29; ppl:  3.81; xent: 1.34; lr: 1.00000; 13685/13856 tok/s;    156 sec\n",
            "[2022-02-19 11:18:03,725 INFO] Step 6680/20000; acc:  65.99; ppl:  4.77; xent: 1.56; lr: 1.00000; 11147/11779 tok/s;    156 sec\n",
            "[2022-02-19 11:18:04,244 INFO] Step 6700/20000; acc:  64.30; ppl:  5.26; xent: 1.66; lr: 1.00000; 14200/14339 tok/s;    157 sec\n",
            "[2022-02-19 11:18:04,727 INFO] Step 6720/20000; acc:  69.26; ppl:  3.70; xent: 1.31; lr: 1.00000; 12756/12863 tok/s;    157 sec\n",
            "[2022-02-19 11:18:05,180 INFO] Step 6740/20000; acc:  72.27; ppl:  3.23; xent: 1.17; lr: 1.00000; 12625/13575 tok/s;    158 sec\n",
            "[2022-02-19 11:18:05,656 INFO] Step 6760/20000; acc:  63.72; ppl:  5.32; xent: 1.67; lr: 1.00000; 14951/15312 tok/s;    158 sec\n",
            "[2022-02-19 11:18:06,118 INFO] Step 6780/20000; acc:  66.46; ppl:  4.53; xent: 1.51; lr: 1.00000; 14456/15477 tok/s;    159 sec\n",
            "[2022-02-19 11:18:06,545 INFO] Step 6800/20000; acc:  69.53; ppl:  3.62; xent: 1.29; lr: 1.00000; 13358/14274 tok/s;    159 sec\n",
            "[2022-02-19 11:18:07,008 INFO] Step 6820/20000; acc:  66.77; ppl:  4.56; xent: 1.52; lr: 1.00000; 13994/14718 tok/s;    160 sec\n",
            "[2022-02-19 11:18:07,420 INFO] Step 6840/20000; acc:  68.83; ppl:  3.96; xent: 1.38; lr: 1.00000; 16222/16623 tok/s;    160 sec\n",
            "[2022-02-19 11:18:07,578 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 12\n",
            "[2022-02-19 11:18:07,845 INFO] Step 6860/20000; acc:  71.17; ppl:  3.46; xent: 1.24; lr: 1.00000; 13203/14319 tok/s;    161 sec\n",
            "[2022-02-19 11:18:08,385 INFO] Validation perplexity: 5.71735\n",
            "[2022-02-19 11:18:08,386 INFO] Validation accuracy: 68.9847\n",
            "[2022-02-19 11:18:08,386 INFO] Model is improving ppl: 5.77013 --> 5.71735.\n",
            "[2022-02-19 11:18:08,386 INFO] Model is improving acc: 68.5675 --> 68.9847.\n",
            "[2022-02-19 11:18:08,438 INFO] Saving checkpoint ./models/best/model_step_6875.pt\n",
            "[2022-02-19 11:18:08,857 INFO] Step 6880/20000; acc:  65.96; ppl:  4.69; xent: 1.55; lr: 1.00000; 6628/6922 tok/s;    162 sec\n",
            "[2022-02-19 11:18:09,298 INFO] Step 6900/20000; acc:  64.78; ppl:  4.89; xent: 1.59; lr: 1.00000; 15398/16070 tok/s;    162 sec\n",
            "[2022-02-19 11:18:09,766 INFO] Step 6920/20000; acc:  68.95; ppl:  3.84; xent: 1.35; lr: 1.00000; 13071/14154 tok/s;    163 sec\n",
            "[2022-02-19 11:18:10,228 INFO] Step 6940/20000; acc:  67.75; ppl:  4.47; xent: 1.50; lr: 1.00000; 14014/14170 tok/s;    163 sec\n",
            "[2022-02-19 11:18:10,686 INFO] Step 6960/20000; acc:  65.40; ppl:  4.63; xent: 1.53; lr: 1.00000; 15236/15936 tok/s;    163 sec\n",
            "[2022-02-19 11:18:11,134 INFO] Step 6980/20000; acc:  70.71; ppl:  3.55; xent: 1.27; lr: 1.00000; 13453/13973 tok/s;    164 sec\n",
            "[2022-02-19 11:18:11,563 INFO] Step 7000/20000; acc:  66.57; ppl:  4.50; xent: 1.50; lr: 1.00000; 15095/16030 tok/s;    164 sec\n",
            "[2022-02-19 11:18:12,042 INFO] Step 7020/20000; acc:  64.04; ppl:  5.08; xent: 1.63; lr: 1.00000; 15429/15744 tok/s;    165 sec\n",
            "[2022-02-19 11:18:12,461 INFO] Step 7040/20000; acc:  68.25; ppl:  3.89; xent: 1.36; lr: 1.00000; 14873/15122 tok/s;    165 sec\n",
            "[2022-02-19 11:18:12,927 INFO] Step 7060/20000; acc:  73.50; ppl:  3.02; xent: 1.11; lr: 1.00000; 12020/12837 tok/s;    166 sec\n",
            "[2022-02-19 11:18:13,407 INFO] Step 7080/20000; acc:  65.22; ppl:  4.95; xent: 1.60; lr: 1.00000; 14034/14738 tok/s;    166 sec\n",
            "[2022-02-19 11:18:13,834 INFO] Step 7100/20000; acc:  69.55; ppl:  3.75; xent: 1.32; lr: 1.00000; 15008/15893 tok/s;    167 sec\n",
            "[2022-02-19 11:18:14,260 INFO] Step 7120/20000; acc:  71.87; ppl:  3.19; xent: 1.16; lr: 1.00000; 13452/14460 tok/s;    167 sec\n",
            "[2022-02-19 11:18:14,717 INFO] Step 7140/20000; acc:  65.39; ppl:  4.73; xent: 1.55; lr: 1.00000; 14545/15077 tok/s;    167 sec\n",
            "[2022-02-19 11:18:15,151 INFO] Step 7160/20000; acc:  68.20; ppl:  4.11; xent: 1.41; lr: 1.00000; 15410/16214 tok/s;    168 sec\n",
            "[2022-02-19 11:18:15,583 INFO] Step 7180/20000; acc:  71.15; ppl:  3.39; xent: 1.22; lr: 1.00000; 13200/14360 tok/s;    168 sec\n",
            "[2022-02-19 11:18:16,043 INFO] Step 7200/20000; acc:  66.50; ppl:  4.65; xent: 1.54; lr: 1.00000; 15165/15278 tok/s;    169 sec\n",
            "[2022-02-19 11:18:16,490 INFO] Step 7220/20000; acc:  65.03; ppl:  4.75; xent: 1.56; lr: 1.00000; 15492/16374 tok/s;    169 sec\n",
            "[2022-02-19 11:18:16,917 INFO] Step 7240/20000; acc:  70.54; ppl:  3.47; xent: 1.24; lr: 1.00000; 14753/14293 tok/s;    170 sec\n",
            "[2022-02-19 11:18:17,348 INFO] Step 7260/20000; acc:  68.51; ppl:  4.24; xent: 1.44; lr: 1.00000; 14900/14825 tok/s;    170 sec\n",
            "[2022-02-19 11:18:17,787 INFO] Step 7280/20000; acc:  66.20; ppl:  4.55; xent: 1.51; lr: 1.00000; 15950/16325 tok/s;    171 sec\n",
            "[2022-02-19 11:18:18,238 INFO] Step 7300/20000; acc:  70.58; ppl:  3.44; xent: 1.24; lr: 1.00000; 13272/13792 tok/s;    171 sec\n",
            "[2022-02-19 11:18:18,808 INFO] Step 7320/20000; acc:  66.86; ppl:  4.44; xent: 1.49; lr: 1.00000; 11196/11781 tok/s;    172 sec\n",
            "[2022-02-19 11:18:19,379 INFO] Step 7340/20000; acc:  64.81; ppl:  4.89; xent: 1.59; lr: 1.00000; 12947/13168 tok/s;    172 sec\n",
            "[2022-02-19 11:18:19,828 INFO] Step 7360/20000; acc:  69.72; ppl:  3.61; xent: 1.28; lr: 1.00000; 13800/14141 tok/s;    173 sec\n",
            "[2022-02-19 11:18:20,278 INFO] Step 7380/20000; acc:  73.81; ppl:  2.97; xent: 1.09; lr: 1.00000; 12573/13225 tok/s;    173 sec\n",
            "[2022-02-19 11:18:20,750 INFO] Step 7400/20000; acc:  65.67; ppl:  4.97; xent: 1.60; lr: 1.00000; 14746/15302 tok/s;    174 sec\n",
            "[2022-02-19 11:18:21,190 INFO] Step 7420/20000; acc:  68.04; ppl:  4.06; xent: 1.40; lr: 1.00000; 14959/15775 tok/s;    174 sec\n",
            "[2022-02-19 11:18:21,616 INFO] Step 7440/20000; acc:  71.90; ppl:  3.09; xent: 1.13; lr: 1.00000; 13384/14582 tok/s;    174 sec\n",
            "[2022-02-19 11:18:22,062 INFO] Step 7460/20000; acc:  67.64; ppl:  4.31; xent: 1.46; lr: 1.00000; 14728/15400 tok/s;    175 sec\n",
            "[2022-02-19 11:18:22,493 INFO] Step 7480/20000; acc:  68.89; ppl:  3.91; xent: 1.36; lr: 1.00000; 15479/15848 tok/s;    175 sec\n",
            "[2022-02-19 11:18:22,654 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 13\n",
            "[2022-02-19 11:18:22,924 INFO] Step 7500/20000; acc:  71.61; ppl:  3.32; xent: 1.20; lr: 1.00000; 13039/14252 tok/s;    176 sec\n",
            "[2022-02-19 11:18:23,127 INFO] Validation perplexity: 5.82189\n",
            "[2022-02-19 11:18:23,127 INFO] Validation accuracy: 68.9383\n",
            "[2022-02-19 11:18:23,127 INFO] Decreasing patience: 2/3\n",
            "[2022-02-19 11:18:23,181 INFO] Saving checkpoint ./models/best/model_step_7500.pt\n",
            "[2022-02-19 11:18:23,964 INFO] Step 7520/20000; acc:  65.87; ppl:  4.58; xent: 1.52; lr: 1.00000; 6519/6842 tok/s;    177 sec\n",
            "[2022-02-19 11:18:24,409 INFO] Step 7540/20000; acc:  66.04; ppl:  4.46; xent: 1.49; lr: 1.00000; 15325/15825 tok/s;    177 sec\n",
            "[2022-02-19 11:18:24,875 INFO] Step 7560/20000; acc:  68.71; ppl:  3.79; xent: 1.33; lr: 1.00000; 13195/14485 tok/s;    178 sec\n",
            "[2022-02-19 11:18:25,292 INFO] Step 7580/20000; acc:  67.99; ppl:  4.28; xent: 1.45; lr: 1.00000; 15532/15980 tok/s;    178 sec\n",
            "[2022-02-19 11:18:25,754 INFO] Step 7600/20000; acc:  67.20; ppl:  4.36; xent: 1.47; lr: 1.00000; 15343/15497 tok/s;    179 sec\n",
            "[2022-02-19 11:18:26,213 INFO] Step 7620/20000; acc:  69.99; ppl:  3.52; xent: 1.26; lr: 1.00000; 13176/13867 tok/s;    179 sec\n",
            "[2022-02-19 11:18:26,628 INFO] Step 7640/20000; acc:  69.26; ppl:  4.06; xent: 1.40; lr: 1.00000; 15308/15954 tok/s;    179 sec\n",
            "[2022-02-19 11:18:27,075 INFO] Step 7660/20000; acc:  66.18; ppl:  4.48; xent: 1.50; lr: 1.00000; 16256/16359 tok/s;    180 sec\n",
            "[2022-02-19 11:18:27,501 INFO] Step 7680/20000; acc:  70.73; ppl:  3.52; xent: 1.26; lr: 1.00000; 14224/15222 tok/s;    180 sec\n",
            "[2022-02-19 11:18:27,953 INFO] Step 7700/20000; acc:  73.01; ppl:  3.04; xent: 1.11; lr: 1.00000; 12430/13403 tok/s;    181 sec\n",
            "[2022-02-19 11:18:28,449 INFO] Step 7720/20000; acc:  65.42; ppl:  4.96; xent: 1.60; lr: 1.00000; 13839/14359 tok/s;    181 sec\n",
            "[2022-02-19 11:18:28,874 INFO] Step 7740/20000; acc:  71.14; ppl:  3.52; xent: 1.26; lr: 1.00000; 15202/15875 tok/s;    182 sec\n",
            "[2022-02-19 11:18:29,305 INFO] Step 7760/20000; acc:  73.78; ppl:  2.92; xent: 1.07; lr: 1.00000; 13403/14258 tok/s;    182 sec\n",
            "[2022-02-19 11:18:29,797 INFO] Step 7780/20000; acc:  67.53; ppl:  4.16; xent: 1.43; lr: 1.00000; 13633/13841 tok/s;    183 sec\n",
            "[2022-02-19 11:18:30,256 INFO] Step 7800/20000; acc:  68.63; ppl:  3.94; xent: 1.37; lr: 1.00000; 14561/15651 tok/s;    183 sec\n",
            "[2022-02-19 11:18:30,694 INFO] Step 7820/20000; acc:  72.12; ppl:  3.21; xent: 1.17; lr: 1.00000; 13017/14018 tok/s;    183 sec\n",
            "[2022-02-19 11:18:31,147 INFO] Step 7840/20000; acc:  67.91; ppl:  4.29; xent: 1.46; lr: 1.00000; 15384/15243 tok/s;    184 sec\n",
            "[2022-02-19 11:18:31,599 INFO] Step 7860/20000; acc:  66.58; ppl:  4.36; xent: 1.47; lr: 1.00000; 15216/16230 tok/s;    184 sec\n",
            "[2022-02-19 11:18:32,031 INFO] Step 7880/20000; acc:  72.58; ppl:  3.15; xent: 1.15; lr: 1.00000; 14554/14280 tok/s;    185 sec\n",
            "[2022-02-19 11:18:32,446 INFO] Step 7900/20000; acc:  69.37; ppl:  4.08; xent: 1.41; lr: 1.00000; 15579/15581 tok/s;    185 sec\n",
            "[2022-02-19 11:18:32,900 INFO] Step 7920/20000; acc:  65.66; ppl:  4.58; xent: 1.52; lr: 1.00000; 15641/16202 tok/s;    186 sec\n",
            "[2022-02-19 11:18:33,422 INFO] Step 7940/20000; acc:  71.57; ppl:  3.25; xent: 1.18; lr: 1.00000; 11607/11701 tok/s;    186 sec\n",
            "[2022-02-19 11:18:33,946 INFO] Step 7960/20000; acc:  69.75; ppl:  3.95; xent: 1.37; lr: 1.00000; 11937/12583 tok/s;    187 sec\n",
            "[2022-02-19 11:18:34,502 INFO] Step 7980/20000; acc:  65.57; ppl:  4.53; xent: 1.51; lr: 1.00000; 13006/13337 tok/s;    187 sec\n",
            "[2022-02-19 11:18:34,958 INFO] Step 8000/20000; acc:  69.90; ppl:  3.66; xent: 1.30; lr: 1.00000; 13188/13994 tok/s;    188 sec\n",
            "[2022-02-19 11:18:35,415 INFO] Step 8020/20000; acc:  74.76; ppl:  2.75; xent: 1.01; lr: 1.00000; 12356/13281 tok/s;    188 sec\n",
            "[2022-02-19 11:18:35,883 INFO] Step 8040/20000; acc:  65.37; ppl:  4.75; xent: 1.56; lr: 1.00000; 14941/15212 tok/s;    189 sec\n",
            "[2022-02-19 11:18:36,317 INFO] Step 8060/20000; acc:  69.61; ppl:  3.64; xent: 1.29; lr: 1.00000; 15112/16042 tok/s;    189 sec\n",
            "[2022-02-19 11:18:36,401 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 14\n",
            "[2022-02-19 11:18:36,761 INFO] Step 8080/20000; acc:  74.37; ppl:  2.82; xent: 1.04; lr: 1.00000; 13089/13303 tok/s;    190 sec\n",
            "[2022-02-19 11:18:37,224 INFO] Step 8100/20000; acc:  66.34; ppl:  4.57; xent: 1.52; lr: 1.00000; 14222/15081 tok/s;    190 sec\n",
            "[2022-02-19 11:18:37,652 INFO] Step 8120/20000; acc:  67.87; ppl:  3.92; xent: 1.37; lr: 1.00000; 15671/16582 tok/s;    190 sec\n",
            "[2022-02-19 11:18:37,952 INFO] Validation perplexity: 5.8062\n",
            "[2022-02-19 11:18:37,953 INFO] Validation accuracy: 69.1701\n",
            "[2022-02-19 11:18:37,953 INFO] Stalled patience: 2/3\n",
            "[2022-02-19 11:18:38,002 INFO] Saving checkpoint ./models/best/model_step_8125.pt\n",
            "[2022-02-19 11:18:38,704 INFO] Step 8140/20000; acc:  73.28; ppl:  3.11; xent: 1.13; lr: 1.00000; 5347/5792 tok/s;    191 sec\n",
            "[2022-02-19 11:18:39,181 INFO] Step 8160/20000; acc:  68.71; ppl:  4.02; xent: 1.39; lr: 1.00000; 14190/14768 tok/s;    192 sec\n",
            "[2022-02-19 11:18:39,627 INFO] Step 8180/20000; acc:  67.57; ppl:  4.09; xent: 1.41; lr: 1.00000; 15172/15754 tok/s;    192 sec\n",
            "[2022-02-19 11:18:40,088 INFO] Step 8200/20000; acc:  70.83; ppl:  3.37; xent: 1.21; lr: 1.00000; 13366/14330 tok/s;    193 sec\n",
            "[2022-02-19 11:18:40,536 INFO] Step 8220/20000; acc:  67.38; ppl:  4.43; xent: 1.49; lr: 1.00000; 14674/15179 tok/s;    193 sec\n",
            "[2022-02-19 11:18:41,002 INFO] Step 8240/20000; acc:  66.73; ppl:  4.35; xent: 1.47; lr: 1.00000; 15435/15577 tok/s;    194 sec\n",
            "[2022-02-19 11:18:41,439 INFO] Step 8260/20000; acc:  72.23; ppl:  3.26; xent: 1.18; lr: 1.00000; 13881/14422 tok/s;    194 sec\n",
            "[2022-02-19 11:18:41,861 INFO] Step 8280/20000; acc:  70.46; ppl:  3.80; xent: 1.33; lr: 1.00000; 14762/15378 tok/s;    195 sec\n",
            "[2022-02-19 11:18:42,302 INFO] Step 8300/20000; acc:  67.73; ppl:  4.02; xent: 1.39; lr: 1.00000; 16119/16330 tok/s;    195 sec\n",
            "[2022-02-19 11:18:42,719 INFO] Step 8320/20000; acc:  70.93; ppl:  3.32; xent: 1.20; lr: 1.00000; 14090/15800 tok/s;    195 sec\n",
            "[2022-02-19 11:18:43,167 INFO] Step 8340/20000; acc:  75.72; ppl:  2.62; xent: 0.96; lr: 1.00000; 12688/13311 tok/s;    196 sec\n",
            "[2022-02-19 11:18:43,714 INFO] Step 8360/20000; acc:  67.12; ppl:  4.39; xent: 1.48; lr: 1.00000; 12629/12892 tok/s;    196 sec\n",
            "[2022-02-19 11:18:44,251 INFO] Step 8380/20000; acc:  70.85; ppl:  3.53; xent: 1.26; lr: 1.00000; 12097/12895 tok/s;    197 sec\n",
            "[2022-02-19 11:18:44,775 INFO] Step 8400/20000; acc:  73.91; ppl:  2.85; xent: 1.05; lr: 1.00000; 11063/11921 tok/s;    198 sec\n",
            "[2022-02-19 11:18:45,239 INFO] Step 8420/20000; acc:  67.63; ppl:  4.34; xent: 1.47; lr: 1.00000; 14707/14735 tok/s;    198 sec\n",
            "[2022-02-19 11:18:45,683 INFO] Step 8440/20000; acc:  68.16; ppl:  3.96; xent: 1.38; lr: 1.00000; 15188/16717 tok/s;    198 sec\n",
            "[2022-02-19 11:18:46,124 INFO] Step 8460/20000; acc:  73.48; ppl:  3.02; xent: 1.11; lr: 1.00000; 13011/13322 tok/s;    199 sec\n",
            "[2022-02-19 11:18:46,577 INFO] Step 8480/20000; acc:  70.47; ppl:  3.74; xent: 1.32; lr: 1.00000; 15063/14964 tok/s;    199 sec\n",
            "[2022-02-19 11:18:47,019 INFO] Step 8500/20000; acc:  67.39; ppl:  4.13; xent: 1.42; lr: 1.00000; 15435/16439 tok/s;    200 sec\n",
            "[2022-02-19 11:18:47,460 INFO] Step 8520/20000; acc:  72.95; ppl:  3.07; xent: 1.12; lr: 1.00000; 14256/14113 tok/s;    200 sec\n",
            "[2022-02-19 11:18:47,904 INFO] Step 8540/20000; acc:  68.36; ppl:  4.17; xent: 1.43; lr: 1.00000; 14861/15215 tok/s;    201 sec\n",
            "[2022-02-19 11:18:48,369 INFO] Step 8560/20000; acc:  65.83; ppl:  4.56; xent: 1.52; lr: 1.00000; 15555/15914 tok/s;    201 sec\n",
            "[2022-02-19 11:18:48,818 INFO] Step 8580/20000; acc:  72.14; ppl:  3.15; xent: 1.15; lr: 1.00000; 13679/13768 tok/s;    202 sec\n",
            "[2022-02-19 11:18:49,224 INFO] Step 8600/20000; acc:  69.67; ppl:  3.69; xent: 1.31; lr: 1.00000; 15228/15981 tok/s;    202 sec\n",
            "[2022-02-19 11:18:49,680 INFO] Step 8620/20000; acc:  67.77; ppl:  3.99; xent: 1.38; lr: 1.00000; 15751/16039 tok/s;    202 sec\n",
            "[2022-02-19 11:18:50,098 INFO] Step 8640/20000; acc:  70.64; ppl:  3.38; xent: 1.22; lr: 1.00000; 14332/15459 tok/s;    203 sec\n",
            "[2022-02-19 11:18:50,548 INFO] Step 8660/20000; acc:  75.55; ppl:  2.67; xent: 0.98; lr: 1.00000; 12402/13219 tok/s;    203 sec\n",
            "[2022-02-19 11:18:51,050 INFO] Step 8680/20000; acc:  67.14; ppl:  4.53; xent: 1.51; lr: 1.00000; 13820/14444 tok/s;    204 sec\n",
            "[2022-02-19 11:18:51,471 INFO] Step 8700/20000; acc:  70.61; ppl:  3.53; xent: 1.26; lr: 1.00000; 15489/16331 tok/s;    204 sec\n",
            "[2022-02-19 11:18:51,547 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 15\n",
            "[2022-02-19 11:18:51,893 INFO] Step 8720/20000; acc:  73.77; ppl:  2.88; xent: 1.06; lr: 1.00000; 13812/14346 tok/s;    205 sec\n",
            "[2022-02-19 11:18:52,344 INFO] Step 8740/20000; acc:  67.97; ppl:  4.17; xent: 1.43; lr: 1.00000; 14610/15199 tok/s;    205 sec\n",
            "[2022-02-19 11:18:52,748 INFO] Validation perplexity: 5.7079\n",
            "[2022-02-19 11:18:52,748 INFO] Validation accuracy: 69.6569\n",
            "[2022-02-19 11:18:52,749 INFO] Model is improving ppl: 5.71735 --> 5.7079.\n",
            "[2022-02-19 11:18:52,749 INFO] Model is improving acc: 68.9847 --> 69.6569.\n",
            "[2022-02-19 11:18:52,807 INFO] Saving checkpoint ./models/best/model_step_8750.pt\n",
            "[2022-02-19 11:18:53,293 INFO] Step 8760/20000; acc:  68.81; ppl:  3.81; xent: 1.34; lr: 1.00000; 7076/7591 tok/s;    206 sec\n",
            "[2022-02-19 11:18:53,733 INFO] Step 8780/20000; acc:  73.55; ppl:  2.97; xent: 1.09; lr: 1.00000; 12874/13889 tok/s;    206 sec\n",
            "[2022-02-19 11:18:54,223 INFO] Step 8800/20000; acc:  69.43; ppl:  3.90; xent: 1.36; lr: 1.00000; 14032/14322 tok/s;    207 sec\n",
            "[2022-02-19 11:18:54,671 INFO] Step 8820/20000; acc:  68.14; ppl:  3.91; xent: 1.36; lr: 1.00000; 15198/15925 tok/s;    207 sec\n",
            "[2022-02-19 11:18:55,142 INFO] Step 8840/20000; acc:  72.39; ppl:  3.11; xent: 1.14; lr: 1.00000; 13226/13875 tok/s;    208 sec\n",
            "[2022-02-19 11:18:55,572 INFO] Step 8860/20000; acc:  68.37; ppl:  4.18; xent: 1.43; lr: 1.00000; 15301/15608 tok/s;    208 sec\n",
            "[2022-02-19 11:18:56,051 INFO] Step 8880/20000; acc:  67.30; ppl:  4.25; xent: 1.45; lr: 1.00000; 14948/15226 tok/s;    209 sec\n",
            "[2022-02-19 11:18:56,503 INFO] Step 8900/20000; acc:  73.18; ppl:  3.04; xent: 1.11; lr: 1.00000; 13314/13896 tok/s;    209 sec\n",
            "[2022-02-19 11:18:56,935 INFO] Step 8920/20000; acc:  71.01; ppl:  3.53; xent: 1.26; lr: 1.00000; 14060/14933 tok/s;    210 sec\n",
            "[2022-02-19 11:18:57,380 INFO] Step 8940/20000; acc:  68.50; ppl:  3.83; xent: 1.34; lr: 1.00000; 15646/16097 tok/s;    210 sec\n",
            "[2022-02-19 11:18:57,778 INFO] Step 8960/20000; acc:  73.63; ppl:  2.92; xent: 1.07; lr: 1.00000; 14496/16218 tok/s;    211 sec\n",
            "[2022-02-19 11:18:58,232 INFO] Step 8980/20000; acc:  75.59; ppl:  2.58; xent: 0.95; lr: 1.00000; 12588/13256 tok/s;    211 sec\n",
            "[2022-02-19 11:18:58,805 INFO] Step 9000/20000; acc:  68.18; ppl:  4.28; xent: 1.45; lr: 1.00000; 12270/12405 tok/s;    212 sec\n",
            "[2022-02-19 11:18:59,338 INFO] Step 9020/20000; acc:  71.14; ppl:  3.35; xent: 1.21; lr: 1.00000; 12268/13349 tok/s;    212 sec\n",
            "[2022-02-19 11:18:59,866 INFO] Step 9040/20000; acc:  75.04; ppl:  2.72; xent: 1.00; lr: 1.00000; 11165/11730 tok/s;    213 sec\n",
            "[2022-02-19 11:19:00,322 INFO] Step 9060/20000; acc:  69.36; ppl:  3.91; xent: 1.36; lr: 1.00000; 15025/15025 tok/s;    213 sec\n",
            "[2022-02-19 11:19:00,766 INFO] Step 9080/20000; acc:  67.18; ppl:  4.10; xent: 1.41; lr: 1.00000; 15450/16444 tok/s;    214 sec\n",
            "[2022-02-19 11:19:01,199 INFO] Step 9100/20000; acc:  74.70; ppl:  2.75; xent: 1.01; lr: 1.00000; 13282/13668 tok/s;    214 sec\n",
            "[2022-02-19 11:19:01,666 INFO] Step 9120/20000; acc:  69.56; ppl:  3.88; xent: 1.35; lr: 1.00000; 14744/14537 tok/s;    214 sec\n",
            "[2022-02-19 11:19:02,109 INFO] Step 9140/20000; acc:  67.11; ppl:  4.11; xent: 1.41; lr: 1.00000; 15465/16395 tok/s;    215 sec\n",
            "[2022-02-19 11:19:02,568 INFO] Step 9160/20000; acc:  71.46; ppl:  3.15; xent: 1.15; lr: 1.00000; 13679/13602 tok/s;    215 sec\n",
            "[2022-02-19 11:19:03,009 INFO] Step 9180/20000; acc:  70.36; ppl:  3.73; xent: 1.32; lr: 1.00000; 14655/14795 tok/s;    216 sec\n",
            "[2022-02-19 11:19:03,470 INFO] Step 9200/20000; acc:  67.48; ppl:  4.21; xent: 1.44; lr: 1.00000; 15428/16073 tok/s;    216 sec\n",
            "[2022-02-19 11:19:03,914 INFO] Step 9220/20000; acc:  74.35; ppl:  2.90; xent: 1.07; lr: 1.00000; 13597/13953 tok/s;    217 sec\n",
            "[2022-02-19 11:19:04,334 INFO] Step 9240/20000; acc:  70.62; ppl:  3.61; xent: 1.28; lr: 1.00000; 14847/15441 tok/s;    217 sec\n",
            "[2022-02-19 11:19:04,783 INFO] Step 9260/20000; acc:  68.59; ppl:  3.84; xent: 1.34; lr: 1.00000; 16048/16362 tok/s;    218 sec\n",
            "[2022-02-19 11:19:05,214 INFO] Step 9280/20000; acc:  71.02; ppl:  3.39; xent: 1.22; lr: 1.00000; 13848/15342 tok/s;    218 sec\n",
            "[2022-02-19 11:19:05,667 INFO] Step 9300/20000; acc:  75.37; ppl:  2.61; xent: 0.96; lr: 1.00000; 12338/13143 tok/s;    218 sec\n",
            "[2022-02-19 11:19:06,164 INFO] Step 9320/20000; acc:  67.56; ppl:  4.26; xent: 1.45; lr: 1.00000; 13780/14422 tok/s;    219 sec\n",
            "[2022-02-19 11:19:06,584 INFO] Step 9340/20000; acc:  72.57; ppl:  3.12; xent: 1.14; lr: 1.00000; 15508/16119 tok/s;    219 sec\n",
            "[2022-02-19 11:19:06,660 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 16\n",
            "[2022-02-19 11:19:07,000 INFO] Step 9360/20000; acc:  74.55; ppl:  2.77; xent: 1.02; lr: 1.00000; 13998/14729 tok/s;    220 sec\n",
            "[2022-02-19 11:19:07,563 INFO] Validation perplexity: 5.77654\n",
            "[2022-02-19 11:19:07,563 INFO] Validation accuracy: 69.1933\n",
            "[2022-02-19 11:19:07,563 INFO] Decreasing patience: 2/3\n",
            "[2022-02-19 11:19:07,616 INFO] Saving checkpoint ./models/best/model_step_9375.pt\n",
            "[2022-02-19 11:19:07,978 INFO] Step 9380/20000; acc:  68.63; ppl:  4.13; xent: 1.42; lr: 1.00000; 6854/7035 tok/s;    221 sec\n",
            "[2022-02-19 11:19:08,429 INFO] Step 9400/20000; acc:  68.55; ppl:  3.77; xent: 1.33; lr: 1.00000; 14985/16037 tok/s;    221 sec\n",
            "[2022-02-19 11:19:08,870 INFO] Step 9420/20000; acc:  73.07; ppl:  2.97; xent: 1.09; lr: 1.00000; 12841/14174 tok/s;    222 sec\n",
            "[2022-02-19 11:19:09,364 INFO] Step 9440/20000; acc:  69.87; ppl:  3.70; xent: 1.31; lr: 1.00000; 13930/14041 tok/s;    222 sec\n",
            "[2022-02-19 11:19:09,804 INFO] Step 9460/20000; acc:  68.98; ppl:  3.77; xent: 1.33; lr: 1.00000; 15529/16423 tok/s;    223 sec\n",
            "[2022-02-19 11:19:10,253 INFO] Step 9480/20000; acc:  73.58; ppl:  2.97; xent: 1.09; lr: 1.00000; 13965/14317 tok/s;    223 sec\n",
            "[2022-02-19 11:19:10,686 INFO] Step 9500/20000; acc:  68.90; ppl:  4.06; xent: 1.40; lr: 1.00000; 15276/15764 tok/s;    223 sec\n",
            "[2022-02-19 11:19:11,147 INFO] Step 9520/20000; acc:  67.12; ppl:  4.13; xent: 1.42; lr: 1.00000; 15555/16012 tok/s;    224 sec\n",
            "[2022-02-19 11:19:11,601 INFO] Step 9540/20000; acc:  73.73; ppl:  2.93; xent: 1.07; lr: 1.00000; 13362/13742 tok/s;    224 sec\n",
            "[2022-02-19 11:19:12,022 INFO] Step 9560/20000; acc:  72.57; ppl:  3.35; xent: 1.21; lr: 1.00000; 14224/15034 tok/s;    225 sec\n",
            "[2022-02-19 11:19:12,468 INFO] Step 9580/20000; acc:  70.56; ppl:  3.53; xent: 1.26; lr: 1.00000; 15459/15935 tok/s;    225 sec\n",
            "[2022-02-19 11:19:12,864 INFO] Step 9600/20000; acc:  75.39; ppl:  2.79; xent: 1.03; lr: 1.00000; 14487/16358 tok/s;    226 sec\n",
            "[2022-02-19 11:19:13,329 INFO] Step 9620/20000; acc:  77.56; ppl:  2.44; xent: 0.89; lr: 1.00000; 12145/12827 tok/s;    226 sec\n",
            "[2022-02-19 11:19:13,904 INFO] Step 9640/20000; acc:  67.84; ppl:  4.18; xent: 1.43; lr: 1.00000; 12189/12571 tok/s;    227 sec\n",
            "[2022-02-19 11:19:14,438 INFO] Step 9660/20000; acc:  71.28; ppl:  3.37; xent: 1.22; lr: 1.00000; 12232/12903 tok/s;    227 sec\n",
            "[2022-02-19 11:19:14,914 INFO] Step 9680/20000; acc:  74.73; ppl:  2.80; xent: 1.03; lr: 1.00000; 12538/13137 tok/s;    228 sec\n",
            "[2022-02-19 11:19:15,380 INFO] Step 9700/20000; acc:  68.67; ppl:  4.08; xent: 1.41; lr: 1.00000; 14920/15219 tok/s;    228 sec\n",
            "[2022-02-19 11:19:15,834 INFO] Step 9720/20000; acc:  67.74; ppl:  3.93; xent: 1.37; lr: 1.00000; 15415/15911 tok/s;    229 sec\n",
            "[2022-02-19 11:19:16,253 INFO] Step 9740/20000; acc:  76.38; ppl:  2.62; xent: 0.96; lr: 1.00000; 13783/13900 tok/s;    229 sec\n",
            "[2022-02-19 11:19:16,708 INFO] Step 9760/20000; acc:  70.45; ppl:  3.63; xent: 1.29; lr: 1.00000; 14934/14996 tok/s;    229 sec\n",
            "[2022-02-19 11:19:17,142 INFO] Step 9780/20000; acc:  69.00; ppl:  3.70; xent: 1.31; lr: 1.00000; 15775/16421 tok/s;    230 sec\n",
            "[2022-02-19 11:19:17,593 INFO] Step 9800/20000; acc:  71.10; ppl:  3.20; xent: 1.16; lr: 1.00000; 13818/13953 tok/s;    230 sec\n",
            "[2022-02-19 11:19:18,041 INFO] Step 9820/20000; acc:  69.56; ppl:  3.76; xent: 1.32; lr: 1.00000; 14489/14842 tok/s;    231 sec\n",
            "[2022-02-19 11:19:18,520 INFO] Step 9840/20000; acc:  66.92; ppl:  4.16; xent: 1.42; lr: 1.00000; 14945/15697 tok/s;    231 sec\n",
            "[2022-02-19 11:19:18,954 INFO] Step 9860/20000; acc:  73.40; ppl:  2.94; xent: 1.08; lr: 1.00000; 13973/14155 tok/s;    232 sec\n",
            "[2022-02-19 11:19:19,401 INFO] Step 9880/20000; acc:  71.98; ppl:  3.32; xent: 1.20; lr: 1.00000; 13718/14207 tok/s;    232 sec\n",
            "[2022-02-19 11:19:19,863 INFO] Step 9900/20000; acc:  69.60; ppl:  3.65; xent: 1.29; lr: 1.00000; 15368/15632 tok/s;    233 sec\n",
            "[2022-02-19 11:19:20,265 INFO] Step 9920/20000; acc:  72.55; ppl:  3.06; xent: 1.12; lr: 1.00000; 14515/16485 tok/s;    233 sec\n",
            "[2022-02-19 11:19:20,727 INFO] Step 9940/20000; acc:  77.09; ppl:  2.44; xent: 0.89; lr: 1.00000; 12143/12999 tok/s;    233 sec\n",
            "[2022-02-19 11:19:21,204 INFO] Step 9960/20000; acc:  67.75; ppl:  4.22; xent: 1.44; lr: 1.00000; 14497/15033 tok/s;    234 sec\n",
            "[2022-02-19 11:19:21,631 INFO] Step 9980/20000; acc:  72.23; ppl:  3.13; xent: 1.14; lr: 1.00000; 15270/15918 tok/s;    234 sec\n",
            "[2022-02-19 11:19:21,705 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 17\n",
            "[2022-02-19 11:19:22,074 INFO] Step 10000/20000; acc:  74.44; ppl:  2.69; xent: 0.99; lr: 1.00000; 13166/13863 tok/s;    235 sec\n",
            "[2022-02-19 11:19:22,279 INFO] Validation perplexity: 5.85282\n",
            "[2022-02-19 11:19:22,279 INFO] Validation accuracy: 69.3556\n",
            "[2022-02-19 11:19:22,279 INFO] Decreasing patience: 1/3\n",
            "[2022-02-19 11:19:22,329 INFO] Saving checkpoint ./models/best/model_step_10000.pt\n",
            "[2022-02-19 11:19:23,090 INFO] Step 10020/20000; acc:  67.60; ppl:  4.25; xent: 1.45; lr: 1.00000; 6655/6991 tok/s;    236 sec\n",
            "[2022-02-19 11:19:23,548 INFO] Step 10040/20000; acc:  69.69; ppl:  3.58; xent: 1.27; lr: 1.00000; 14842/15857 tok/s;    236 sec\n",
            "[2022-02-19 11:19:23,996 INFO] Step 10060/20000; acc:  73.44; ppl:  2.94; xent: 1.08; lr: 1.00000; 12760/13838 tok/s;    237 sec\n",
            "[2022-02-19 11:19:24,448 INFO] Step 10080/20000; acc:  69.44; ppl:  3.89; xent: 1.36; lr: 1.00000; 15407/15453 tok/s;    237 sec\n",
            "[2022-02-19 11:19:24,916 INFO] Step 10100/20000; acc:  69.13; ppl:  3.75; xent: 1.32; lr: 1.00000; 14837/15469 tok/s;    238 sec\n",
            "[2022-02-19 11:19:25,364 INFO] Step 10120/20000; acc:  73.72; ppl:  2.92; xent: 1.07; lr: 1.00000; 14000/14372 tok/s;    238 sec\n",
            "[2022-02-19 11:19:25,789 INFO] Step 10140/20000; acc:  70.73; ppl:  3.71; xent: 1.31; lr: 1.00000; 15225/15312 tok/s;    239 sec\n",
            "[2022-02-19 11:19:26,246 INFO] Step 10160/20000; acc:  67.86; ppl:  3.85; xent: 1.35; lr: 1.00000; 15273/16262 tok/s;    239 sec\n",
            "[2022-02-19 11:19:26,691 INFO] Step 10180/20000; acc:  74.86; ppl:  2.74; xent: 1.01; lr: 1.00000; 13409/13947 tok/s;    239 sec\n",
            "[2022-02-19 11:19:27,134 INFO] Step 10200/20000; acc:  71.53; ppl:  3.48; xent: 1.25; lr: 1.00000; 13681/14645 tok/s;    240 sec\n",
            "[2022-02-19 11:19:27,581 INFO] Step 10220/20000; acc:  70.06; ppl:  3.45; xent: 1.24; lr: 1.00000; 15547/15808 tok/s;    240 sec\n",
            "[2022-02-19 11:19:27,974 INFO] Step 10240/20000; acc:  74.86; ppl:  2.72; xent: 1.00; lr: 1.00000; 14755/16449 tok/s;    241 sec\n",
            "[2022-02-19 11:19:28,431 INFO] Step 10260/20000; acc:  77.54; ppl:  2.40; xent: 0.88; lr: 1.00000; 12433/12991 tok/s;    241 sec\n",
            "[2022-02-19 11:19:29,023 INFO] Step 10280/20000; acc:  68.24; ppl:  3.97; xent: 1.38; lr: 1.00000; 11911/12124 tok/s;    242 sec\n",
            "[2022-02-19 11:19:29,539 INFO] Step 10300/20000; acc:  71.08; ppl:  3.29; xent: 1.19; lr: 1.00000; 12674/13549 tok/s;    242 sec\n",
            "[2022-02-19 11:19:30,063 INFO] Step 10320/20000; acc:  75.65; ppl:  2.61; xent: 0.96; lr: 1.00000; 11351/11837 tok/s;    243 sec\n",
            "[2022-02-19 11:19:30,522 INFO] Step 10340/20000; acc:  69.94; ppl:  3.88; xent: 1.35; lr: 1.00000; 15047/15290 tok/s;    243 sec\n",
            "[2022-02-19 11:19:30,998 INFO] Step 10360/20000; acc:  68.94; ppl:  3.73; xent: 1.32; lr: 1.00000; 14665/15208 tok/s;    244 sec\n",
            "[2022-02-19 11:19:31,423 INFO] Step 10380/20000; acc:  76.35; ppl:  2.49; xent: 0.91; lr: 1.00000; 13598/13770 tok/s;    244 sec\n",
            "[2022-02-19 11:19:31,866 INFO] Step 10400/20000; acc:  69.92; ppl:  3.76; xent: 1.32; lr: 1.00000; 15528/15280 tok/s;    245 sec\n",
            "[2022-02-19 11:19:32,342 INFO] Step 10420/20000; acc:  68.54; ppl:  3.83; xent: 1.34; lr: 1.00000; 14630/15342 tok/s;    245 sec\n",
            "[2022-02-19 11:19:32,802 INFO] Step 10440/20000; acc:  74.04; ppl:  2.91; xent: 1.07; lr: 1.00000; 13629/13653 tok/s;    246 sec\n",
            "[2022-02-19 11:19:33,248 INFO] Step 10460/20000; acc:  71.82; ppl:  3.37; xent: 1.21; lr: 1.00000; 14175/14976 tok/s;    246 sec\n",
            "[2022-02-19 11:19:33,720 INFO] Step 10480/20000; acc:  68.13; ppl:  3.75; xent: 1.32; lr: 1.00000; 14805/15522 tok/s;    246 sec\n",
            "[2022-02-19 11:19:34,352 INFO] Step 10500/20000; acc:  73.14; ppl:  2.91; xent: 1.07; lr: 1.00000; 13178/13606 tok/s;    247 sec\n",
            "[2022-02-19 11:19:34,778 INFO] Step 10520/20000; acc:  72.15; ppl:  3.26; xent: 1.18; lr: 1.00000; 14362/15257 tok/s;    248 sec\n",
            "[2022-02-19 11:19:35,230 INFO] Step 10540/20000; acc:  70.73; ppl:  3.45; xent: 1.24; lr: 1.00000; 15692/15818 tok/s;    248 sec\n",
            "[2022-02-19 11:19:35,637 INFO] Step 10560/20000; acc:  73.94; ppl:  2.90; xent: 1.07; lr: 1.00000; 14442/16046 tok/s;    248 sec\n",
            "[2022-02-19 11:19:35,645 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 18\n",
            "[2022-02-19 11:19:36,099 INFO] Step 10580/20000; acc:  78.62; ppl:  2.31; xent: 0.84; lr: 1.00000; 12340/12357 tok/s;    249 sec\n",
            "[2022-02-19 11:19:36,590 INFO] Step 10600/20000; acc:  66.09; ppl:  4.66; xent: 1.54; lr: 1.00000; 14191/14997 tok/s;    249 sec\n",
            "[2022-02-19 11:19:37,021 INFO] Step 10620/20000; acc:  71.56; ppl:  3.26; xent: 1.18; lr: 1.00000; 15185/16185 tok/s;    250 sec\n",
            "[2022-02-19 11:19:37,351 INFO] Validation perplexity: 5.79985\n",
            "[2022-02-19 11:19:37,351 INFO] Validation accuracy: 70.3292\n",
            "[2022-02-19 11:19:37,351 INFO] Stalled patience: 2/3\n",
            "[2022-02-19 11:19:37,403 INFO] Saving checkpoint ./models/best/model_step_10625.pt\n",
            "[2022-02-19 11:19:37,978 INFO] Step 10640/20000; acc:  76.46; ppl:  2.50; xent: 0.92; lr: 1.00000; 6092/6434 tok/s;    251 sec\n",
            "[2022-02-19 11:19:38,456 INFO] Step 10660/20000; acc:  69.72; ppl:  3.80; xent: 1.33; lr: 1.00000; 14143/14630 tok/s;    251 sec\n",
            "[2022-02-19 11:19:38,920 INFO] Step 10680/20000; acc:  68.92; ppl:  3.54; xent: 1.26; lr: 1.00000; 14531/15687 tok/s;    252 sec\n",
            "[2022-02-19 11:19:39,345 INFO] Step 10700/20000; acc:  74.56; ppl:  2.71; xent: 1.00; lr: 1.00000; 13513/14352 tok/s;    252 sec\n",
            "[2022-02-19 11:19:39,802 INFO] Step 10720/20000; acc:  69.05; ppl:  3.85; xent: 1.35; lr: 1.00000; 15336/15489 tok/s;    253 sec\n",
            "[2022-02-19 11:19:40,260 INFO] Step 10740/20000; acc:  69.05; ppl:  3.84; xent: 1.34; lr: 1.00000; 15349/16112 tok/s;    253 sec\n",
            "[2022-02-19 11:19:40,703 INFO] Step 10760/20000; acc:  74.34; ppl:  2.76; xent: 1.02; lr: 1.00000; 14180/14135 tok/s;    253 sec\n",
            "[2022-02-19 11:19:41,132 INFO] Step 10780/20000; acc:  71.93; ppl:  3.42; xent: 1.23; lr: 1.00000; 14715/15317 tok/s;    254 sec\n",
            "[2022-02-19 11:19:41,564 INFO] Step 10800/20000; acc:  69.23; ppl:  3.60; xent: 1.28; lr: 1.00000; 15859/16558 tok/s;    254 sec\n",
            "[2022-02-19 11:19:42,028 INFO] Step 10820/20000; acc:  74.65; ppl:  2.80; xent: 1.03; lr: 1.00000; 12545/13759 tok/s;    255 sec\n",
            "[2022-02-19 11:19:42,467 INFO] Step 10840/20000; acc:  73.64; ppl:  2.99; xent: 1.10; lr: 1.00000; 13934/14260 tok/s;    255 sec\n",
            "[2022-02-19 11:19:42,920 INFO] Step 10860/20000; acc:  70.61; ppl:  3.38; xent: 1.22; lr: 1.00000; 15489/15693 tok/s;    256 sec\n",
            "[2022-02-19 11:19:43,321 INFO] Step 10880/20000; acc:  73.63; ppl:  2.91; xent: 1.07; lr: 1.00000; 14547/16561 tok/s;    256 sec\n",
            "[2022-02-19 11:19:43,893 INFO] Step 10900/20000; acc:  77.45; ppl:  2.35; xent: 0.85; lr: 1.00000; 9956/10712 tok/s;    257 sec\n",
            "[2022-02-19 11:19:44,462 INFO] Step 10920/20000; acc:  68.52; ppl:  4.15; xent: 1.42; lr: 1.00000; 12612/12603 tok/s;    257 sec\n",
            "[2022-02-19 11:19:44,962 INFO] Step 10940/20000; acc:  70.30; ppl:  3.43; xent: 1.23; lr: 1.00000; 13210/14118 tok/s;    258 sec\n",
            "[2022-02-19 11:19:45,400 INFO] Step 10960/20000; acc:  76.27; ppl:  2.51; xent: 0.92; lr: 1.00000; 13565/13993 tok/s;    258 sec\n",
            "[2022-02-19 11:19:45,843 INFO] Step 10980/20000; acc:  70.93; ppl:  3.49; xent: 1.25; lr: 1.00000; 15315/15245 tok/s;    259 sec\n",
            "[2022-02-19 11:19:46,301 INFO] Step 11000/20000; acc:  69.20; ppl:  3.68; xent: 1.30; lr: 1.00000; 15070/15813 tok/s;    259 sec\n",
            "[2022-02-19 11:19:46,725 INFO] Step 11020/20000; acc:  76.55; ppl:  2.54; xent: 0.93; lr: 1.00000; 13732/14009 tok/s;    259 sec\n",
            "[2022-02-19 11:19:47,192 INFO] Step 11040/20000; acc:  69.57; ppl:  3.90; xent: 1.36; lr: 1.00000; 15052/15090 tok/s;    260 sec\n",
            "[2022-02-19 11:19:47,659 INFO] Step 11060/20000; acc:  68.43; ppl:  3.83; xent: 1.34; lr: 1.00000; 15174/15749 tok/s;    260 sec\n",
            "[2022-02-19 11:19:48,104 INFO] Step 11080/20000; acc:  72.76; ppl:  2.99; xent: 1.10; lr: 1.00000; 14187/14044 tok/s;    261 sec\n",
            "[2022-02-19 11:19:48,526 INFO] Step 11100/20000; acc:  71.81; ppl:  3.27; xent: 1.19; lr: 1.00000; 14881/15701 tok/s;    261 sec\n",
            "[2022-02-19 11:19:48,984 INFO] Step 11120/20000; acc:  69.16; ppl:  3.60; xent: 1.28; lr: 1.00000; 15174/15766 tok/s;    262 sec\n",
            "[2022-02-19 11:19:49,443 INFO] Step 11140/20000; acc:  74.31; ppl:  2.80; xent: 1.03; lr: 1.00000; 12835/13592 tok/s;    262 sec\n",
            "[2022-02-19 11:19:49,874 INFO] Step 11160/20000; acc:  71.40; ppl:  3.49; xent: 1.25; lr: 1.00000; 14143/14946 tok/s;    263 sec\n",
            "[2022-02-19 11:19:50,322 INFO] Step 11180/20000; acc:  70.01; ppl:  3.48; xent: 1.25; lr: 1.00000; 15668/16163 tok/s;    263 sec\n",
            "[2022-02-19 11:19:50,709 INFO] Step 11200/20000; acc:  74.38; ppl:  2.79; xent: 1.03; lr: 1.00000; 15069/16653 tok/s;    263 sec\n",
            "[2022-02-19 11:19:50,715 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 19\n",
            "[2022-02-19 11:19:51,180 INFO] Step 11220/20000; acc:  78.29; ppl:  2.40; xent: 0.87; lr: 1.00000; 12164/12457 tok/s;    264 sec\n",
            "[2022-02-19 11:19:51,649 INFO] Step 11240/20000; acc:  66.99; ppl:  4.22; xent: 1.44; lr: 1.00000; 14770/15252 tok/s;    264 sec\n",
            "[2022-02-19 11:19:52,091 INFO] Validation perplexity: 6.10905\n",
            "[2022-02-19 11:19:52,091 INFO] Validation accuracy: 69.0311\n",
            "[2022-02-19 11:19:52,091 INFO] Decreasing patience: 0/3\n",
            "[2022-02-19 11:19:52,091 INFO] Training finished after not improving. Early Stop!\n",
            "[2022-02-19 11:19:52,091 INFO] Best model found at step 8750\n",
            "[2022-02-19 11:19:52,143 INFO] Saving checkpoint ./models/best/model_step_11250.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-best.yaml -early_stopping 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhbkBo7cqahE"
      },
      "source": [
        "D'après early_stopping, le meilleur modèle est avec 8750 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt9f7pV7pIKr",
        "outputId": "049dd793-fe1d-414a-c40d-7a8fc30f0b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-19 11:20:42,962 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-19 11:20:47,920 INFO] PRED AVG SCORE: -0.4651, PRED PPL: 1.5922\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -beam_size 3 -model models/best/model_step_8750.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/best/pred_8750.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cldpJPXXpdci",
        "outputId": "478d3fae-84d5-41f7-cce6-222195dedc9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 33.98, 64.2/39.7/29.0/20.2 (BP=0.972, ratio=0.973, hyp_len=3476, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/best/pred_8750.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kEBsNxVp0sB"
      },
      "source": [
        "On obtient un score BLEU de 33,98%.\n",
        "\n",
        "Nous savons qu'il existe sûrement un meilleur jeu de paramètres qui permettrait d'avoir un meilleur score BLEU. Cependant cela est compliqué à réaliser à la main et cela nécessiterait des techniques de cross validation.\n",
        "\n",
        "<span style=\"color:red\"> La cross validation est nécessaire dans le cas où l'on a pas d'ensemble de validation. Dans notre cas nous avons bien un valid set, et donc on peut bien faire un recherche des hyperparametres plus convenant pour notre réseau. </span>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npHT6QdXX-1o",
        "outputId": "689b0ce5-608d-489c-9380-6654dad02974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-26 09:35:02,804 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-26 09:35:09,726 INFO] PRED AVG SCORE: -0.4570, PRED PPL: 1.5793\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -beam_size 3 -model models/best/model_step_8750.pt -src BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -output Traduction/best/pred_dev_8750.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c7YeY79YEE8",
        "outputId": "9eab7c5c-c747-436b-a515-492412858465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 35.69, 66.5/42.2/30.3/21.7 (BP=0.968, ratio=0.969, hyp_len=3689, ref_len=3808)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt < Traduction/best/pred_dev_8750.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BctGTl_iYrow"
      },
      "source": [
        "On voit qu'on obtient un score BLEU de 35,69 % ce qui est meilleur que le score avec le test. Cela est logique car le vocabulaire de dev est semblable à celui du train alors que dans le test, certains mots n'auront jamais été vus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL1AEIm0DDGS"
      },
      "source": [
        "**Q17** Observez les prédictions de votre meilleur modèle. Avez-vous l'impression que la qualité des traductions générées par ce modèle sont meilleures de celles générées par le premier modèle que vous aviez entraîné à la question Q6 ? Donnez quelque exemple. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j738x3JrrX7T"
      },
      "source": [
        "On observe que les prédictions sont meilleures avec notre modèle.\n",
        "Voici un tableau avec quelques exemples :\n",
        "\n",
        "| Attendu  | Modèle de base          | Notre modèle |\n",
        "| :--------------- |:---------------| :-----|\n",
        "| Every ten minutes.  |   All right.        |  Every ten minutes. |\n",
        "| Who is speaking, please?  | Who is the way, please?             |   Who is speaking, please? |\n",
        "| What kind of leather is it?  | What kind of food is it?         |    What kind of leather? |\n",
        "| Do I need a reservation?  | Do I have to change?     |    Do I have to make a reservation? |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S781sw-kC2HU"
      },
      "source": [
        "**Q13 - attention [OPTIONNEL]** : Les architectures avec mécanismes d’attention sont devenus la norme dans de nombreuses applications comme la traduction, la reconnaissance de la parole, la description d’image, etc. ([cet article de blog constitue une excellente introduction en la matière](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)). Ajouter un mécanisme d’attention en changeant le paramètre de _global_attention_ dans le fichier de configuration en le passant de _none_ à _mlp_. Observez-vous un gain de performance ? Pourquoi ? Comment-est ce que c'est mécanisme fonctionne ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxZav3qJZfmW"
      },
      "source": [
        "On réalise cette question optionnelle à la fin. On revient donc au modèle de base pour comparer les changements de paramètre un par un."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKZ_S0v2Zq95",
        "outputId": "52b918c8-2a1f-4602-d57d-e39c7e06910b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-02-26 10:17:09,336 INFO] Missing transforms field for train data, set to default: [].\n",
            "[2022-02-26 10:17:09,340 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n",
            "[2022-02-26 10:17:09,340 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2022-02-26 10:17:09,343 INFO] Parsed 2 corpora from -data.\n",
            "[2022-02-26 10:17:09,344 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2022-02-26 10:17:09,344 INFO] Loading vocab from text file...\n",
            "[2022-02-26 10:17:09,344 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n",
            "[2022-02-26 10:17:09,370 INFO] Loaded src vocab has 9978 tokens.\n",
            "[2022-02-26 10:17:09,384 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n",
            "[2022-02-26 10:17:09,406 INFO] Loaded tgt vocab has 8194 tokens.\n",
            "[2022-02-26 10:17:09,411 INFO] Building fields with vocab in counters...\n",
            "[2022-02-26 10:17:09,420 INFO]  * tgt vocab size: 8198.\n",
            "[2022-02-26 10:17:09,433 INFO]  * src vocab size: 9980.\n",
            "[2022-02-26 10:17:09,434 INFO]  * src vocab size = 9980\n",
            "[2022-02-26 10:17:09,434 INFO]  * tgt vocab size = 8198\n",
            "[2022-02-26 10:17:09,439 INFO] Building model...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-26 10:17:11,946 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(9980, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(8198, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(756, 256)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_context): Linear(in_features=256, out_features=256, bias=False)\n",
            "      (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
            "      (linear_out): Linear(in_features=512, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=8198, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2022-02-26 10:17:11,947 INFO] encoder: 5635120\n",
            "[2022-02-26 10:17:11,947 INFO] decoder: 7507134\n",
            "[2022-02-26 10:17:11,948 INFO] * number of parameters: 13142254\n",
            "[2022-02-26 10:17:11,950 INFO] Starting training on GPU: [0]\n",
            "[2022-02-26 10:17:11,950 INFO] Start training loop and validate every 625 steps...\n",
            "[2022-02-26 10:17:11,950 INFO] train's transforms: TransformPipe()\n",
            "[2022-02-26 10:17:11,951 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 1\n",
            "[2022-02-26 10:17:13,106 INFO] Step 20/ 2000; acc:   7.78; ppl: 43422.72; xent: 10.68; lr: 1.00000; 5926/6098 tok/s;      1 sec\n",
            "[2022-02-26 10:17:14,147 INFO] Step 40/ 2000; acc:  13.07; ppl: 1734.14; xent: 7.46; lr: 1.00000; 6266/6679 tok/s;      2 sec\n",
            "[2022-02-26 10:17:15,021 INFO] Step 60/ 2000; acc:  12.65; ppl: 3044.81; xent: 8.02; lr: 1.00000; 7348/7770 tok/s;      3 sec\n",
            "[2022-02-26 10:17:15,862 INFO] Step 80/ 2000; acc:  16.21; ppl: 2522.28; xent: 7.83; lr: 1.00000; 7264/7269 tok/s;      4 sec\n",
            "[2022-02-26 10:17:16,763 INFO] Step 100/ 2000; acc:  13.97; ppl: 940.70; xent: 6.85; lr: 1.00000; 7270/7496 tok/s;      5 sec\n",
            "[2022-02-26 10:17:17,806 INFO] Step 120/ 2000; acc:  15.80; ppl: 486.84; xent: 6.19; lr: 1.00000; 7110/7290 tok/s;      6 sec\n",
            "[2022-02-26 10:17:18,585 INFO] Step 140/ 2000; acc:  17.50; ppl: 461.76; xent: 6.14; lr: 1.00000; 6755/7281 tok/s;      7 sec\n",
            "[2022-02-26 10:17:19,504 INFO] Step 160/ 2000; acc:  17.06; ppl: 388.19; xent: 5.96; lr: 1.00000; 7640/7833 tok/s;      8 sec\n",
            "[2022-02-26 10:17:20,478 INFO] Step 180/ 2000; acc:  17.80; ppl: 343.53; xent: 5.84; lr: 1.00000; 7092/7501 tok/s;      9 sec\n",
            "[2022-02-26 10:17:21,264 INFO] Step 200/ 2000; acc:  22.05; ppl: 224.07; xent: 5.41; lr: 1.00000; 7057/7688 tok/s;      9 sec\n",
            "[2022-02-26 10:17:22,195 INFO] Step 220/ 2000; acc:  19.64; ppl: 282.56; xent: 5.64; lr: 1.00000; 7383/7716 tok/s;     10 sec\n",
            "[2022-02-26 10:17:23,142 INFO] Step 240/ 2000; acc:  22.85; ppl: 217.62; xent: 5.38; lr: 1.00000; 6825/7146 tok/s;     11 sec\n",
            "[2022-02-26 10:17:23,933 INFO] Step 260/ 2000; acc:  25.73; ppl: 148.54; xent: 5.00; lr: 1.00000; 7042/7414 tok/s;     12 sec\n",
            "[2022-02-26 10:17:24,842 INFO] Step 280/ 2000; acc:  23.33; ppl: 185.53; xent: 5.22; lr: 1.00000; 7319/7850 tok/s;     13 sec\n",
            "[2022-02-26 10:17:25,816 INFO] Step 300/ 2000; acc:  26.05; ppl: 152.68; xent: 5.03; lr: 1.00000; 6699/7007 tok/s;     14 sec\n",
            "[2022-02-26 10:17:26,640 INFO] Step 320/ 2000; acc:  27.19; ppl: 134.18; xent: 4.90; lr: 1.00000; 7362/7759 tok/s;     15 sec\n",
            "[2022-02-26 10:17:27,587 INFO] Step 340/ 2000; acc:  25.80; ppl: 145.44; xent: 4.98; lr: 1.00000; 7350/7140 tok/s;     16 sec\n",
            "[2022-02-26 10:17:28,589 INFO] Step 360/ 2000; acc:  25.69; ppl: 154.34; xent: 5.04; lr: 1.00000; 6666/6884 tok/s;     17 sec\n",
            "[2022-02-26 10:17:29,450 INFO] Step 380/ 2000; acc:  27.69; ppl: 109.88; xent: 4.70; lr: 1.00000; 7582/7811 tok/s;     17 sec\n",
            "[2022-02-26 10:17:30,290 INFO] Step 400/ 2000; acc:  29.68; ppl: 122.48; xent: 4.81; lr: 1.00000; 7335/7320 tok/s;     18 sec\n",
            "[2022-02-26 10:17:31,175 INFO] Step 420/ 2000; acc:  26.64; ppl: 120.23; xent: 4.79; lr: 1.00000; 7443/7745 tok/s;     19 sec\n",
            "[2022-02-26 10:17:32,195 INFO] Step 440/ 2000; acc:  26.38; ppl: 115.16; xent: 4.75; lr: 1.00000; 7324/7294 tok/s;     20 sec\n",
            "[2022-02-26 10:17:33,008 INFO] Step 460/ 2000; acc:  32.46; ppl: 73.54; xent: 4.30; lr: 1.00000; 6445/7061 tok/s;     21 sec\n",
            "[2022-02-26 10:17:33,977 INFO] Step 480/ 2000; acc:  27.43; ppl: 128.11; xent: 4.85; lr: 1.00000; 7191/7440 tok/s;     22 sec\n",
            "[2022-02-26 10:17:34,997 INFO] Step 500/ 2000; acc:  29.37; ppl: 89.36; xent: 4.49; lr: 1.00000; 6704/6904 tok/s;     23 sec\n",
            "[2022-02-26 10:17:35,817 INFO] Step 520/ 2000; acc:  35.41; ppl: 66.91; xent: 4.20; lr: 1.00000; 6746/7317 tok/s;     24 sec\n",
            "[2022-02-26 10:17:36,799 INFO] Step 540/ 2000; acc:  27.97; ppl: 103.20; xent: 4.64; lr: 1.00000; 7175/7508 tok/s;     25 sec\n",
            "[2022-02-26 10:17:37,703 INFO] Step 560/ 2000; acc:  31.57; ppl: 91.11; xent: 4.51; lr: 1.00000; 7223/7485 tok/s;     26 sec\n",
            "[2022-02-26 10:17:38,320 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 2\n",
            "[2022-02-26 10:17:38,513 INFO] Step 580/ 2000; acc:  35.84; ppl: 63.67; xent: 4.15; lr: 1.00000; 6913/7457 tok/s;     27 sec\n",
            "[2022-02-26 10:17:39,412 INFO] Step 600/ 2000; acc:  31.35; ppl: 79.89; xent: 4.38; lr: 1.00000; 7414/7745 tok/s;     27 sec\n",
            "[2022-02-26 10:17:40,366 INFO] Step 620/ 2000; acc:  33.01; ppl: 78.67; xent: 4.37; lr: 1.00000; 6784/6932 tok/s;     28 sec\n",
            "[2022-02-26 10:17:40,605 INFO] valid's transforms: TransformPipe()\n",
            "[2022-02-26 10:17:41,059 INFO] Validation perplexity: 30.8565\n",
            "[2022-02-26 10:17:41,060 INFO] Validation accuracy: 44.5758\n",
            "[2022-02-26 10:17:41,120 INFO] Saving checkpoint ./models/attention/model_step_625.pt\n",
            "[2022-02-26 10:17:41,978 INFO] Step 640/ 2000; acc:  34.43; ppl: 62.90; xent: 4.14; lr: 1.00000; 3754/4051 tok/s;     30 sec\n",
            "[2022-02-26 10:17:43,007 INFO] Step 660/ 2000; acc:  31.48; ppl: 71.22; xent: 4.27; lr: 1.00000; 6586/6795 tok/s;     31 sec\n",
            "[2022-02-26 10:17:44,058 INFO] Step 680/ 2000; acc:  31.83; ppl: 75.42; xent: 4.32; lr: 1.00000; 6168/6608 tok/s;     32 sec\n",
            "[2022-02-26 10:17:44,928 INFO] Step 700/ 2000; acc:  35.67; ppl: 58.67; xent: 4.07; lr: 1.00000; 7358/7599 tok/s;     33 sec\n",
            "[2022-02-26 10:17:45,759 INFO] Step 720/ 2000; acc:  35.37; ppl: 61.65; xent: 4.12; lr: 1.00000; 7380/7545 tok/s;     34 sec\n",
            "[2022-02-26 10:17:46,772 INFO] Step 740/ 2000; acc:  33.89; ppl: 62.68; xent: 4.14; lr: 1.00000; 6560/6746 tok/s;     35 sec\n",
            "[2022-02-26 10:17:47,808 INFO] Step 760/ 2000; acc:  33.19; ppl: 70.77; xent: 4.26; lr: 1.00000; 7303/7287 tok/s;     36 sec\n",
            "[2022-02-26 10:17:48,621 INFO] Step 780/ 2000; acc:  39.04; ppl: 44.45; xent: 3.79; lr: 1.00000; 6336/7321 tok/s;     37 sec\n",
            "[2022-02-26 10:17:49,639 INFO] Step 800/ 2000; acc:  34.78; ppl: 59.42; xent: 4.08; lr: 1.00000; 6723/6988 tok/s;     38 sec\n",
            "[2022-02-26 10:17:50,763 INFO] Step 820/ 2000; acc:  34.86; ppl: 61.39; xent: 4.12; lr: 1.00000; 6018/6359 tok/s;     39 sec\n",
            "[2022-02-26 10:17:51,584 INFO] Step 840/ 2000; acc:  41.82; ppl: 37.98; xent: 3.64; lr: 1.00000; 6714/7093 tok/s;     40 sec\n",
            "[2022-02-26 10:17:52,518 INFO] Step 860/ 2000; acc:  34.38; ppl: 60.93; xent: 4.11; lr: 1.00000; 7450/7660 tok/s;     41 sec\n",
            "[2022-02-26 10:17:53,467 INFO] Step 880/ 2000; acc:  36.35; ppl: 55.11; xent: 4.01; lr: 1.00000; 6885/7202 tok/s;     42 sec\n",
            "[2022-02-26 10:17:54,326 INFO] Step 900/ 2000; acc:  40.89; ppl: 41.08; xent: 3.72; lr: 1.00000; 6522/6951 tok/s;     42 sec\n",
            "[2022-02-26 10:17:55,293 INFO] Step 920/ 2000; acc:  36.49; ppl: 49.40; xent: 3.90; lr: 1.00000; 7002/7280 tok/s;     43 sec\n",
            "[2022-02-26 10:17:56,346 INFO] Step 940/ 2000; acc:  36.29; ppl: 54.61; xent: 4.00; lr: 1.00000; 6305/6591 tok/s;     44 sec\n",
            "[2022-02-26 10:17:57,392 INFO] Step 960/ 2000; acc:  40.20; ppl: 41.50; xent: 3.73; lr: 1.00000; 5870/6122 tok/s;     45 sec\n",
            "[2022-02-26 10:17:58,396 INFO] Step 980/ 2000; acc:  38.21; ppl: 45.67; xent: 3.82; lr: 1.00000; 6863/6615 tok/s;     46 sec\n",
            "[2022-02-26 10:17:59,353 INFO] Step 1000/ 2000; acc:  36.27; ppl: 47.99; xent: 3.87; lr: 1.00000; 6791/7172 tok/s;     47 sec\n",
            "[2022-02-26 10:18:00,195 INFO] Step 1020/ 2000; acc:  38.95; ppl: 39.58; xent: 3.68; lr: 1.00000; 7681/7925 tok/s;     48 sec\n",
            "[2022-02-26 10:18:01,049 INFO] Step 1040/ 2000; acc:  39.71; ppl: 41.87; xent: 3.73; lr: 1.00000; 7201/7384 tok/s;     49 sec\n",
            "[2022-02-26 10:18:01,965 INFO] Step 1060/ 2000; acc:  37.80; ppl: 42.93; xent: 3.76; lr: 1.00000; 7334/7268 tok/s;     50 sec\n",
            "[2022-02-26 10:18:03,021 INFO] Step 1080/ 2000; acc:  35.43; ppl: 53.00; xent: 3.97; lr: 1.00000; 7222/7232 tok/s;     51 sec\n",
            "[2022-02-26 10:18:03,843 INFO] Step 1100/ 2000; acc:  41.35; ppl: 33.93; xent: 3.52; lr: 1.00000; 6346/7107 tok/s;     52 sec\n",
            "[2022-02-26 10:18:04,759 INFO] Step 1120/ 2000; acc:  36.73; ppl: 48.77; xent: 3.89; lr: 1.00000; 7607/7741 tok/s;     53 sec\n",
            "[2022-02-26 10:18:05,722 INFO] Step 1140/ 2000; acc:  38.15; ppl: 40.87; xent: 3.71; lr: 1.00000; 7002/7404 tok/s;     54 sec\n",
            "[2022-02-26 10:18:06,541 INFO] Step 1160/ 2000; acc:  43.53; ppl: 31.35; xent: 3.45; lr: 1.00000; 6705/7330 tok/s;     55 sec\n",
            "[2022-02-26 10:18:07,480 INFO] Step 1180/ 2000; acc:  37.62; ppl: 43.72; xent: 3.78; lr: 1.00000; 7444/7680 tok/s;     56 sec\n",
            "[2022-02-26 10:18:08,402 INFO] Step 1200/ 2000; acc:  41.01; ppl: 37.02; xent: 3.61; lr: 1.00000; 7032/7277 tok/s;     56 sec\n",
            "[2022-02-26 10:18:09,067 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 3\n",
            "[2022-02-26 10:18:09,315 INFO] Step 1220/ 2000; acc:  41.86; ppl: 32.64; xent: 3.49; lr: 1.00000; 6073/6595 tok/s;     57 sec\n",
            "[2022-02-26 10:18:10,245 INFO] Step 1240/ 2000; acc:  37.79; ppl: 40.06; xent: 3.69; lr: 1.00000; 7208/7583 tok/s;     58 sec\n",
            "[2022-02-26 10:18:11,096 INFO] Validation perplexity: 21.2346\n",
            "[2022-02-26 10:18:11,097 INFO] Validation accuracy: 47.8442\n",
            "[2022-02-26 10:18:11,158 INFO] Saving checkpoint ./models/attention/model_step_1250.pt\n",
            "[2022-02-26 10:18:11,941 INFO] Step 1260/ 2000; acc:  39.63; ppl: 37.89; xent: 3.63; lr: 1.00000; 3789/4026 tok/s;     60 sec\n",
            "[2022-02-26 10:18:12,784 INFO] Step 1280/ 2000; acc:  42.78; ppl: 32.22; xent: 3.47; lr: 1.00000; 7293/7572 tok/s;     61 sec\n",
            "[2022-02-26 10:18:13,766 INFO] Step 1300/ 2000; acc:  42.25; ppl: 32.22; xent: 3.47; lr: 1.00000; 7034/6940 tok/s;     62 sec\n",
            "[2022-02-26 10:18:14,824 INFO] Step 1320/ 2000; acc:  38.70; ppl: 38.68; xent: 3.66; lr: 1.00000; 6187/6645 tok/s;     63 sec\n",
            "[2022-02-26 10:18:15,699 INFO] Step 1340/ 2000; acc:  41.49; ppl: 31.83; xent: 3.46; lr: 1.00000; 7382/7729 tok/s;     64 sec\n",
            "[2022-02-26 10:18:16,556 INFO] Step 1360/ 2000; acc:  43.11; ppl: 31.95; xent: 3.46; lr: 1.00000; 7125/7170 tok/s;     65 sec\n",
            "[2022-02-26 10:18:17,482 INFO] Step 1380/ 2000; acc:  40.31; ppl: 36.80; xent: 3.61; lr: 1.00000; 7136/7415 tok/s;     66 sec\n",
            "[2022-02-26 10:18:18,516 INFO] Step 1400/ 2000; acc:  38.38; ppl: 39.08; xent: 3.67; lr: 1.00000; 7290/7421 tok/s;     67 sec\n",
            "[2022-02-26 10:18:19,327 INFO] Step 1420/ 2000; acc:  45.81; ppl: 25.24; xent: 3.23; lr: 1.00000; 6290/7166 tok/s;     67 sec\n",
            "[2022-02-26 10:18:20,482 INFO] Step 1440/ 2000; acc:  41.78; ppl: 33.65; xent: 3.52; lr: 1.00000; 5790/6082 tok/s;     69 sec\n",
            "[2022-02-26 10:18:21,456 INFO] Step 1460/ 2000; acc:  42.40; ppl: 32.31; xent: 3.48; lr: 1.00000; 6848/7040 tok/s;     70 sec\n",
            "[2022-02-26 10:18:22,375 INFO] Step 1480/ 2000; acc:  46.51; ppl: 23.50; xent: 3.16; lr: 1.00000; 5884/6696 tok/s;     70 sec\n",
            "[2022-02-26 10:18:23,312 INFO] Step 1500/ 2000; acc:  41.01; ppl: 33.13; xent: 3.50; lr: 1.00000; 7536/7617 tok/s;     71 sec\n",
            "[2022-02-26 10:18:24,269 INFO] Step 1520/ 2000; acc:  41.42; ppl: 31.19; xent: 3.44; lr: 1.00000; 6833/7325 tok/s;     72 sec\n",
            "[2022-02-26 10:18:25,151 INFO] Step 1540/ 2000; acc:  47.64; ppl: 22.22; xent: 3.10; lr: 1.00000; 6405/6769 tok/s;     73 sec\n",
            "[2022-02-26 10:18:26,070 INFO] Step 1560/ 2000; acc:  42.52; ppl: 29.80; xent: 3.39; lr: 1.00000; 7484/7385 tok/s;     74 sec\n",
            "[2022-02-26 10:18:27,037 INFO] Step 1580/ 2000; acc:  40.34; ppl: 34.93; xent: 3.55; lr: 1.00000; 6947/7242 tok/s;     75 sec\n",
            "[2022-02-26 10:18:27,863 INFO] Step 1600/ 2000; acc:  45.25; ppl: 24.56; xent: 3.20; lr: 1.00000; 7496/7875 tok/s;     76 sec\n",
            "[2022-02-26 10:18:28,831 INFO] Step 1620/ 2000; acc:  42.84; ppl: 29.94; xent: 3.40; lr: 1.00000; 7149/6949 tok/s;     77 sec\n",
            "[2022-02-26 10:18:29,803 INFO] Step 1640/ 2000; acc:  41.73; ppl: 32.12; xent: 3.47; lr: 1.00000; 6773/7152 tok/s;     78 sec\n",
            "[2022-02-26 10:18:30,666 INFO] Step 1660/ 2000; acc:  43.01; ppl: 25.94; xent: 3.26; lr: 1.00000; 7528/7725 tok/s;     79 sec\n",
            "[2022-02-26 10:18:31,509 INFO] Step 1680/ 2000; acc:  46.29; ppl: 24.32; xent: 3.19; lr: 1.00000; 7253/7107 tok/s;     80 sec\n",
            "[2022-02-26 10:18:32,443 INFO] Step 1700/ 2000; acc:  43.00; ppl: 25.54; xent: 3.24; lr: 1.00000; 7032/7435 tok/s;     80 sec\n",
            "[2022-02-26 10:18:33,526 INFO] Step 1720/ 2000; acc:  39.51; ppl: 35.49; xent: 3.57; lr: 1.00000; 6906/6921 tok/s;     82 sec\n",
            "[2022-02-26 10:18:34,345 INFO] Step 1740/ 2000; acc:  48.70; ppl: 19.45; xent: 2.97; lr: 1.00000; 6312/7162 tok/s;     82 sec\n",
            "[2022-02-26 10:18:35,324 INFO] Step 1760/ 2000; acc:  41.89; ppl: 31.14; xent: 3.44; lr: 1.00000; 7105/7261 tok/s;     83 sec\n",
            "[2022-02-26 10:18:36,266 INFO] Step 1780/ 2000; acc:  43.06; ppl: 26.52; xent: 3.28; lr: 1.00000; 7186/7502 tok/s;     84 sec\n",
            "[2022-02-26 10:18:37,148 INFO] Step 1800/ 2000; acc:  48.86; ppl: 19.81; xent: 2.99; lr: 1.00000; 6218/6778 tok/s;     85 sec\n",
            "[2022-02-26 10:18:38,089 INFO] Step 1820/ 2000; acc:  42.36; ppl: 28.20; xent: 3.34; lr: 1.00000; 7408/7599 tok/s;     86 sec\n",
            "[2022-02-26 10:18:39,039 INFO] Step 1840/ 2000; acc:  46.47; ppl: 23.22; xent: 3.15; lr: 1.00000; 6741/7005 tok/s;     87 sec\n",
            "[2022-02-26 10:18:39,721 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* train: 4\n",
            "[2022-02-26 10:18:39,927 INFO] Step 1860/ 2000; acc:  47.62; ppl: 20.38; xent: 3.01; lr: 1.00000; 6257/6763 tok/s;     88 sec\n",
            "[2022-02-26 10:18:41,124 INFO] Validation perplexity: 15.4759\n",
            "[2022-02-26 10:18:41,125 INFO] Validation accuracy: 50.3245\n",
            "[2022-02-26 10:18:41,186 INFO] Saving checkpoint ./models/attention/model_step_1875.pt\n",
            "[2022-02-26 10:18:41,583 INFO] Step 1880/ 2000; acc:  44.88; ppl: 24.49; xent: 3.20; lr: 1.00000; 4094/4203 tok/s;     90 sec\n",
            "[2022-02-26 10:18:42,605 INFO] Step 1900/ 2000; acc:  43.23; ppl: 25.75; xent: 3.25; lr: 1.00000; 6367/6896 tok/s;     91 sec\n",
            "[2022-02-26 10:18:43,475 INFO] Step 1920/ 2000; acc:  47.27; ppl: 20.79; xent: 3.03; lr: 1.00000; 7085/7428 tok/s;     92 sec\n",
            "[2022-02-26 10:18:44,464 INFO] Step 1940/ 2000; acc:  46.45; ppl: 22.64; xent: 3.12; lr: 1.00000; 7035/6887 tok/s;     93 sec\n",
            "[2022-02-26 10:18:45,542 INFO] Step 1960/ 2000; acc:  43.24; ppl: 26.23; xent: 3.27; lr: 1.00000; 6092/6587 tok/s;     94 sec\n",
            "[2022-02-26 10:18:46,504 INFO] Step 1980/ 2000; acc:  46.75; ppl: 20.77; xent: 3.03; lr: 1.00000; 6758/7077 tok/s;     95 sec\n",
            "[2022-02-26 10:18:47,446 INFO] Step 2000/ 2000; acc:  48.18; ppl: 20.71; xent: 3.03; lr: 1.00000; 6505/6526 tok/s;     95 sec\n",
            "[2022-02-26 10:18:47,509 INFO] Saving checkpoint ./models/attention/model_step_2000.pt\n"
          ]
        }
      ],
      "source": [
        "!onmt_train -config config-base-attention.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdhttPFMaiuC",
        "outputId": "c341f47f-e8dc-4a27-f6b4-b1ff55b20dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "[2022-02-26 10:18:50,342 INFO] Translating shard 0.\n",
            "/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self._batch_index = self.topk_ids // vocab_size\n",
            "[2022-02-26 10:18:58,933 INFO] PRED AVG SCORE: -1.2705, PRED PPL: 3.5626\n"
          ]
        }
      ],
      "source": [
        "!onmt_translate -beam_size 3 -model models/attention/model_step_2000.pt -src BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output Traduction/attention/pred_2000.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI5NrtgJao7H",
        "outputId": "80b5da8d-7f20-482d-d025-52d25fff8381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 9.25, 28.8/12.8/6.9/2.9 (BP=1.000, ratio=1.586, hyp_len=5668, ref_len=3574)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < Traduction/attention/pred_2000.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgbOpZJPa0x0"
      },
      "source": [
        "On n'observe pas de gain de performance, on a même une diminution du score BLEU (de 18,48% à 9.25%). Une hypothèse serait selon nous qu'il faut mettre en place d'autres configurations pour que cette technique améliore la performance.\n",
        "\n",
        "Ce mécanisme consiste à se focaliser sur des aspects en particulier. L'attention va aider à avoir un contexte pour chacun des mots.\n",
        "\n",
        "<span style=\"color:red\"> Bravo pour avoir essayé! Vous n'observé pas d'amélioration car votre modèle est sous entrainé (vous entrainé pour 2000 steps seulements). En effet, en rajoutant le mechanisme d'attention on rajoute des paramètres au modèle, ce qui implique que notre modèle aura besoin de plus de temps d'entrainement. </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TP_nlp_2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
