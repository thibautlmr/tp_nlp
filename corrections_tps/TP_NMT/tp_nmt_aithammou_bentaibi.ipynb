{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2773,"status":"ok","timestamp":1647087355386,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"y1NaZJSs9jAs","outputId":"08dd202b-5f31-4c2f-bc02-75fbf6e8dab9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: OpenNMT-py in /usr/local/lib/python3.7/dist-packages (2.2.0)\n","Requirement already satisfied: pyonmttok<2,>=1.23 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.31.0)\n","Requirement already satisfied: waitress in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.1.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.1.4)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.10.0+cu111)\n","Requirement already satisfied: configargparse in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (1.5.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (3.13)\n","Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (0.5.0)\n","Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py) (2.8.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (0.1.96)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py) (1.15.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (57.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.17.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.44.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.2.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py) (2.0.1)\n"]}],"source":["!pip install OpenNMT-py"]},{"cell_type":"markdown","metadata":{"id":"zwscrcG39q9Y"},"source":["Q1)Le fichier IWSLT10_BTEC.train.fr.txt contient un ensemble de phrases en français\n","et le fichier IWSLT10_BTEC.train.en.txt nous donne une traduction en anglais de chaque phrase de cet ensemble ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1647087360826,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"5_-UCGxk9uex","outputId":"b7ac8e38-110d-4ef8-85a1-ee9a884ab1a5"},"outputs":[{"data":{"text/plain":[]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd /content/drive/MyDrive/tp_nmt/BTEC-en-fr/train/\n","awk -F '\\' '{print $NF}' IWSLT10_BTEC.train.en.txt > IWSLT10_BTEC.train.en.clean.txt\n","awk -F '\\' '{print $NF}' IWSLT10_BTEC.train.fr.txt > IWSLT10_BTEC.train.fr.clean.txt\n","\n","cd /content/drive/MyDrive/tp_nmt/BTEC-en-fr/dev/\n","awk -F '\\' '{print $NF}' IWSLT10.devset1_CSTAR03.en.txt > IWSLT10.devset1_CSTAR03.en.clean.txt\n","awk -F '\\' '{print $NF}' IWSLT10.devset1_CSTAR03.fr.txt > IWSLT10.devset1_CSTAR03.fr.clean.txt\n","\n","cd /content/drive/MyDrive/tp_nmt/BTEC-en-fr/test\n","awk -F '\\' '{print $NF}' IWSLT09_BTEC.testset.en.txt > IWSLT09_BTEC.testset.en.clean.txt\n","awk -F '\\' '{print $NF}' IWSLT09_BTEC.testset.fr.txt > IWSLT09_BTEC.testset.fr.clean.txt"]},{"cell_type":"markdown","metadata":{"id":"r18VOzX3-0vu"},"source":["Q2) La difference reside dans la representation des apostrophes et les guillemets , on remarque qu'il sont remplcés par leurs tokens dans les fichiers .tok\n","(&apos &quot)\n","Le but de l'opération effectuée (escaping) est de rendre les textes plus faciles a traiter par python .\n","\n","<span style=\"color:red\">La difference importante est representé par les espaces entroduit entre chanque mot et la ponctuation. Le but de cette stratégie basique de tokenization est de réduire la taille du vocabulaire. Si on ne sépare pas les mots et la ponctuation on sera obligé à avoir plus de tokens dans le vocabularie. E.g. \"Salut\", \"Salut!\", \"Salut,\", \"Salut?\", etc. </span>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1801,"status":"ok","timestamp":1647087364350,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"qGnH_G0L-yG3","outputId":"fd041a98-20c1-42b9-92b3-e6dea6f87f77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","Tokenizer Version 1.1\n","Language: fr\n","Number of threads: 1\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","Tokenizer Version 1.1\n","Language: fr\n","Number of threads: 1\n","Tokenizer Version 1.1\n","Language: en\n","Number of threads: 1\n","Tokenizer Version 1.1\n","Language: fr\n","Number of threads: 1\n"]},{"data":{"text/plain":[]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","\n","perl tokenizer.perl -l en -lc < ./BTEC-en-fr/train/IWSLT10_BTEC.train.en.clean.txt > ./BTEC-en-fr/train/IWSLT10_BTEC.train.en.tok.txt\n","perl tokenizer.perl -l fr -lc < ./BTEC-en-fr/train/IWSLT10_BTEC.train.fr.clean.txt > ./BTEC-en-fr/train/IWSLT10_BTEC.train.fr.tok.txt\n","\n","\n","perl tokenizer.perl -l en -lc < ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.clean.txt > ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt\n","perl tokenizer.perl -l fr -lc < ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.clean.txt > ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt\n","\n","perl tokenizer.perl -l en -lc < ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.clean.txt > ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt\n","perl tokenizer.perl -l fr -lc < ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.clean.txt > ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt\n"]},{"cell_type":"markdown","metadata":{"id":"VUDWIPaT_rqW"},"source":["Q3)Pour la phase de training on utilise l'ensemble train pour trouver les paramètres de modele .On utilise l'ensemble dev pour  comparer les modèles qu'on a créé afin de trouver/choisir le meilleur, ensuite on utilise l'ensemble test pour effectuer une évaluation(pseudo finale) à nouveau de ce modele . \n","Il est important de garder les mêmes sets pendant les différentes opérations afin d de comparer/évaluer les modèles correctement. et (en général il est aussi recommandé que ces sets aient la même distribution)\n","\n","<span style=\"color:red\">Ca veut dire quoi \"pséudo finale\"? </span>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2113,"status":"ok","timestamp":1647087414026,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"uj3dWSUpBfN0","outputId":"cf656353-d7a2-415d-b363-61c900a255e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Corpus train's weight should be given. We default it to 1 for you.\n","[2022-03-12 12:16:52,523 INFO] Counter vocab from -1 samples.\n","[2022-03-12 12:16:52,523 INFO] n_sample=-1: Build vocab on full datasets.\n","[2022-03-12 12:16:52,533 INFO] train's transforms: TransformPipe()\n","[2022-03-12 12:16:52,838 INFO] Counters src:9978\n","[2022-03-12 12:16:52,838 INFO] Counters tgt:8194\n"]},{"data":{"text/plain":[]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_build_vocab -config config-base.yaml -n_sample -1"]},{"cell_type":"markdown","metadata":{"id":"IF1UXWvbB3Iu"},"source":["\n","Q4) un système de TA utilise le vocabulaire pour faciliter la phase de training.Concernant le choix de vocabulaire , on utilise les données de training pour garder un context/vocabulaire ciblé et specifique au domaine de modele .\n","\n","  **Le vocabulaire permet d'associer à chaque mot un idéntifiant unique et un vecteur qui le represente et dont les valuers seronts appris par le réseau à travers l'entrainement. Les tokens du vocabulaire basé sur l'ensemble d'apprentissage sont suffisants pour l’entraînement du modèle parce que pendent l’entraînement le modèle ne voit que des phrases issues du training set. En autre, utiliser un vocabulaire plus vaste serait inefficace et gourmand en calcul et en mémoire, car des tokens de ce vocabulaire ne seront jamais utilisés pour l’entraînement.**\n","\n","Q5) Les deux lignes représentent le nombre de lexèmes detectés dans la source et la cible."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495255,"status":"ok","timestamp":1647093275184,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"LXvACU7YCgBV","outputId":"a4b21cd7-0ff5-450d-c370-0ad7ee3ec731"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-03-12 13:46:20,524 WARNING] You have a CUDA device, should run with -gpu_ranks\n","[2022-03-12 13:46:20,526 INFO] Missing transforms field for train data, set to default: [].\n","[2022-03-12 13:46:20,527 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-03-12 13:46:20,527 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-03-12 13:46:20,527 INFO] Parsed 2 corpora from -data.\n","[2022-03-12 13:46:20,528 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-03-12 13:46:20,528 INFO] Loading vocab from text file...\n","[2022-03-12 13:46:20,528 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-03-12 13:46:20,548 INFO] Loaded src vocab has 9978 tokens.\n","[2022-03-12 13:46:20,552 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-03-12 13:46:20,581 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-03-12 13:46:20,584 INFO] Building fields with vocab in counters...\n","[2022-03-12 13:46:20,593 INFO]  * tgt vocab size: 8198.\n","[2022-03-12 13:46:20,604 INFO]  * src vocab size: 9980.\n","[2022-03-12 13:46:20,605 INFO]  * src vocab size = 9980\n","[2022-03-12 13:46:20,605 INFO]  * tgt vocab size = 8198\n","[2022-03-12 13:46:20,608 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:46:20,788 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(9980, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-03-12 13:46:20,789 INFO] encoder: 5635120\n","[2022-03-12 13:46:20,789 INFO] decoder: 7244222\n","[2022-03-12 13:46:20,789 INFO] * number of parameters: 12879342\n","[2022-03-12 13:46:20,790 INFO] Starting training on CPU, could be very slow\n","[2022-03-12 13:46:20,791 INFO] Start training loop and validate every 625 steps...\n","[2022-03-12 13:46:20,791 INFO] train's transforms: TransformPipe()\n","[2022-03-12 13:46:20,791 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-03-12 13:46:25,618 INFO] Step 20/ 2000; acc:   8.33; ppl: 9511.50; xent: 9.16; lr: 1.00000; 1317/1338 tok/s;      5 sec\n","[2022-03-12 13:46:31,785 INFO] Step 40/ 2000; acc:  13.62; ppl: 528.93; xent: 6.27; lr: 1.00000; 1246/1287 tok/s;     11 sec\n","[2022-03-12 13:46:36,270 INFO] Step 60/ 2000; acc:  19.70; ppl: 229.52; xent: 5.44; lr: 1.00000; 1206/1303 tok/s;     15 sec\n","[2022-03-12 13:46:40,984 INFO] Step 80/ 2000; acc:  19.15; ppl: 213.71; xent: 5.36; lr: 1.00000; 1310/1370 tok/s;     20 sec\n","[2022-03-12 13:46:46,122 INFO] Step 100/ 2000; acc:  20.06; ppl: 196.66; xent: 5.28; lr: 1.00000; 1338/1372 tok/s;     25 sec\n","[2022-03-12 13:46:51,365 INFO] Step 120/ 2000; acc:  22.48; ppl: 179.94; xent: 5.19; lr: 1.00000; 1290/1327 tok/s;     31 sec\n","[2022-03-12 13:46:56,055 INFO] Step 140/ 2000; acc:  25.03; ppl: 120.21; xent: 4.79; lr: 1.00000; 1289/1361 tok/s;     35 sec\n","[2022-03-12 13:47:01,341 INFO] Step 160/ 2000; acc:  24.78; ppl: 128.86; xent: 4.86; lr: 1.00000; 1311/1376 tok/s;     41 sec\n","[2022-03-12 13:47:06,147 INFO] Step 180/ 2000; acc:  27.77; ppl: 120.43; xent: 4.79; lr: 1.00000; 1387/1409 tok/s;     45 sec\n","[2022-03-12 13:47:10,666 INFO] Step 200/ 2000; acc:  31.38; ppl: 86.24; xent: 4.46; lr: 1.00000; 1333/1402 tok/s;     50 sec\n","[2022-03-12 13:47:15,202 INFO] Step 220/ 2000; acc:  33.29; ppl: 80.15; xent: 4.38; lr: 1.00000; 1356/1420 tok/s;     54 sec\n","[2022-03-12 13:47:20,863 INFO] Step 240/ 2000; acc:  28.39; ppl: 95.31; xent: 4.56; lr: 1.00000; 1209/1294 tok/s;     60 sec\n","[2022-03-12 13:47:25,061 INFO] Step 260/ 2000; acc:  36.21; ppl: 65.11; xent: 4.18; lr: 1.00000; 1345/1470 tok/s;     64 sec\n","[2022-03-12 13:47:29,845 INFO] Step 280/ 2000; acc:  33.24; ppl: 70.40; xent: 4.25; lr: 1.00000; 1352/1421 tok/s;     69 sec\n","[2022-03-12 13:47:35,402 INFO] Step 300/ 2000; acc:  30.77; ppl: 83.44; xent: 4.42; lr: 1.00000; 1316/1338 tok/s;     75 sec\n","[2022-03-12 13:47:39,595 INFO] Step 320/ 2000; acc:  38.97; ppl: 52.75; xent: 3.97; lr: 1.00000; 1280/1420 tok/s;     79 sec\n","[2022-03-12 13:47:44,423 INFO] Step 340/ 2000; acc:  35.46; ppl: 56.18; xent: 4.03; lr: 1.00000; 1327/1366 tok/s;     84 sec\n","[2022-03-12 13:47:50,018 INFO] Step 360/ 2000; acc:  32.17; ppl: 73.89; xent: 4.30; lr: 1.00000; 1400/1365 tok/s;     89 sec\n","[2022-03-12 13:47:54,344 INFO] Step 380/ 2000; acc:  39.53; ppl: 46.54; xent: 3.84; lr: 1.00000; 1282/1343 tok/s;     94 sec\n","[2022-03-12 13:47:58,981 INFO] Step 400/ 2000; acc:  35.99; ppl: 53.87; xent: 3.99; lr: 1.00000; 1340/1372 tok/s;     98 sec\n","[2022-03-12 13:48:03,952 INFO] Step 420/ 2000; acc:  35.88; ppl: 55.04; xent: 4.01; lr: 1.00000; 1395/1421 tok/s;    103 sec\n","[2022-03-12 13:48:09,096 INFO] Step 440/ 2000; acc:  34.43; ppl: 62.45; xent: 4.13; lr: 1.00000; 1319/1338 tok/s;    108 sec\n","[2022-03-12 13:48:13,657 INFO] Step 460/ 2000; acc:  38.41; ppl: 51.48; xent: 3.94; lr: 1.00000; 1324/1334 tok/s;    113 sec\n","[2022-03-12 13:48:18,770 INFO] Step 480/ 2000; acc:  35.67; ppl: 53.57; xent: 3.98; lr: 1.00000; 1352/1407 tok/s;    118 sec\n","[2022-03-12 13:48:24,294 INFO] Step 500/ 2000; acc:  36.64; ppl: 51.93; xent: 3.95; lr: 1.00000; 1189/1226 tok/s;    124 sec\n","[2022-03-12 13:48:28,770 INFO] Step 520/ 2000; acc:  40.21; ppl: 43.93; xent: 3.78; lr: 1.00000; 1346/1416 tok/s;    128 sec\n","[2022-03-12 13:48:33,382 INFO] Step 540/ 2000; acc:  40.19; ppl: 41.71; xent: 3.73; lr: 1.00000; 1346/1431 tok/s;    133 sec\n","[2022-03-12 13:48:38,895 INFO] Step 560/ 2000; acc:  36.05; ppl: 53.69; xent: 3.98; lr: 1.00000; 1273/1360 tok/s;    138 sec\n","[2022-03-12 13:48:42,259 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-03-12 13:48:43,114 INFO] Step 580/ 2000; acc:  42.78; ppl: 34.22; xent: 3.53; lr: 1.00000; 1346/1436 tok/s;    142 sec\n","[2022-03-12 13:48:48,061 INFO] Step 600/ 2000; acc:  38.49; ppl: 43.47; xent: 3.77; lr: 1.00000; 1309/1383 tok/s;    147 sec\n","[2022-03-12 13:48:53,701 INFO] Step 620/ 2000; acc:  35.74; ppl: 51.35; xent: 3.94; lr: 1.00000; 1278/1331 tok/s;    153 sec\n","[2022-03-12 13:48:54,611 INFO] valid's transforms: TransformPipe()\n","[2022-03-12 13:48:56,024 INFO] Validation perplexity: 21.4624\n","[2022-03-12 13:48:56,025 INFO] Validation accuracy: 49.3973\n","[2022-03-12 13:48:56,073 INFO] Saving checkpoint ./models/base/model_step_625.pt\n","[2022-03-12 13:48:59,508 INFO] Step 640/ 2000; acc:  44.04; ppl: 33.07; xent: 3.50; lr: 1.00000; 925/991 tok/s;    159 sec\n","[2022-03-12 13:49:04,191 INFO] Step 660/ 2000; acc:  40.37; ppl: 39.12; xent: 3.67; lr: 1.00000; 1342/1401 tok/s;    163 sec\n","[2022-03-12 13:49:10,311 INFO] Step 680/ 2000; acc:  37.04; ppl: 46.41; xent: 3.84; lr: 1.00000; 1245/1252 tok/s;    170 sec\n","[2022-03-12 13:49:14,638 INFO] Step 700/ 2000; acc:  44.65; ppl: 28.51; xent: 3.35; lr: 1.00000; 1250/1350 tok/s;    174 sec\n","[2022-03-12 13:49:19,151 INFO] Step 720/ 2000; acc:  40.70; ppl: 36.70; xent: 3.60; lr: 1.00000; 1378/1445 tok/s;    178 sec\n","[2022-03-12 13:49:24,115 INFO] Step 740/ 2000; acc:  40.69; ppl: 37.85; xent: 3.63; lr: 1.00000; 1407/1439 tok/s;    183 sec\n","[2022-03-12 13:49:29,246 INFO] Step 760/ 2000; acc:  40.00; ppl: 41.87; xent: 3.73; lr: 1.00000; 1335/1377 tok/s;    188 sec\n","[2022-03-12 13:49:33,859 INFO] Step 780/ 2000; acc:  44.71; ppl: 28.42; xent: 3.35; lr: 1.00000; 1299/1360 tok/s;    193 sec\n","[2022-03-12 13:49:38,878 INFO] Step 800/ 2000; acc:  41.29; ppl: 36.02; xent: 3.58; lr: 1.00000; 1350/1412 tok/s;    198 sec\n","[2022-03-12 13:49:44,075 INFO] Step 820/ 2000; acc:  41.98; ppl: 37.46; xent: 3.62; lr: 1.00000; 1247/1316 tok/s;    203 sec\n","[2022-03-12 13:49:48,377 INFO] Step 840/ 2000; acc:  45.82; ppl: 27.90; xent: 3.33; lr: 1.00000; 1392/1464 tok/s;    208 sec\n","[2022-03-12 13:49:53,011 INFO] Step 860/ 2000; acc:  44.47; ppl: 29.63; xent: 3.39; lr: 1.00000; 1334/1428 tok/s;    212 sec\n","[2022-03-12 13:49:58,471 INFO] Step 880/ 2000; acc:  41.37; ppl: 35.15; xent: 3.56; lr: 1.00000; 1277/1305 tok/s;    218 sec\n","[2022-03-12 13:50:02,821 INFO] Step 900/ 2000; acc:  45.93; ppl: 26.14; xent: 3.26; lr: 1.00000; 1307/1427 tok/s;    222 sec\n","[2022-03-12 13:50:07,638 INFO] Step 920/ 2000; acc:  43.97; ppl: 29.30; xent: 3.38; lr: 1.00000; 1356/1432 tok/s;    227 sec\n","[2022-03-12 13:50:13,244 INFO] Step 940/ 2000; acc:  40.92; ppl: 37.97; xent: 3.64; lr: 1.00000; 1337/1315 tok/s;    232 sec\n","[2022-03-12 13:50:17,536 INFO] Step 960/ 2000; acc:  47.33; ppl: 23.81; xent: 3.17; lr: 1.00000; 1260/1403 tok/s;    237 sec\n","[2022-03-12 13:50:22,326 INFO] Step 980/ 2000; acc:  44.90; ppl: 26.72; xent: 3.29; lr: 1.00000; 1329/1338 tok/s;    242 sec\n","[2022-03-12 13:50:27,924 INFO] Step 1000/ 2000; acc:  41.14; ppl: 33.27; xent: 3.50; lr: 1.00000; 1363/1359 tok/s;    247 sec\n","[2022-03-12 13:50:32,139 INFO] Step 1020/ 2000; acc:  49.24; ppl: 22.21; xent: 3.10; lr: 1.00000; 1301/1367 tok/s;    251 sec\n","[2022-03-12 13:50:36,809 INFO] Step 1040/ 2000; acc:  45.10; ppl: 25.97; xent: 3.26; lr: 1.00000; 1341/1379 tok/s;    256 sec\n","[2022-03-12 13:50:41,953 INFO] Step 1060/ 2000; acc:  44.12; ppl: 28.93; xent: 3.36; lr: 1.00000; 1365/1396 tok/s;    261 sec\n","[2022-03-12 13:50:47,178 INFO] Step 1080/ 2000; acc:  42.65; ppl: 32.55; xent: 3.48; lr: 1.00000; 1328/1342 tok/s;    266 sec\n","[2022-03-12 13:50:51,787 INFO] Step 1100/ 2000; acc:  46.67; ppl: 25.28; xent: 3.23; lr: 1.00000; 1315/1342 tok/s;    271 sec\n","[2022-03-12 13:50:56,901 INFO] Step 1120/ 2000; acc:  43.49; ppl: 28.51; xent: 3.35; lr: 1.00000; 1347/1404 tok/s;    276 sec\n","[2022-03-12 13:51:01,953 INFO] Step 1140/ 2000; acc:  44.73; ppl: 28.15; xent: 3.34; lr: 1.00000; 1285/1321 tok/s;    281 sec\n","[2022-03-12 13:51:06,489 INFO] Step 1160/ 2000; acc:  47.07; ppl: 22.47; xent: 3.11; lr: 1.00000; 1304/1465 tok/s;    286 sec\n","[2022-03-12 13:51:11,111 INFO] Step 1180/ 2000; acc:  47.17; ppl: 23.22; xent: 3.14; lr: 1.00000; 1329/1404 tok/s;    290 sec\n","[2022-03-12 13:51:16,718 INFO] Step 1200/ 2000; acc:  43.10; ppl: 31.93; xent: 3.46; lr: 1.00000; 1244/1315 tok/s;    296 sec\n","[2022-03-12 13:51:20,098 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-03-12 13:51:21,031 INFO] Step 1220/ 2000; acc:  49.98; ppl: 19.59; xent: 2.98; lr: 1.00000; 1316/1369 tok/s;    300 sec\n","[2022-03-12 13:51:25,926 INFO] Step 1240/ 2000; acc:  45.58; ppl: 25.73; xent: 3.25; lr: 1.00000; 1342/1364 tok/s;    305 sec\n","[2022-03-12 13:51:29,743 INFO] Validation perplexity: 13.3041\n","[2022-03-12 13:51:29,743 INFO] Validation accuracy: 55.6792\n","[2022-03-12 13:51:29,792 INFO] Saving checkpoint ./models/base/model_step_1250.pt\n","[2022-03-12 13:51:33,147 INFO] Step 1260/ 2000; acc:  42.81; ppl: 29.80; xent: 3.39; lr: 1.00000; 997/1039 tok/s;    312 sec\n","[2022-03-12 13:51:37,481 INFO] Step 1280/ 2000; acc:  50.01; ppl: 19.04; xent: 2.95; lr: 1.00000; 1244/1375 tok/s;    317 sec\n","[2022-03-12 13:51:42,366 INFO] Step 1300/ 2000; acc:  47.72; ppl: 21.78; xent: 3.08; lr: 1.00000; 1303/1338 tok/s;    322 sec\n","[2022-03-12 13:51:48,576 INFO] Step 1320/ 2000; acc:  43.55; ppl: 27.77; xent: 3.32; lr: 1.00000; 1238/1260 tok/s;    328 sec\n","[2022-03-12 13:51:52,623 INFO] Step 1340/ 2000; acc:  51.24; ppl: 17.70; xent: 2.87; lr: 1.00000; 1356/1429 tok/s;    332 sec\n","[2022-03-12 13:51:57,254 INFO] Step 1360/ 2000; acc:  48.03; ppl: 21.29; xent: 3.06; lr: 1.00000; 1341/1390 tok/s;    336 sec\n","[2022-03-12 13:52:02,272 INFO] Step 1380/ 2000; acc:  47.05; ppl: 24.11; xent: 3.18; lr: 1.00000; 1389/1404 tok/s;    341 sec\n","[2022-03-12 13:52:07,404 INFO] Step 1400/ 2000; acc:  46.00; ppl: 26.32; xent: 3.27; lr: 1.00000; 1332/1389 tok/s;    347 sec\n","[2022-03-12 13:52:11,882 INFO] Step 1420/ 2000; acc:  50.38; ppl: 18.55; xent: 2.92; lr: 1.00000; 1320/1397 tok/s;    351 sec\n","[2022-03-12 13:52:16,770 INFO] Step 1440/ 2000; acc:  47.08; ppl: 22.97; xent: 3.13; lr: 1.00000; 1361/1415 tok/s;    356 sec\n","[2022-03-12 13:52:21,843 INFO] Step 1460/ 2000; acc:  47.05; ppl: 23.38; xent: 3.15; lr: 1.00000; 1234/1393 tok/s;    361 sec\n","[2022-03-12 13:52:26,155 INFO] Step 1480/ 2000; acc:  51.57; ppl: 17.01; xent: 2.83; lr: 1.00000; 1384/1444 tok/s;    365 sec\n","[2022-03-12 13:52:30,723 INFO] Step 1500/ 2000; acc:  48.99; ppl: 19.11; xent: 2.95; lr: 1.00000; 1364/1467 tok/s;    370 sec\n","[2022-03-12 13:52:36,167 INFO] Step 1520/ 2000; acc:  46.81; ppl: 23.87; xent: 3.17; lr: 1.00000; 1296/1291 tok/s;    375 sec\n","[2022-03-12 13:52:40,559 INFO] Step 1540/ 2000; acc:  50.79; ppl: 17.53; xent: 2.86; lr: 1.00000; 1306/1436 tok/s;    380 sec\n","[2022-03-12 13:52:45,312 INFO] Step 1560/ 2000; acc:  48.89; ppl: 19.17; xent: 2.95; lr: 1.00000; 1391/1431 tok/s;    385 sec\n","[2022-03-12 13:52:50,594 INFO] Step 1580/ 2000; acc:  45.91; ppl: 24.96; xent: 3.22; lr: 1.00000; 1431/1387 tok/s;    390 sec\n","[2022-03-12 13:52:54,885 INFO] Step 1600/ 2000; acc:  51.28; ppl: 15.70; xent: 2.75; lr: 1.00000; 1277/1426 tok/s;    394 sec\n","[2022-03-12 13:52:59,549 INFO] Step 1620/ 2000; acc:  49.33; ppl: 18.64; xent: 2.93; lr: 1.00000; 1371/1392 tok/s;    399 sec\n","[2022-03-12 13:53:05,076 INFO] Step 1640/ 2000; acc:  45.12; ppl: 23.76; xent: 3.17; lr: 1.00000; 1401/1391 tok/s;    404 sec\n","[2022-03-12 13:53:09,117 INFO] Step 1660/ 2000; acc:  52.82; ppl: 15.50; xent: 2.74; lr: 1.00000; 1359/1420 tok/s;    408 sec\n","[2022-03-12 13:53:13,569 INFO] Step 1680/ 2000; acc:  49.30; ppl: 18.94; xent: 2.94; lr: 1.00000; 1396/1433 tok/s;    413 sec\n","[2022-03-12 13:53:18,664 INFO] Step 1700/ 2000; acc:  47.79; ppl: 20.21; xent: 3.01; lr: 1.00000; 1359/1409 tok/s;    418 sec\n","[2022-03-12 13:53:23,757 INFO] Step 1720/ 2000; acc:  47.60; ppl: 21.00; xent: 3.04; lr: 1.00000; 1333/1343 tok/s;    423 sec\n","[2022-03-12 13:53:28,324 INFO] Step 1740/ 2000; acc:  50.53; ppl: 18.38; xent: 2.91; lr: 1.00000; 1312/1353 tok/s;    428 sec\n","[2022-03-12 13:53:33,426 INFO] Step 1760/ 2000; acc:  48.49; ppl: 19.48; xent: 2.97; lr: 1.00000; 1348/1401 tok/s;    433 sec\n","[2022-03-12 13:53:38,610 INFO] Step 1780/ 2000; acc:  48.91; ppl: 19.73; xent: 2.98; lr: 1.00000; 1253/1323 tok/s;    438 sec\n","[2022-03-12 13:53:43,174 INFO] Step 1800/ 2000; acc:  52.03; ppl: 15.21; xent: 2.72; lr: 1.00000; 1295/1443 tok/s;    442 sec\n","[2022-03-12 13:53:47,700 INFO] Step 1820/ 2000; acc:  51.11; ppl: 15.70; xent: 2.75; lr: 1.00000; 1360/1417 tok/s;    447 sec\n","[2022-03-12 13:53:53,170 INFO] Step 1840/ 2000; acc:  47.51; ppl: 21.30; xent: 3.06; lr: 1.00000; 1260/1326 tok/s;    452 sec\n","[2022-03-12 13:53:56,524 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-03-12 13:53:57,435 INFO] Step 1860/ 2000; acc:  54.19; ppl: 13.67; xent: 2.62; lr: 1.00000; 1327/1403 tok/s;    457 sec\n","[2022-03-12 13:54:02,122 INFO] Validation perplexity: 10.3103\n","[2022-03-12 13:54:02,122 INFO] Validation accuracy: 58.9244\n","[2022-03-12 13:54:02,171 INFO] Saving checkpoint ./models/base/model_step_1875.pt\n","[2022-03-12 13:54:03,908 INFO] Step 1880/ 2000; acc:  49.42; ppl: 19.04; xent: 2.95; lr: 1.00000; 1021/1039 tok/s;    463 sec\n","[2022-03-12 13:54:09,694 INFO] Step 1900/ 2000; acc:  47.12; ppl: 21.66; xent: 3.08; lr: 1.00000; 1268/1294 tok/s;    469 sec\n","[2022-03-12 13:54:14,149 INFO] Step 1920/ 2000; acc:  54.06; ppl: 13.71; xent: 2.62; lr: 1.00000; 1212/1361 tok/s;    473 sec\n","[2022-03-12 13:54:18,980 INFO] Step 1940/ 2000; acc:  51.54; ppl: 15.53; xent: 2.74; lr: 1.00000; 1323/1366 tok/s;    478 sec\n","[2022-03-12 13:54:25,056 INFO] Step 1960/ 2000; acc:  46.48; ppl: 21.51; xent: 3.07; lr: 1.00000; 1276/1275 tok/s;    484 sec\n","[2022-03-12 13:54:29,114 INFO] Step 1980/ 2000; acc:  54.60; ppl: 13.16; xent: 2.58; lr: 1.00000; 1353/1473 tok/s;    488 sec\n","[2022-03-12 13:54:33,835 INFO] Step 2000/ 2000; acc:  51.71; ppl: 17.06; xent: 2.84; lr: 1.00000; 1322/1374 tok/s;    493 sec\n","[2022-03-12 13:54:33,885 INFO] Saving checkpoint ./models/base/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_train -config config-base.yaml"]},{"cell_type":"markdown","metadata":{"id":"AzfFoAb_F0FP"},"source":["Q6) **\"acc\"** : représente la precision (accuracy) de modele , c'est à dire le nombre de mots correctement traduits divisé par le nombre total des mots , il vaut mieux que cette valeur soit grande parceque cela signifie qu'on traduit correctement plus de mots .\n","\n","**\"ppl\"** : represente la perplexité  ~ le nombre de mots que le modele n'a pas pu traduire divisé par le nombre total des mots ) , il vaut mieux que cette valeur soit petite. **Mauvaise définition de perplexité.**\n","\n","Q7) train_steps : nombre d'étapes de training (apprentissage) . **C'est quoi un pas d'apprentissage? (vous n'avez que traduit le nom de la variable sans expliquer sa signification). train_step = nombre total d'étapes d'optimisation des poids du modèle, où chaque étape est constituée d'un passage en avant d'un lot (batch) de données d'apprentissage dans le modèle (forward pass) et d'un passage en arrière des gradients générés par la fonction de perte (backward pass), suivi par une mise à jours des poids du modèle.**\n","\n","valid_steps :  nombre de steps (pas) à réaliser avant de valider .\n","\n","enc_layers: nombre de couches de l'encodeur.\n","\n","dec_layers:  nombre de couches du décodeur.\n","\n","enc_rnn_size:  nombre de neurones dans chaque couche de l'encodeur.\n","\n","dec_rnn_size: Le nombre de neurones dans chaque couche du décodeur.\n","\n","Batch_size: Le nombre de batchs utilisés dans chaque étape de training. **le nombre d'échantillons qui seront propagés dans le réseau dans la même étape d'entrainement/évalutaion**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21012,"status":"ok","timestamp":1647093402056,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"mWepd4ChIm3H","outputId":"8cca04a5-d9c4-4413-a302-57c5d0c52571"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:56:21,813 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:56:25,143 INFO] PRED AVG SCORE: -1.3080, PRED PPL: 3.6989\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:56:26,777 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:56:30,426 INFO] PRED AVG SCORE: -1.1011, PRED PPL: 3.0075\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:56:32,027 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:56:35,555 INFO] PRED AVG SCORE: -0.9522, PRED PPL: 2.5914\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:56:37,164 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:56:40,863 INFO] PRED AVG SCORE: -1.0030, PRED PPL: 2.7263\n"]},{"data":{"text/plain":[]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_translate -model ./models/base/model_step_625.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_625.txt\n","onmt_translate -model ./models/base/model_step_1250.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_1250.txt\n","onmt_translate -model ./models/base/model_step_1875.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_1875.txt\n","onmt_translate -model ./models/base/model_step_2000.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_2000.txt"]},{"cell_type":"markdown","metadata":{"id":"OQj-kRiuIbA_"},"source":["**Q8** : le score BLEU a une valeur comprise entre 0 et 100 (percent),il vaut mieux que cette valeur soit grande car ce score represente la similarité entre la traduction automatique et la traduction manuelle ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":521,"status":"ok","timestamp":1647093411516,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"YLYIdOxuJnMe","outputId":"70db7b15-8edb-4bd8-8e2c-d0df6ab3cccd"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 8.78, 43.5/14.2/7.7/3.1 (BP=0.797, ratio=0.815, hyp_len=2912, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 12.93, 49.5/20.5/11.7/4.4 (BP=0.855, ratio=0.865, hyp_len=3091, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 17.85, 54.1/24.9/15.4/7.8 (BP=0.890, ratio=0.895, hyp_len=3200, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 17.70, 54.6/25.4/15.6/8.0 (BP=0.868, ratio=0.876, hyp_len=3130, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_625.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_1250.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_1875.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_2000.txt"]},{"cell_type":"markdown","metadata":{"id":"XCSWwczWJdlZ"},"source":["**Q9**: le modele a 2000/1875 etapes ont les plus grands scores BLEU (ce qui etait attendu)"]},{"cell_type":"markdown","metadata":{"id":"hddISeGpMjho"},"source":["**Q10** : avoir plus d'etapes donne au modele plus de temps pour 'tuner' ses parametres , càd avoir plus de precision. ainsi oui , augmenter le nombre de steps ameliore le score BLEU. **Cela n'est pas toujours vrai. Jusqu'à quelle étape le modèle s'amélieure? c'est quoi le \"early stopping\"?**"]},{"cell_type":"markdown","metadata":{"id":"diLfAzBqOAgJ"},"source":["**Q11** : on teste avec deux couches dans le code suivant \n","\n"]},{"cell_type":"markdown","metadata":{"id":"xdIzGaaVOtzu"},"source":["On refait la meme operation de la question 9 pour trouver les scores BLEU "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":607982,"status":"ok","timestamp":1647090857837,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"dooS6Q9UKCF4","outputId":"4e6cfe33-df61-4002-e34c-bd1c7ff742e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-03-12 13:04:10,500 WARNING] You have a CUDA device, should run with -gpu_ranks\n","[2022-03-12 13:04:10,502 INFO] Missing transforms field for train data, set to default: [].\n","[2022-03-12 13:04:10,503 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-03-12 13:04:10,503 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-03-12 13:04:10,503 INFO] Parsed 2 corpora from -data.\n","[2022-03-12 13:04:10,504 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-03-12 13:04:10,504 INFO] Loading vocab from text file...\n","[2022-03-12 13:04:10,504 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-03-12 13:04:10,525 INFO] Loaded src vocab has 9978 tokens.\n","[2022-03-12 13:04:10,529 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-03-12 13:04:10,544 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-03-12 13:04:10,547 INFO] Building fields with vocab in counters...\n","[2022-03-12 13:04:10,556 INFO]  * tgt vocab size: 8198.\n","[2022-03-12 13:04:10,568 INFO]  * src vocab size: 9980.\n","[2022-03-12 13:04:10,569 INFO]  * src vocab size = 9980\n","[2022-03-12 13:04:10,569 INFO]  * tgt vocab size = 8198\n","[2022-03-12 13:04:10,571 INFO] Building model...\n","[2022-03-12 13:04:10,761 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(9980, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 128, num_layers=2, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(756, 256)\n","        (1): LSTMCell(256, 256)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=256, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-03-12 13:04:10,762 INFO] encoder: 6030384\n","[2022-03-12 13:04:10,762 INFO] decoder: 7770558\n","[2022-03-12 13:04:10,762 INFO] * number of parameters: 13800942\n","[2022-03-12 13:04:10,764 INFO] Starting training on CPU, could be very slow\n","[2022-03-12 13:04:10,764 INFO] Start training loop and validate every 625 steps...\n","[2022-03-12 13:04:10,764 INFO] train's transforms: TransformPipe()\n","[2022-03-12 13:04:10,764 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-03-12 13:04:16,874 INFO] Step 20/ 2000; acc:   7.68; ppl: 2427.02; xent: 7.79; lr: 1.00000; 1067/1117 tok/s;      6 sec\n","[2022-03-12 13:04:22,494 INFO] Step 40/ 2000; acc:  10.92; ppl: 608.04; xent: 6.41; lr: 1.00000; 966/1035 tok/s;     12 sec\n","[2022-03-12 13:04:29,906 INFO] Step 60/ 2000; acc:   8.98; ppl: 524.01; xent: 6.26; lr: 1.00000; 994/1020 tok/s;     19 sec\n","[2022-03-12 13:04:36,102 INFO] Step 80/ 2000; acc:  15.11; ppl: 319.87; xent: 5.77; lr: 1.00000; 1080/1120 tok/s;     25 sec\n","[2022-03-12 13:04:41,563 INFO] Step 100/ 2000; acc:  16.57; ppl: 231.85; xent: 5.45; lr: 1.00000; 1061/1099 tok/s;     31 sec\n","[2022-03-12 13:04:48,671 INFO] Step 120/ 2000; acc:  16.15; ppl: 251.97; xent: 5.53; lr: 1.00000; 1063/1091 tok/s;     38 sec\n","[2022-03-12 13:04:54,419 INFO] Step 140/ 2000; acc:  20.61; ppl: 174.75; xent: 5.16; lr: 1.00000; 1052/1126 tok/s;     44 sec\n","[2022-03-12 13:05:00,142 INFO] Step 160/ 2000; acc:  20.69; ppl: 163.02; xent: 5.09; lr: 1.00000; 1150/1145 tok/s;     49 sec\n","[2022-03-12 13:05:05,660 INFO] Step 180/ 2000; acc:  24.38; ppl: 121.41; xent: 4.80; lr: 1.00000; 992/1100 tok/s;     55 sec\n","[2022-03-12 13:05:12,493 INFO] Step 200/ 2000; acc:  20.43; ppl: 171.61; xent: 5.15; lr: 1.00000; 1113/1126 tok/s;     62 sec\n","[2022-03-12 13:05:18,452 INFO] Step 220/ 2000; acc:  26.16; ppl: 113.71; xent: 4.73; lr: 1.00000; 1021/1079 tok/s;     68 sec\n","[2022-03-12 13:05:23,467 INFO] Step 240/ 2000; acc:  30.92; ppl: 81.15; xent: 4.40; lr: 1.00000; 1025/1155 tok/s;     73 sec\n","[2022-03-12 13:05:30,870 INFO] Step 260/ 2000; acc:  23.90; ppl: 139.56; xent: 4.94; lr: 1.00000; 1060/1078 tok/s;     80 sec\n","[2022-03-12 13:05:36,574 INFO] Step 280/ 2000; acc:  30.00; ppl: 89.62; xent: 4.50; lr: 1.00000; 1053/1099 tok/s;     86 sec\n","[2022-03-12 13:05:41,723 INFO] Step 300/ 2000; acc:  33.51; ppl: 69.57; xent: 4.24; lr: 1.00000; 991/1134 tok/s;     91 sec\n","[2022-03-12 13:05:48,641 INFO] Step 320/ 2000; acc:  28.19; ppl: 103.08; xent: 4.64; lr: 1.00000; 1079/1100 tok/s;     98 sec\n","[2022-03-12 13:05:54,784 INFO] Step 340/ 2000; acc:  29.81; ppl: 85.53; xent: 4.45; lr: 1.00000; 1081/1118 tok/s;    104 sec\n","[2022-03-12 13:05:59,875 INFO] Step 360/ 2000; acc:  35.84; ppl: 59.70; xent: 4.09; lr: 1.00000; 1089/1117 tok/s;    109 sec\n","[2022-03-12 13:06:06,719 INFO] Step 380/ 2000; acc:  29.41; ppl: 84.58; xent: 4.44; lr: 1.00000; 1093/1077 tok/s;    116 sec\n","[2022-03-12 13:06:13,116 INFO] Step 400/ 2000; acc:  31.53; ppl: 73.92; xent: 4.30; lr: 1.00000; 1056/1108 tok/s;    122 sec\n","[2022-03-12 13:06:18,422 INFO] Step 420/ 2000; acc:  34.45; ppl: 59.06; xent: 4.08; lr: 1.00000; 1090/1107 tok/s;    128 sec\n","[2022-03-12 13:06:25,298 INFO] Step 440/ 2000; acc:  30.63; ppl: 84.47; xent: 4.44; lr: 1.00000; 1111/1093 tok/s;    135 sec\n","[2022-03-12 13:06:31,050 INFO] Step 460/ 2000; acc:  35.21; ppl: 52.38; xent: 3.96; lr: 1.00000; 1046/1108 tok/s;    140 sec\n","[2022-03-12 13:06:37,068 INFO] Step 480/ 2000; acc:  33.65; ppl: 63.14; xent: 4.15; lr: 1.00000; 1087/1082 tok/s;    146 sec\n","[2022-03-12 13:06:42,397 INFO] Step 500/ 2000; acc:  37.21; ppl: 49.78; xent: 3.91; lr: 1.00000; 1019/1125 tok/s;    152 sec\n","[2022-03-12 13:06:49,653 INFO] Step 520/ 2000; acc:  31.01; ppl: 74.88; xent: 4.32; lr: 1.00000; 1050/1070 tok/s;    159 sec\n","[2022-03-12 13:06:55,464 INFO] Step 540/ 2000; acc:  36.40; ppl: 54.09; xent: 3.99; lr: 1.00000; 1057/1115 tok/s;    165 sec\n","[2022-03-12 13:07:00,613 INFO] Step 560/ 2000; acc:  40.29; ppl: 40.43; xent: 3.70; lr: 1.00000; 1006/1125 tok/s;    170 sec\n","[2022-03-12 13:07:06,412 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-03-12 13:07:07,916 INFO] Step 580/ 2000; acc:  30.67; ppl: 75.71; xent: 4.33; lr: 1.00000; 1089/1109 tok/s;    177 sec\n","[2022-03-12 13:07:13,576 INFO] Step 600/ 2000; acc:  38.63; ppl: 46.64; xent: 3.84; lr: 1.00000; 1063/1099 tok/s;    183 sec\n","[2022-03-12 13:07:18,598 INFO] Step 620/ 2000; acc:  40.02; ppl: 38.47; xent: 3.65; lr: 1.00000; 1013/1136 tok/s;    188 sec\n","[2022-03-12 13:07:19,844 INFO] valid's transforms: TransformPipe()\n","[2022-03-12 13:07:21,607 INFO] Validation perplexity: 26.9376\n","[2022-03-12 13:07:21,607 INFO] Validation accuracy: 44.599\n","[2022-03-12 13:07:21,656 INFO] Saving checkpoint ./models/couche_2/model_step_625.pt\n","[2022-03-12 13:07:27,647 INFO] Step 640/ 2000; acc:  32.65; ppl: 61.64; xent: 4.12; lr: 1.00000; 818/845 tok/s;    197 sec\n","[2022-03-12 13:07:33,695 INFO] Step 660/ 2000; acc:  36.96; ppl: 47.24; xent: 3.86; lr: 1.00000; 1070/1114 tok/s;    203 sec\n","[2022-03-12 13:07:38,860 INFO] Step 680/ 2000; acc:  41.81; ppl: 35.69; xent: 3.58; lr: 1.00000; 1050/1123 tok/s;    208 sec\n","[2022-03-12 13:07:46,188 INFO] Step 700/ 2000; acc:  33.97; ppl: 57.67; xent: 4.05; lr: 1.00000; 998/1008 tok/s;    215 sec\n","[2022-03-12 13:07:52,222 INFO] Step 720/ 2000; acc:  36.26; ppl: 49.31; xent: 3.90; lr: 1.00000; 1121/1184 tok/s;    221 sec\n","[2022-03-12 13:07:57,447 INFO] Step 740/ 2000; acc:  40.23; ppl: 41.33; xent: 3.72; lr: 1.00000; 1116/1150 tok/s;    227 sec\n","[2022-03-12 13:08:04,366 INFO] Step 760/ 2000; acc:  35.61; ppl: 55.09; xent: 4.01; lr: 1.00000; 1113/1127 tok/s;    234 sec\n","[2022-03-12 13:08:09,883 INFO] Step 780/ 2000; acc:  39.76; ppl: 38.88; xent: 3.66; lr: 1.00000; 1088/1153 tok/s;    239 sec\n","[2022-03-12 13:08:15,823 INFO] Step 800/ 2000; acc:  40.13; ppl: 40.31; xent: 3.70; lr: 1.00000; 1082/1086 tok/s;    245 sec\n","[2022-03-12 13:08:20,931 INFO] Step 820/ 2000; acc:  42.56; ppl: 32.98; xent: 3.50; lr: 1.00000; 1046/1187 tok/s;    250 sec\n","[2022-03-12 13:08:27,618 INFO] Step 840/ 2000; acc:  36.22; ppl: 51.11; xent: 3.93; lr: 1.00000; 1115/1163 tok/s;    257 sec\n","[2022-03-12 13:08:33,281 INFO] Step 860/ 2000; acc:  42.25; ppl: 34.46; xent: 3.54; lr: 1.00000; 1088/1130 tok/s;    263 sec\n","[2022-03-12 13:08:38,166 INFO] Step 880/ 2000; acc:  44.59; ppl: 29.14; xent: 3.37; lr: 1.00000; 1061/1187 tok/s;    267 sec\n","[2022-03-12 13:08:45,564 INFO] Step 900/ 2000; acc:  35.49; ppl: 50.85; xent: 3.93; lr: 1.00000; 1075/1092 tok/s;    275 sec\n","[2022-03-12 13:08:51,047 INFO] Step 920/ 2000; acc:  42.71; ppl: 32.83; xent: 3.49; lr: 1.00000; 1108/1148 tok/s;    280 sec\n","[2022-03-12 13:08:56,050 INFO] Step 940/ 2000; acc:  45.68; ppl: 27.41; xent: 3.31; lr: 1.00000; 1040/1107 tok/s;    285 sec\n","[2022-03-12 13:09:03,021 INFO] Step 960/ 2000; acc:  37.37; ppl: 46.47; xent: 3.84; lr: 1.00000; 1085/1133 tok/s;    292 sec\n","[2022-03-12 13:09:08,943 INFO] Step 980/ 2000; acc:  41.70; ppl: 32.82; xent: 3.49; lr: 1.00000; 1109/1138 tok/s;    298 sec\n","[2022-03-12 13:09:14,084 INFO] Step 1000/ 2000; acc:  44.90; ppl: 29.34; xent: 3.38; lr: 1.00000; 1068/1090 tok/s;    303 sec\n","[2022-03-12 13:09:20,920 INFO] Step 1020/ 2000; acc:  38.94; ppl: 40.31; xent: 3.70; lr: 1.00000; 1072/1061 tok/s;    310 sec\n","[2022-03-12 13:09:27,031 INFO] Step 1040/ 2000; acc:  39.97; ppl: 35.06; xent: 3.56; lr: 1.00000; 1111/1171 tok/s;    316 sec\n","[2022-03-12 13:09:32,130 INFO] Step 1060/ 2000; acc:  44.49; ppl: 29.25; xent: 3.38; lr: 1.00000; 1145/1173 tok/s;    321 sec\n","[2022-03-12 13:09:39,215 INFO] Step 1080/ 2000; acc:  38.52; ppl: 42.23; xent: 3.74; lr: 1.00000; 1102/1099 tok/s;    328 sec\n","[2022-03-12 13:09:44,687 INFO] Step 1100/ 2000; acc:  43.86; ppl: 29.10; xent: 3.37; lr: 1.00000; 1106/1161 tok/s;    334 sec\n","[2022-03-12 13:09:50,536 INFO] Step 1120/ 2000; acc:  43.41; ppl: 30.65; xent: 3.42; lr: 1.00000; 1114/1112 tok/s;    340 sec\n","[2022-03-12 13:09:55,771 INFO] Step 1140/ 2000; acc:  44.82; ppl: 28.58; xent: 3.35; lr: 1.00000; 1034/1153 tok/s;    345 sec\n","[2022-03-12 13:10:02,423 INFO] Step 1160/ 2000; acc:  38.94; ppl: 41.35; xent: 3.72; lr: 1.00000; 1127/1146 tok/s;    352 sec\n","[2022-03-12 13:10:08,063 INFO] Step 1180/ 2000; acc:  43.95; ppl: 27.83; xent: 3.33; lr: 1.00000; 1064/1188 tok/s;    357 sec\n","[2022-03-12 13:10:13,094 INFO] Step 1200/ 2000; acc:  47.97; ppl: 22.28; xent: 3.10; lr: 1.00000; 1024/1116 tok/s;    362 sec\n","[2022-03-12 13:10:18,986 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-03-12 13:10:20,505 INFO] Step 1220/ 2000; acc:  38.97; ppl: 41.05; xent: 3.71; lr: 1.00000; 1073/1089 tok/s;    370 sec\n","[2022-03-12 13:10:26,156 INFO] Step 1240/ 2000; acc:  44.83; ppl: 29.00; xent: 3.37; lr: 1.00000; 1078/1089 tok/s;    375 sec\n","[2022-03-12 13:10:30,353 INFO] Validation perplexity: 16.7476\n","[2022-03-12 13:10:30,353 INFO] Validation accuracy: 52.1094\n","[2022-03-12 13:10:30,402 INFO] Saving checkpoint ./models/couche_2/model_step_1250.pt\n","[2022-03-12 13:10:33,074 INFO] Step 1260/ 2000; acc:  46.35; ppl: 23.81; xent: 3.17; lr: 1.00000; 738/827 tok/s;    382 sec\n","[2022-03-12 13:10:40,269 INFO] Step 1280/ 2000; acc:  39.54; ppl: 37.78; xent: 3.63; lr: 1.00000; 1029/1070 tok/s;    390 sec\n","[2022-03-12 13:10:46,162 INFO] Step 1300/ 2000; acc:  43.74; ppl: 26.79; xent: 3.29; lr: 1.00000; 1111/1143 tok/s;    395 sec\n","[2022-03-12 13:10:51,314 INFO] Step 1320/ 2000; acc:  47.87; ppl: 21.94; xent: 3.09; lr: 1.00000; 1064/1139 tok/s;    401 sec\n","[2022-03-12 13:10:58,614 INFO] Step 1340/ 2000; acc:  40.80; ppl: 35.39; xent: 3.57; lr: 1.00000; 1014/1016 tok/s;    408 sec\n","[2022-03-12 13:11:04,837 INFO] Step 1360/ 2000; acc:  41.97; ppl: 31.54; xent: 3.45; lr: 1.00000; 1088/1142 tok/s;    414 sec\n","[2022-03-12 13:11:10,102 INFO] Step 1380/ 2000; acc:  47.25; ppl: 24.42; xent: 3.20; lr: 1.00000; 1097/1142 tok/s;    419 sec\n","[2022-03-12 13:11:17,022 INFO] Step 1400/ 2000; acc:  40.80; ppl: 34.62; xent: 3.54; lr: 1.00000; 1111/1126 tok/s;    426 sec\n","[2022-03-12 13:11:22,424 INFO] Step 1420/ 2000; acc:  46.32; ppl: 24.35; xent: 3.19; lr: 1.00000; 1097/1139 tok/s;    432 sec\n","[2022-03-12 13:11:28,494 INFO] Step 1440/ 2000; acc:  44.70; ppl: 25.18; xent: 3.23; lr: 1.00000; 1027/1129 tok/s;    438 sec\n","[2022-03-12 13:11:33,264 INFO] Step 1460/ 2000; acc:  49.88; ppl: 20.18; xent: 3.00; lr: 1.00000; 1110/1208 tok/s;    443 sec\n","[2022-03-12 13:11:40,103 INFO] Step 1480/ 2000; acc:  41.10; ppl: 33.39; xent: 3.51; lr: 1.00000; 1075/1131 tok/s;    449 sec\n","[2022-03-12 13:11:45,599 INFO] Step 1500/ 2000; acc:  46.78; ppl: 23.19; xent: 3.14; lr: 1.00000; 1137/1142 tok/s;    455 sec\n","[2022-03-12 13:11:50,637 INFO] Step 1520/ 2000; acc:  48.75; ppl: 20.88; xent: 3.04; lr: 1.00000; 1039/1155 tok/s;    460 sec\n","[2022-03-12 13:11:58,135 INFO] Step 1540/ 2000; acc:  40.83; ppl: 32.06; xent: 3.47; lr: 1.00000; 1065/1108 tok/s;    467 sec\n","[2022-03-12 13:12:03,747 INFO] Step 1560/ 2000; acc:  46.98; ppl: 22.10; xent: 3.10; lr: 1.00000; 1095/1128 tok/s;    473 sec\n","[2022-03-12 13:12:08,685 INFO] Step 1580/ 2000; acc:  50.52; ppl: 18.13; xent: 2.90; lr: 1.00000; 1069/1140 tok/s;    478 sec\n","[2022-03-12 13:12:15,424 INFO] Step 1600/ 2000; acc:  41.19; ppl: 31.42; xent: 3.45; lr: 1.00000; 1135/1143 tok/s;    485 sec\n","[2022-03-12 13:12:21,340 INFO] Step 1620/ 2000; acc:  45.77; ppl: 23.03; xent: 3.14; lr: 1.00000; 1115/1149 tok/s;    491 sec\n","[2022-03-12 13:12:26,460 INFO] Step 1640/ 2000; acc:  49.07; ppl: 19.57; xent: 2.97; lr: 1.00000; 1072/1108 tok/s;    496 sec\n","[2022-03-12 13:12:33,108 INFO] Step 1660/ 2000; acc:  42.45; ppl: 31.02; xent: 3.43; lr: 1.00000; 1119/1102 tok/s;    502 sec\n","[2022-03-12 13:12:39,192 INFO] Step 1680/ 2000; acc:  45.34; ppl: 22.97; xent: 3.13; lr: 1.00000; 1106/1167 tok/s;    508 sec\n","[2022-03-12 13:12:44,436 INFO] Step 1700/ 2000; acc:  48.71; ppl: 20.13; xent: 3.00; lr: 1.00000; 1100/1113 tok/s;    514 sec\n","[2022-03-12 13:12:51,491 INFO] Step 1720/ 2000; acc:  41.98; ppl: 30.73; xent: 3.43; lr: 1.00000; 1084/1083 tok/s;    521 sec\n","[2022-03-12 13:12:57,009 INFO] Step 1740/ 2000; acc:  47.57; ppl: 20.47; xent: 3.02; lr: 1.00000; 1088/1157 tok/s;    526 sec\n","[2022-03-12 13:13:02,823 INFO] Step 1760/ 2000; acc:  47.02; ppl: 22.82; xent: 3.13; lr: 1.00000; 1116/1128 tok/s;    532 sec\n","[2022-03-12 13:13:07,988 INFO] Step 1780/ 2000; acc:  49.57; ppl: 18.78; xent: 2.93; lr: 1.00000; 1040/1181 tok/s;    537 sec\n","[2022-03-12 13:13:14,903 INFO] Step 1800/ 2000; acc:  42.35; ppl: 28.02; xent: 3.33; lr: 1.00000; 1088/1099 tok/s;    544 sec\n","[2022-03-12 13:13:20,444 INFO] Step 1820/ 2000; acc:  48.06; ppl: 19.60; xent: 2.98; lr: 1.00000; 1086/1189 tok/s;    550 sec\n","[2022-03-12 13:13:25,428 INFO] Step 1840/ 2000; acc:  50.10; ppl: 16.98; xent: 2.83; lr: 1.00000; 1030/1147 tok/s;    555 sec\n","[2022-03-12 13:13:31,025 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-03-12 13:13:32,622 INFO] Step 1860/ 2000; acc:  42.67; ppl: 29.02; xent: 3.37; lr: 1.00000; 1097/1109 tok/s;    562 sec\n","[2022-03-12 13:13:38,151 INFO] Validation perplexity: 12.7891\n","[2022-03-12 13:13:38,151 INFO] Validation accuracy: 55.1229\n","[2022-03-12 13:13:38,208 INFO] Saving checkpoint ./models/couche_2/model_step_1875.pt\n","[2022-03-12 13:13:39,889 INFO] Step 1880/ 2000; acc:  48.41; ppl: 20.86; xent: 3.04; lr: 1.00000; 841/860 tok/s;    569 sec\n","[2022-03-12 13:13:45,085 INFO] Step 1900/ 2000; acc:  50.04; ppl: 17.07; xent: 2.84; lr: 1.00000; 990/1092 tok/s;    574 sec\n","[2022-03-12 13:13:52,188 INFO] Step 1920/ 2000; acc:  43.24; ppl: 27.10; xent: 3.30; lr: 1.00000; 1059/1099 tok/s;    581 sec\n","[2022-03-12 13:13:57,974 INFO] Step 1940/ 2000; acc:  47.15; ppl: 20.87; xent: 3.04; lr: 1.00000; 1143/1157 tok/s;    587 sec\n","[2022-03-12 13:14:03,041 INFO] Step 1960/ 2000; acc:  51.53; ppl: 16.74; xent: 2.82; lr: 1.00000; 1084/1176 tok/s;    592 sec\n","[2022-03-12 13:14:10,266 INFO] Step 1980/ 2000; acc:  42.99; ppl: 26.53; xent: 3.28; lr: 1.00000; 1027/1044 tok/s;    600 sec\n","[2022-03-12 13:14:16,388 INFO] Step 2000/ 2000; acc:  46.60; ppl: 21.13; xent: 3.05; lr: 1.00000; 1111/1161 tok/s;    606 sec\n","[2022-03-12 13:14:16,440 INFO] Saving checkpoint ./models/couche_2/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_train -config config-couche-2.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22644,"status":"ok","timestamp":1647090989178,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"MR-F0yLWPA1u","outputId":"e503faa2-8c01-4685-abff-727c2437da55"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-03-12 13:16:07,308 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:16:10,966 INFO] PRED AVG SCORE: -1.4642, PRED PPL: 4.3242\n","[2022-03-12 13:16:12,616 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:16:16,614 INFO] PRED AVG SCORE: -1.2955, PRED PPL: 3.6527\n","[2022-03-12 13:16:18,223 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:16:22,306 INFO] PRED AVG SCORE: -1.0875, PRED PPL: 2.9669\n","[2022-03-12 13:16:23,894 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:16:27,978 INFO] PRED AVG SCORE: -1.1095, PRED PPL: 3.0329\n"]},{"data":{"text/plain":[]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_translate -model ./models/couche_2/model_step_625.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_couche_2_625.txt\n","onmt_translate -model ./models/couche_2/model_step_1250.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_couche_2_1250.txt\n","onmt_translate -model ./models/couche_2/model_step_1875.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_couche_2_1875.txt\n","onmt_translate -model ./models/couche_2/model_step_2000.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_couche_2_2000.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1647091006547,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"teQvd_rFPO52","outputId":"a1d26ff5-c6df-41c0-a94d-c81c7ce73a45"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 6.59, 41.3/12.1/6.7/1.7 (BP=0.760, ratio=0.784, hyp_len=2803, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 11.24, 46.2/17.7/10.1/3.6 (BP=0.856, ratio=0.865, hyp_len=3092, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 13.18, 48.0/19.7/11.5/4.6 (BP=0.880, ratio=0.887, hyp_len=3170, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 14.10, 49.8/20.8/12.0/4.8 (BP=0.902, ratio=0.907, hyp_len=3240, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_couche_2_625.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_couche_2_1250.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_couche_2_1875.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_couche_2_2000.txt"]},{"cell_type":"markdown","metadata":{"id":"a6UjHXUcRnMR"},"source":["les scores bleu obtenus sont faibles relativement à ceux trouvés avant , le changement n'ameliore pas la performance .\n","\n","**Il aurait fallu utiliser l'option early_stopping pour bien repondre à cette question (ainsi que les suivantes). En effet, si l'on utilise pas cette option, on risque de comparer des modèles qui sont sous-entrainé (c'est votre cas) ou sur-entrainés (overfitted),  sans donc pouvoir en tirer des conclusions. Vous aurez pu aussi essayer d'autres combinaisons de couches (e.g. 1+3,2+1,3+3...) pour pouvoir tirer des conclusions plus solides.**"]},{"cell_type":"markdown","metadata":{"id":"olqJ-nsyPZSP"},"source":["Q12 : on teste avec nbr d'unites = 512 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":976523,"status":"ok","timestamp":1647092062534,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"BOx4PspAPX3f","outputId":"42df260b-0c62-4554-b30d-64cc9f6b0f12"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2022-03-12 13:18:06,654 WARNING] You have a CUDA device, should run with -gpu_ranks\n","[2022-03-12 13:18:06,656 INFO] Missing transforms field for train data, set to default: [].\n","[2022-03-12 13:18:06,656 WARNING] Corpus train's weight should be given. We default it to 1 for you.\n","[2022-03-12 13:18:06,657 INFO] Missing transforms field for valid data, set to default: [].\n","[2022-03-12 13:18:06,657 INFO] Parsed 2 corpora from -data.\n","[2022-03-12 13:18:06,658 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n","[2022-03-12 13:18:06,658 INFO] Loading vocab from text file...\n","[2022-03-12 13:18:06,658 INFO] Loading src vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.src\n","[2022-03-12 13:18:06,681 INFO] Loaded src vocab has 9978 tokens.\n","[2022-03-12 13:18:06,686 INFO] Loading tgt vocabulary from ./BTEC-en-fr/BTEC-en-fr/vocab.tgt\n","[2022-03-12 13:18:06,703 INFO] Loaded tgt vocab has 8194 tokens.\n","[2022-03-12 13:18:06,707 INFO] Building fields with vocab in counters...\n","[2022-03-12 13:18:06,716 INFO]  * tgt vocab size: 8198.\n","[2022-03-12 13:18:06,728 INFO]  * src vocab size: 9980.\n","[2022-03-12 13:18:06,728 INFO]  * src vocab size = 9980\n","[2022-03-12 13:18:06,728 INFO]  * tgt vocab size = 8198\n","[2022-03-12 13:18:06,731 INFO] Building model...\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:18:06,990 INFO] NMTModel(\n","  (encoder): RNNEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(9980, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (rnn): LSTM(500, 256, dropout=0.3, bidirectional=True)\n","  )\n","  (decoder): InputFeedRNNDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(8198, 500, padding_idx=1)\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","    (rnn): StackedLSTM(\n","      (dropout): Dropout(p=0.3, inplace=False)\n","      (layers): ModuleList(\n","        (0): LSTMCell(1012, 512)\n","      )\n","    )\n","  )\n","  (generator): Sequential(\n","    (0): Linear(in_features=512, out_features=8198, bias=True)\n","    (1): Cast()\n","    (2): LogSoftmax(dim=-1)\n","  )\n",")\n","[2022-03-12 13:18:06,991 INFO] encoder: 6542384\n","[2022-03-12 13:18:06,991 INFO] decoder: 11429822\n","[2022-03-12 13:18:06,991 INFO] * number of parameters: 17972206\n","[2022-03-12 13:18:06,993 INFO] Starting training on CPU, could be very slow\n","[2022-03-12 13:18:06,995 INFO] Start training loop and validate every 625 steps...\n","[2022-03-12 13:18:06,995 INFO] train's transforms: TransformPipe()\n","[2022-03-12 13:18:06,995 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 1\n","[2022-03-12 13:18:16,725 INFO] Step 20/ 2000; acc:   8.70; ppl: 5220.63; xent: 8.56; lr: 1.00000; 623/663 tok/s;     10 sec\n","[2022-03-12 13:18:25,772 INFO] Step 40/ 2000; acc:  15.28; ppl: 487.83; xent: 6.19; lr: 1.00000; 654/695 tok/s;     19 sec\n","[2022-03-12 13:18:38,071 INFO] Step 60/ 2000; acc:  15.40; ppl: 376.75; xent: 5.93; lr: 1.00000; 611/616 tok/s;     31 sec\n","[2022-03-12 13:18:47,189 INFO] Step 80/ 2000; acc:  19.44; ppl: 223.84; xent: 5.41; lr: 1.00000; 680/713 tok/s;     40 sec\n","[2022-03-12 13:18:56,651 INFO] Step 100/ 2000; acc:  22.54; ppl: 174.46; xent: 5.16; lr: 1.00000; 627/673 tok/s;     50 sec\n","[2022-03-12 13:19:06,814 INFO] Step 120/ 2000; acc:  21.95; ppl: 177.35; xent: 5.18; lr: 1.00000; 666/683 tok/s;     60 sec\n","[2022-03-12 13:19:16,623 INFO] Step 140/ 2000; acc:  25.60; ppl: 141.06; xent: 4.95; lr: 1.00000; 696/685 tok/s;     70 sec\n","[2022-03-12 13:19:26,006 INFO] Step 160/ 2000; acc:  26.90; ppl: 113.44; xent: 4.73; lr: 1.00000; 644/714 tok/s;     79 sec\n","[2022-03-12 13:19:35,676 INFO] Step 180/ 2000; acc:  28.94; ppl: 105.30; xent: 4.66; lr: 1.00000; 683/705 tok/s;     89 sec\n","[2022-03-12 13:19:45,779 INFO] Step 200/ 2000; acc:  29.48; ppl: 107.42; xent: 4.68; lr: 1.00000; 646/682 tok/s;     99 sec\n","[2022-03-12 13:19:55,547 INFO] Step 220/ 2000; acc:  30.79; ppl: 90.89; xent: 4.51; lr: 1.00000; 680/699 tok/s;    109 sec\n","[2022-03-12 13:20:04,666 INFO] Step 240/ 2000; acc:  35.33; ppl: 66.91; xent: 4.20; lr: 1.00000; 660/715 tok/s;    118 sec\n","[2022-03-12 13:20:14,878 INFO] Step 260/ 2000; acc:  32.19; ppl: 77.18; xent: 4.35; lr: 1.00000; 631/669 tok/s;    128 sec\n","[2022-03-12 13:20:24,535 INFO] Step 280/ 2000; acc:  33.87; ppl: 72.00; xent: 4.28; lr: 1.00000; 659/692 tok/s;    138 sec\n","[2022-03-12 13:20:32,972 INFO] Step 300/ 2000; acc:  37.79; ppl: 55.88; xent: 4.02; lr: 1.00000; 689/720 tok/s;    146 sec\n","[2022-03-12 13:20:44,152 INFO] Step 320/ 2000; acc:  32.59; ppl: 74.34; xent: 4.31; lr: 1.00000; 628/663 tok/s;    157 sec\n","[2022-03-12 13:20:53,095 INFO] Step 340/ 2000; acc:  36.02; ppl: 54.17; xent: 3.99; lr: 1.00000; 688/700 tok/s;    166 sec\n","[2022-03-12 13:21:02,091 INFO] Step 360/ 2000; acc:  37.34; ppl: 51.22; xent: 3.94; lr: 1.00000; 666/714 tok/s;    175 sec\n","[2022-03-12 13:21:13,260 INFO] Step 380/ 2000; acc:  32.76; ppl: 73.75; xent: 4.30; lr: 1.00000; 687/665 tok/s;    186 sec\n","[2022-03-12 13:21:22,416 INFO] Step 400/ 2000; acc:  37.91; ppl: 53.81; xent: 3.99; lr: 1.00000; 687/686 tok/s;    195 sec\n","[2022-03-12 13:21:31,466 INFO] Step 420/ 2000; acc:  37.55; ppl: 46.75; xent: 3.84; lr: 1.00000; 651/702 tok/s;    204 sec\n","[2022-03-12 13:21:41,367 INFO] Step 440/ 2000; acc:  37.01; ppl: 49.45; xent: 3.90; lr: 1.00000; 692/696 tok/s;    214 sec\n","[2022-03-12 13:21:51,671 INFO] Step 460/ 2000; acc:  36.58; ppl: 56.08; xent: 4.03; lr: 1.00000; 660/644 tok/s;    225 sec\n","[2022-03-12 13:22:01,051 INFO] Step 480/ 2000; acc:  38.49; ppl: 46.62; xent: 3.84; lr: 1.00000; 646/707 tok/s;    234 sec\n","[2022-03-12 13:22:10,808 INFO] Step 500/ 2000; acc:  38.82; ppl: 45.28; xent: 3.81; lr: 1.00000; 674/688 tok/s;    244 sec\n","[2022-03-12 13:22:21,166 INFO] Step 520/ 2000; acc:  37.69; ppl: 47.95; xent: 3.87; lr: 1.00000; 621/661 tok/s;    254 sec\n","[2022-03-12 13:22:30,820 INFO] Step 540/ 2000; acc:  39.25; ppl: 43.64; xent: 3.78; lr: 1.00000; 699/714 tok/s;    264 sec\n","[2022-03-12 13:22:40,267 INFO] Step 560/ 2000; acc:  41.94; ppl: 39.50; xent: 3.68; lr: 1.00000; 649/695 tok/s;    273 sec\n","[2022-03-12 13:22:48,907 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 2\n","[2022-03-12 13:22:50,520 INFO] Step 580/ 2000; acc:  40.54; ppl: 41.79; xent: 3.73; lr: 1.00000; 634/682 tok/s;    284 sec\n","[2022-03-12 13:23:00,089 INFO] Step 600/ 2000; acc:  40.13; ppl: 41.52; xent: 3.73; lr: 1.00000; 663/709 tok/s;    293 sec\n","[2022-03-12 13:23:08,744 INFO] Step 620/ 2000; acc:  42.89; ppl: 35.57; xent: 3.57; lr: 1.00000; 674/680 tok/s;    302 sec\n","[2022-03-12 13:23:11,436 INFO] valid's transforms: TransformPipe()\n","[2022-03-12 13:23:14,285 INFO] Validation perplexity: 20.5617\n","[2022-03-12 13:23:14,285 INFO] Validation accuracy: 49.7914\n","[2022-03-12 13:23:14,336 INFO] Saving checkpoint ./models/unite_512/model_step_625.pt\n","[2022-03-12 13:23:23,349 INFO] Step 640/ 2000; acc:  38.49; ppl: 44.27; xent: 3.79; lr: 1.00000; 475/507 tok/s;    316 sec\n","[2022-03-12 13:23:33,098 INFO] Step 660/ 2000; acc:  42.55; ppl: 34.15; xent: 3.53; lr: 1.00000; 620/657 tok/s;    326 sec\n","[2022-03-12 13:23:41,875 INFO] Step 680/ 2000; acc:  43.89; ppl: 30.64; xent: 3.42; lr: 1.00000; 673/708 tok/s;    335 sec\n","[2022-03-12 13:23:53,759 INFO] Step 700/ 2000; acc:  37.69; ppl: 44.36; xent: 3.79; lr: 1.00000; 625/629 tok/s;    347 sec\n","[2022-03-12 13:24:02,862 INFO] Step 720/ 2000; acc:  41.98; ppl: 35.02; xent: 3.56; lr: 1.00000; 685/733 tok/s;    356 sec\n","[2022-03-12 13:24:12,234 INFO] Step 740/ 2000; acc:  43.79; ppl: 29.39; xent: 3.38; lr: 1.00000; 637/672 tok/s;    365 sec\n","[2022-03-12 13:24:22,000 INFO] Step 760/ 2000; acc:  42.71; ppl: 34.86; xent: 3.55; lr: 1.00000; 706/712 tok/s;    375 sec\n","[2022-03-12 13:24:32,311 INFO] Step 780/ 2000; acc:  41.10; ppl: 38.25; xent: 3.64; lr: 1.00000; 657/669 tok/s;    385 sec\n","[2022-03-12 13:24:41,216 INFO] Step 800/ 2000; acc:  44.52; ppl: 28.52; xent: 3.35; lr: 1.00000; 671/725 tok/s;    394 sec\n","[2022-03-12 13:24:50,822 INFO] Step 820/ 2000; acc:  44.68; ppl: 28.38; xent: 3.35; lr: 1.00000; 670/706 tok/s;    404 sec\n","[2022-03-12 13:25:00,397 INFO] Step 840/ 2000; acc:  42.50; ppl: 36.28; xent: 3.59; lr: 1.00000; 669/718 tok/s;    413 sec\n","[2022-03-12 13:25:09,872 INFO] Step 860/ 2000; acc:  43.82; ppl: 28.91; xent: 3.36; lr: 1.00000; 706/733 tok/s;    423 sec\n","[2022-03-12 13:25:19,217 INFO] Step 880/ 2000; acc:  46.65; ppl: 25.02; xent: 3.22; lr: 1.00000; 653/684 tok/s;    432 sec\n","[2022-03-12 13:25:29,137 INFO] Step 900/ 2000; acc:  44.41; ppl: 30.06; xent: 3.40; lr: 1.00000; 659/686 tok/s;    442 sec\n","[2022-03-12 13:25:38,446 INFO] Step 920/ 2000; acc:  44.56; ppl: 27.23; xent: 3.30; lr: 1.00000; 689/723 tok/s;    451 sec\n","[2022-03-12 13:25:47,168 INFO] Step 940/ 2000; acc:  46.21; ppl: 25.65; xent: 3.24; lr: 1.00000; 673/715 tok/s;    460 sec\n","[2022-03-12 13:25:58,044 INFO] Step 960/ 2000; acc:  43.06; ppl: 32.39; xent: 3.48; lr: 1.00000; 661/681 tok/s;    471 sec\n","[2022-03-12 13:26:07,050 INFO] Step 980/ 2000; acc:  47.41; ppl: 23.35; xent: 3.15; lr: 1.00000; 678/682 tok/s;    480 sec\n","[2022-03-12 13:26:15,649 INFO] Step 1000/ 2000; acc:  46.74; ppl: 23.23; xent: 3.15; lr: 1.00000; 692/731 tok/s;    489 sec\n","[2022-03-12 13:26:26,730 INFO] Step 1020/ 2000; acc:  41.99; ppl: 32.66; xent: 3.49; lr: 1.00000; 676/657 tok/s;    500 sec\n","[2022-03-12 13:26:35,862 INFO] Step 1040/ 2000; acc:  46.20; ppl: 23.90; xent: 3.17; lr: 1.00000; 689/728 tok/s;    509 sec\n","[2022-03-12 13:26:44,671 INFO] Step 1060/ 2000; acc:  48.41; ppl: 21.39; xent: 3.06; lr: 1.00000; 681/703 tok/s;    518 sec\n","[2022-03-12 13:26:54,200 INFO] Step 1080/ 2000; acc:  44.84; ppl: 24.70; xent: 3.21; lr: 1.00000; 729/734 tok/s;    527 sec\n","[2022-03-12 13:27:04,811 INFO] Step 1100/ 2000; acc:  43.40; ppl: 31.33; xent: 3.44; lr: 1.00000; 646/653 tok/s;    538 sec\n","[2022-03-12 13:27:13,847 INFO] Step 1120/ 2000; acc:  47.68; ppl: 21.35; xent: 3.06; lr: 1.00000; 669/729 tok/s;    547 sec\n","[2022-03-12 13:27:23,156 INFO] Step 1140/ 2000; acc:  45.73; ppl: 25.19; xent: 3.23; lr: 1.00000; 705/716 tok/s;    556 sec\n","[2022-03-12 13:27:32,948 INFO] Step 1160/ 2000; acc:  46.42; ppl: 25.13; xent: 3.22; lr: 1.00000; 648/691 tok/s;    566 sec\n","[2022-03-12 13:27:42,604 INFO] Step 1180/ 2000; acc:  47.06; ppl: 23.14; xent: 3.14; lr: 1.00000; 695/694 tok/s;    576 sec\n","[2022-03-12 13:27:51,663 INFO] Step 1200/ 2000; acc:  48.42; ppl: 20.93; xent: 3.04; lr: 1.00000; 668/714 tok/s;    585 sec\n","[2022-03-12 13:28:00,233 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 3\n","[2022-03-12 13:28:02,091 INFO] Step 1220/ 2000; acc:  47.46; ppl: 22.06; xent: 3.09; lr: 1.00000; 618/693 tok/s;    595 sec\n","[2022-03-12 13:28:11,724 INFO] Step 1240/ 2000; acc:  47.43; ppl: 22.00; xent: 3.09; lr: 1.00000; 664/698 tok/s;    605 sec\n","[2022-03-12 13:28:18,487 INFO] Validation perplexity: 12.7373\n","[2022-03-12 13:28:18,487 INFO] Validation accuracy: 56.0501\n","[2022-03-12 13:28:18,536 INFO] Saving checkpoint ./models/unite_512/model_step_1250.pt\n","[2022-03-12 13:28:23,262 INFO] Step 1260/ 2000; acc:  49.63; ppl: 20.32; xent: 3.01; lr: 1.00000; 505/538 tok/s;    616 sec\n","[2022-03-12 13:28:34,171 INFO] Step 1280/ 2000; acc:  45.02; ppl: 25.82; xent: 3.25; lr: 1.00000; 641/658 tok/s;    627 sec\n","[2022-03-12 13:28:43,342 INFO] Step 1300/ 2000; acc:  49.54; ppl: 19.46; xent: 2.97; lr: 1.00000; 669/683 tok/s;    636 sec\n","[2022-03-12 13:28:52,249 INFO] Step 1320/ 2000; acc:  49.95; ppl: 18.63; xent: 2.92; lr: 1.00000; 670/715 tok/s;    645 sec\n","[2022-03-12 13:29:04,110 INFO] Step 1340/ 2000; acc:  45.29; ppl: 24.07; xent: 3.18; lr: 1.00000; 633/633 tok/s;    657 sec\n","[2022-03-12 13:29:13,150 INFO] Step 1360/ 2000; acc:  50.88; ppl: 17.74; xent: 2.88; lr: 1.00000; 690/723 tok/s;    666 sec\n","[2022-03-12 13:29:22,301 INFO] Step 1380/ 2000; acc:  50.38; ppl: 17.61; xent: 2.87; lr: 1.00000; 645/698 tok/s;    675 sec\n","[2022-03-12 13:29:32,333 INFO] Step 1400/ 2000; acc:  47.87; ppl: 21.96; xent: 3.09; lr: 1.00000; 682/703 tok/s;    685 sec\n","[2022-03-12 13:29:42,720 INFO] Step 1420/ 2000; acc:  47.79; ppl: 23.31; xent: 3.15; lr: 1.00000; 647/672 tok/s;    696 sec\n","[2022-03-12 13:29:51,553 INFO] Step 1440/ 2000; acc:  50.97; ppl: 17.70; xent: 2.87; lr: 1.00000; 669/705 tok/s;    705 sec\n","[2022-03-12 13:30:00,824 INFO] Step 1460/ 2000; acc:  51.97; ppl: 16.34; xent: 2.79; lr: 1.00000; 678/730 tok/s;    714 sec\n","[2022-03-12 13:30:10,429 INFO] Step 1480/ 2000; acc:  48.29; ppl: 20.60; xent: 3.03; lr: 1.00000; 659/701 tok/s;    723 sec\n","[2022-03-12 13:30:19,679 INFO] Step 1500/ 2000; acc:  50.58; ppl: 17.15; xent: 2.84; lr: 1.00000; 730/753 tok/s;    733 sec\n","[2022-03-12 13:30:28,908 INFO] Step 1520/ 2000; acc:  50.66; ppl: 17.03; xent: 2.84; lr: 1.00000; 669/692 tok/s;    742 sec\n","[2022-03-12 13:30:39,089 INFO] Step 1540/ 2000; acc:  49.40; ppl: 18.36; xent: 2.91; lr: 1.00000; 643/694 tok/s;    752 sec\n","[2022-03-12 13:30:48,401 INFO] Step 1560/ 2000; acc:  50.88; ppl: 16.36; xent: 2.79; lr: 1.00000; 693/722 tok/s;    761 sec\n","[2022-03-12 13:30:56,585 INFO] Step 1580/ 2000; acc:  52.61; ppl: 15.20; xent: 2.72; lr: 1.00000; 727/753 tok/s;    770 sec\n","[2022-03-12 13:31:07,510 INFO] Step 1600/ 2000; acc:  46.47; ppl: 22.63; xent: 3.12; lr: 1.00000; 669/674 tok/s;    781 sec\n","[2022-03-12 13:31:16,105 INFO] Step 1620/ 2000; acc:  51.16; ppl: 16.81; xent: 2.82; lr: 1.00000; 715/710 tok/s;    789 sec\n","[2022-03-12 13:31:24,822 INFO] Step 1640/ 2000; acc:  53.32; ppl: 13.86; xent: 2.63; lr: 1.00000; 682/731 tok/s;    798 sec\n","[2022-03-12 13:31:36,080 INFO] Step 1660/ 2000; acc:  46.62; ppl: 21.78; xent: 3.08; lr: 1.00000; 674/663 tok/s;    809 sec\n","[2022-03-12 13:31:45,100 INFO] Step 1680/ 2000; acc:  51.60; ppl: 15.73; xent: 2.76; lr: 1.00000; 694/711 tok/s;    818 sec\n","[2022-03-12 13:31:53,913 INFO] Step 1700/ 2000; acc:  53.09; ppl: 14.19; xent: 2.65; lr: 1.00000; 667/718 tok/s;    827 sec\n","[2022-03-12 13:32:03,562 INFO] Step 1720/ 2000; acc:  50.29; ppl: 17.20; xent: 2.85; lr: 1.00000; 711/714 tok/s;    837 sec\n","[2022-03-12 13:32:13,904 INFO] Step 1740/ 2000; acc:  48.88; ppl: 18.68; xent: 2.93; lr: 1.00000; 651/666 tok/s;    847 sec\n","[2022-03-12 13:32:22,996 INFO] Step 1760/ 2000; acc:  52.71; ppl: 14.26; xent: 2.66; lr: 1.00000; 663/730 tok/s;    856 sec\n","[2022-03-12 13:32:32,462 INFO] Step 1780/ 2000; acc:  50.75; ppl: 16.42; xent: 2.80; lr: 1.00000; 690/701 tok/s;    865 sec\n","[2022-03-12 13:32:42,186 INFO] Step 1800/ 2000; acc:  51.32; ppl: 16.58; xent: 2.81; lr: 1.00000; 656/695 tok/s;    875 sec\n","[2022-03-12 13:32:51,469 INFO] Step 1820/ 2000; acc:  51.57; ppl: 14.76; xent: 2.69; lr: 1.00000; 721/726 tok/s;    884 sec\n","[2022-03-12 13:33:00,263 INFO] Step 1840/ 2000; acc:  53.13; ppl: 13.71; xent: 2.62; lr: 1.00000; 685/736 tok/s;    893 sec\n","[2022-03-12 13:33:08,679 INFO] Weighted corpora loaded so far:\n","\t\t\t* train: 4\n","[2022-03-12 13:33:10,495 INFO] Step 1860/ 2000; acc:  51.53; ppl: 15.15; xent: 2.72; lr: 1.00000; 627/682 tok/s;    904 sec\n","[2022-03-12 13:33:20,193 INFO] Validation perplexity: 9.08292\n","[2022-03-12 13:33:20,194 INFO] Validation accuracy: 60.8948\n","[2022-03-12 13:33:20,244 INFO] Saving checkpoint ./models/unite_512/model_step_1875.pt\n","[2022-03-12 13:33:23,182 INFO] Step 1880/ 2000; acc:  52.07; ppl: 14.81; xent: 2.70; lr: 1.00000; 507/536 tok/s;    916 sec\n","[2022-03-12 13:33:31,995 INFO] Step 1900/ 2000; acc:  53.47; ppl: 13.78; xent: 2.62; lr: 1.00000; 663/712 tok/s;    925 sec\n","[2022-03-12 13:33:42,686 INFO] Step 1920/ 2000; acc:  48.97; ppl: 17.82; xent: 2.88; lr: 1.00000; 665/680 tok/s;    936 sec\n","[2022-03-12 13:33:51,485 INFO] Step 1940/ 2000; acc:  54.60; ppl: 12.72; xent: 2.54; lr: 1.00000; 701/712 tok/s;    944 sec\n","[2022-03-12 13:34:00,395 INFO] Step 1960/ 2000; acc:  54.08; ppl: 12.77; xent: 2.55; lr: 1.00000; 672/725 tok/s;    953 sec\n","[2022-03-12 13:34:11,958 INFO] Step 1980/ 2000; acc:  48.88; ppl: 17.83; xent: 2.88; lr: 1.00000; 652/658 tok/s;    965 sec\n","[2022-03-12 13:34:21,080 INFO] Step 2000/ 2000; acc:  54.25; ppl: 13.55; xent: 2.61; lr: 1.00000; 689/717 tok/s;    974 sec\n","[2022-03-12 13:34:21,130 INFO] Saving checkpoint ./models/unite_512/model_step_2000.pt\n"]},{"data":{"text/plain":[]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_train -config config-unite-512.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33750,"status":"ok","timestamp":1647092116646,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"IXoEoT0JPxeH","outputId":"91dbcc76-77a6-4fa5-b951-81fc2c45185d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:34:43,686 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:34:49,902 INFO] PRED AVG SCORE: -1.2312, PRED PPL: 3.4255\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:34:51,553 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:34:58,038 INFO] PRED AVG SCORE: -1.0668, PRED PPL: 2.9062\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:34:59,697 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:35:06,687 INFO] PRED AVG SCORE: -0.9096, PRED PPL: 2.4834\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","[2022-03-12 13:35:08,341 INFO] Translating shard 0.\n","/usr/local/lib/python3.7/dist-packages/onmt/translate/beam_search.py:282: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  self._batch_index = self.topk_ids // vocab_size\n","[2022-03-12 13:35:15,480 INFO] PRED AVG SCORE: -0.9263, PRED PPL: 2.5252\n"]},{"data":{"text/plain":[]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","onmt_translate -model ./models/unite_512/model_step_625.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_unite_512_625.txt\n","onmt_translate -model ./models/unite_512/model_step_1250.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_unite_512_1250.txt\n","onmt_translate -model ./models/unite_512/model_step_1875.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_unite_512_1875.txt\n","onmt_translate -model ./models/unite_512/model_step_2000.pt -src ./BTEC-en-fr/test/IWSLT09_BTEC.testset.fr.tok.txt -output ./trans/trans_unite_512_2000.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":582,"status":"ok","timestamp":1647092366323,"user":{"displayName":"Driss Ait Hammou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03817800683404566417"},"user_tz":-60},"id":"xiPgoMmKP-Ar","outputId":"b151ceab-5f1c-4643-c5be-73e1ed7f84d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU = 9.45, 48.0/16.7/9.3/3.1 (BP=0.766, ratio=0.789, hyp_len=2821, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 13.68, 53.4/22.4/13.2/5.4 (BP=0.799, ratio=0.816, hyp_len=2918, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 19.41, 53.9/25.5/16.1/8.6 (BP=0.929, ratio=0.931, hyp_len=3329, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n","BLEU = 20.68, 53.9/26.6/16.7/9.0 (BP=0.961, ratio=0.961, hyp_len=3436, ref_len=3574)\n","It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"]},{"data":{"text/plain":[]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","cd drive/MyDrive/tp_nmt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_unite_512_625.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_unite_512_1250.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_unite_512_1875.txt\n","perl multi-bleu.perl ./BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < ./trans/trans_unite_512_2000.txt"]},{"cell_type":"markdown","metadata":{"id":"lhEQfzk3QI7m"},"source":["**Q14** :  le beam search (la recherche en faisceau ) est un algorithme de recherche heuristique qui explore un graphe en ne considérant qu'un ensemble limité de fils de chaque nœud , c'est une optimisation  l'algorithme de parcours en largeur(d'apres Wiki).la taille de beam par défaut est 5 .\n","\n","**C'est quoi les noeuds et les fils dans notre cas ? --> les tokens de chaque étape de décodage de la phrase cible, c.à.d. de la traduction**"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNE7LEgP/VnwT31SOCypy1s","collapsed_sections":[],"mount_file_id":"1D2YHwWajeyf3p4iZgg7zp3jKbkbivr6L","name":"tp_nmt.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
